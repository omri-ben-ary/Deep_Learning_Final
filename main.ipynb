{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AutoDecoder, AD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(64, 7 * 7 * 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (64, 7, 7)),\n",
    "        nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(32, 4 * 4 * 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (128, 4, 4)),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    ",\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(16, 7 * 7 * 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (32, 7, 7)),\n",
    "        nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(8, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(256, 7 * 7 * 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (512, 7, 7)),\n",
    "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "latent_dims = [64, 32, 128, 16, 256]\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in architectures]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=latent_dims[i], device=device, lr=0.005) for i in range(len(latent_dims))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(5)]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "csv_file_path = 'results.csv'\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()\n",
    "    train_loss = trainer.train(num_epochs=200)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d803c3-0df7-467f-abb5-04679f345f1c",
   "metadata": {},
   "source": [
    "## Fine Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab8519-6f71-453e-ba8b-8634e5a61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in archs for _ in range (3)]\n",
    "learning_rates = [lr for lr in [0.001, 0.0001, 0.005] for _ in range(4)]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=128, device=device, lr=learning_rates[i]) for i in range(len(auto_decoders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f11895-3fa7-4341-826d-1ff9e6e674f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(12)]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "csv_file_path = 'fine_tune_results.csv'\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()\n",
    "    train_loss = trainer.train(num_epochs=200)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb38dc2-2dfc-483e-b3e2-eea2bfbd2fe2",
   "metadata": {},
   "source": [
    "## Fine tune LeakyReLu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d86d51-198a-473d-8420-0e43439d1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "]\n",
    "\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in archs]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=128, device=device, lr=5e-3) for i in range(len(auto_decoders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3cad4-e319-4c58-a69e-1cabfae94da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(len(auto_decoders))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "csv_file_path = 'fine_tune_leaky_results.csv'\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()\n",
    "    train_loss = trainer.train(num_epochs=200)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f34cdc-26d1-42b3-9e0c-fa6ae8a881ae",
   "metadata": {},
   "source": [
    "## Fine tune dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ff21a-f2cd-457a-8ea9-2aeb0f529a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "   nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "   nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "    \n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in archs]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=128, device=device, lr=5e-3) for i in range(len(auto_decoders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c144e5-c08e-4070-8e28-d1ed61d78249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune results\n",
    "\n",
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(len(auto_decoders))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'fine_tune_dropout_results.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=200)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eee134-c043-4fab-b768-dcb153c3a622",
   "metadata": {},
   "source": [
    "## Fine Tune Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec23ab3-a49c-4ecb-a207-4b8fb055176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archs = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.05),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.15),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    ),\n",
    "     nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.25),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "]\n",
    "\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in archs]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=128, device=device, lr=5e-3) for i in range(len(auto_decoders))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f37c6-e32e-487e-8dc0-a06dcac02b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune results\n",
    "\n",
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(len(auto_decoders))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'fine_tune_both_results.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=200)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989a70f-794b-43de-af22-c74b0acb4cd0",
   "metadata": {},
   "source": [
    "## Best Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fb302-6e51-48f1-83d4-148f777e16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = nn.Sequential(\n",
    "        nn.Linear(128, 7 * 7 * 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Unflatten(1, (256, 7, 7)),\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "decoder = AutoDecoder.AutoDecoder(arch)\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=decoder, dataloader=train_dl, latent_dim=128, device=device, lr=5e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617675bd-fd62-4c49-9837-745061c42ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune results\n",
    "\n",
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[0].latent_dim).to(device))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'best_results.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] +  ['Final Train Loss'] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=500)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss_evaluate = evaluate_model(model=auto_decoders[index], test_dl=train_dl, opt=optimizers[index], latents=latents_list[index], epochs=500, device=device) \n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=500, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [train_loss_evaluate] +  [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0cc968-c83d-4796-bc9d-3febbc3722f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=500)\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "latents = torch.nn.Parameter(torch.randn(num_test_samples, trainer.latent_dim).to(device))\n",
    "opt = optim.Adam([latents], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a2d1a-8cdc-459a-9a51-28628b188e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate_model(model=decoder, test_dl=test_dl, opt=opt, latents=latents, epochs=1000, device=device)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Sample specific vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(latents)), 5)\n",
    "\n",
    "sampled_latents = [latents[i] for i in sampled_indices]\n",
    "\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "sampled_test_images = decoder(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = decoder(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
