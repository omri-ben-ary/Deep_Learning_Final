{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import AutoDecoder, AD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    # 1. Simple Architecture (Latent space: 64)\n",
    "    nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 784)\n",
    "    ),\n",
    "    \n",
    "    # 2. Deeper Architecture (Latent space: 32)\n",
    "    nn.Sequential(\n",
    "        nn.Linear(32, 128),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(256, 512),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(512, 1024),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(1024, 784)\n",
    "    ),\n",
    "    \n",
    "    # 3. Wider Architecture (Latent space: 128)\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(1024, 784)\n",
    "    ),\n",
    "    \n",
    "    # 4. Progressive Architecture (Latent space: 16)\n",
    "    nn.Sequential(\n",
    "        nn.Linear(16, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 784)\n",
    "    ),\n",
    "    \n",
    "    # 5. Bottlenecked Architecture (Latent space: 10)\n",
    "    nn.Sequential(\n",
    "        nn.Linear(10, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 784)\n",
    "    )\n",
    "]\n",
    "\n",
    "latent_dims = [64, 32, 128, 16, 10 for _ in range(5)]\n",
    "auto_decoders = [AutoDecoder.AutoDecoder(arch) for arch in architectures for _ in range(5)]\n",
    "learning_rates = [0.001, 0.0005, 0.0001, 0.002, 0.005 for _ in range(5)]\n",
    "trainers = [AD_Trainer.AD_Trainer(decoder=auto_decoders[i], dataloader=train_dl, latent_dim=latent_dims[i], device=device, lr=learning_rate) for i in range(len(latent_dims)) for learning_rate in learning_rates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.5142\n",
      "Epoch [2/200], Loss: 0.4211\n",
      "Epoch [3/200], Loss: 0.3514\n",
      "Epoch [4/200], Loss: 0.3360\n",
      "Epoch [5/200], Loss: 0.3309\n",
      "Epoch [6/200], Loss: 0.3291\n",
      "Epoch [7/200], Loss: 0.3266\n",
      "Epoch [8/200], Loss: 0.3245\n",
      "Epoch [9/200], Loss: 0.3226\n",
      "Epoch [10/200], Loss: 0.3208\n",
      "Epoch [11/200], Loss: 0.3191\n",
      "Epoch [12/200], Loss: 0.3175\n",
      "Epoch [13/200], Loss: 0.3159\n",
      "Epoch [14/200], Loss: 0.3143\n",
      "Epoch [15/200], Loss: 0.3128\n",
      "Epoch [16/200], Loss: 0.3114\n",
      "Epoch [17/200], Loss: 0.3100\n",
      "Epoch [18/200], Loss: 0.3087\n",
      "Epoch [19/200], Loss: 0.3074\n",
      "Epoch [20/200], Loss: 0.3062\n",
      "Epoch [21/200], Loss: 0.3051\n",
      "Epoch [22/200], Loss: 0.3040\n",
      "Epoch [23/200], Loss: 0.3030\n",
      "Epoch [24/200], Loss: 0.3022\n",
      "Epoch [25/200], Loss: 0.3014\n",
      "Epoch [26/200], Loss: 0.3007\n",
      "Epoch [27/200], Loss: 0.3001\n",
      "Epoch [28/200], Loss: 0.2996\n",
      "Epoch [29/200], Loss: 0.2992\n",
      "Epoch [30/200], Loss: 0.2989\n",
      "Epoch [31/200], Loss: 0.2986\n",
      "Epoch [32/200], Loss: 0.2984\n",
      "Epoch [33/200], Loss: 0.2983\n",
      "Epoch [34/200], Loss: 0.2982\n",
      "Epoch [35/200], Loss: 0.2981\n",
      "Epoch [36/200], Loss: 0.2981\n",
      "Epoch [37/200], Loss: 0.2981\n",
      "Epoch [38/200], Loss: 0.2983\n",
      "Epoch [39/200], Loss: 0.2986\n",
      "Epoch [40/200], Loss: 0.2992\n",
      "Epoch [41/200], Loss: 0.3002\n",
      "Epoch [42/200], Loss: 0.3019\n",
      "Epoch [43/200], Loss: 0.3048\n",
      "Epoch [44/200], Loss: 0.3092\n",
      "Epoch [45/200], Loss: 0.3132\n",
      "Epoch [46/200], Loss: 0.3220\n",
      "Epoch [47/200], Loss: 0.3136\n",
      "Epoch [48/200], Loss: 0.3022\n",
      "Epoch [49/200], Loss: 0.2985\n",
      "Epoch [50/200], Loss: 0.2975\n",
      "Epoch [51/200], Loss: 0.2973\n",
      "Epoch [52/200], Loss: 0.2971\n",
      "Epoch [53/200], Loss: 0.2967\n",
      "Epoch [54/200], Loss: 0.2962\n",
      "Epoch [55/200], Loss: 0.2957\n",
      "Epoch [56/200], Loss: 0.2950\n",
      "Epoch [57/200], Loss: 0.2942\n",
      "Epoch [58/200], Loss: 0.2934\n",
      "Epoch [59/200], Loss: 0.2924\n",
      "Epoch [60/200], Loss: 0.2914\n",
      "Epoch [61/200], Loss: 0.2903\n",
      "Epoch [62/200], Loss: 0.2890\n",
      "Epoch [63/200], Loss: 0.2877\n",
      "Epoch [64/200], Loss: 0.2862\n",
      "Epoch [65/200], Loss: 0.2846\n",
      "Epoch [66/200], Loss: 0.2827\n",
      "Epoch [67/200], Loss: 0.2806\n",
      "Epoch [68/200], Loss: 0.2786\n",
      "Epoch [69/200], Loss: 0.2767\n",
      "Epoch [70/200], Loss: 0.2752\n",
      "Epoch [71/200], Loss: 0.2743\n",
      "Epoch [72/200], Loss: 0.2742\n",
      "Epoch [73/200], Loss: 0.2752\n",
      "Epoch [74/200], Loss: 0.2787\n",
      "Epoch [75/200], Loss: 0.2827\n",
      "Epoch [76/200], Loss: 0.2754\n",
      "Epoch [77/200], Loss: 0.2722\n",
      "Epoch [78/200], Loss: 0.2669\n",
      "Epoch [79/200], Loss: 0.2642\n",
      "Epoch [80/200], Loss: 0.2630\n",
      "Epoch [81/200], Loss: 0.2621\n",
      "Epoch [82/200], Loss: 0.2614\n",
      "Epoch [83/200], Loss: 0.2607\n",
      "Epoch [84/200], Loss: 0.2603\n",
      "Epoch [85/200], Loss: 0.2604\n",
      "Epoch [86/200], Loss: 0.2617\n",
      "Epoch [87/200], Loss: 0.2634\n",
      "Epoch [88/200], Loss: 0.2620\n",
      "Epoch [89/200], Loss: 0.2606\n",
      "Epoch [90/200], Loss: 0.2602\n",
      "Epoch [91/200], Loss: 0.2558\n",
      "Epoch [92/200], Loss: 0.2518\n",
      "Epoch [93/200], Loss: 0.2506\n",
      "Epoch [94/200], Loss: 0.2497\n",
      "Epoch [95/200], Loss: 0.2467\n",
      "Epoch [96/200], Loss: 0.2466\n",
      "Epoch [97/200], Loss: 0.2462\n",
      "Epoch [98/200], Loss: 0.2456\n",
      "Epoch [99/200], Loss: 0.2449\n",
      "Epoch [100/200], Loss: 0.2436\n",
      "Epoch [101/200], Loss: 0.2436\n",
      "Epoch [102/200], Loss: 0.2433\n",
      "Epoch [103/200], Loss: 0.2427\n",
      "Epoch [104/200], Loss: 0.2420\n",
      "Epoch [105/200], Loss: 0.2412\n",
      "Epoch [106/200], Loss: 0.2412\n",
      "Epoch [107/200], Loss: 0.2410\n",
      "Epoch [108/200], Loss: 0.2408\n",
      "Epoch [109/200], Loss: 0.2404\n",
      "Epoch [110/200], Loss: 0.2394\n",
      "Epoch [111/200], Loss: 0.2393\n",
      "Epoch [112/200], Loss: 0.2393\n",
      "Epoch [113/200], Loss: 0.2391\n",
      "Epoch [114/200], Loss: 0.2390\n",
      "Epoch [115/200], Loss: 0.2380\n",
      "Epoch [116/200], Loss: 0.2379\n",
      "Epoch [117/200], Loss: 0.2381\n",
      "Epoch [118/200], Loss: 0.2381\n",
      "Epoch [119/200], Loss: 0.2385\n",
      "Epoch [120/200], Loss: 0.2373\n",
      "Epoch [121/200], Loss: 0.2372\n",
      "Epoch [122/200], Loss: 0.2376\n",
      "Epoch [123/200], Loss: 0.2374\n",
      "Epoch [124/200], Loss: 0.2379\n",
      "Epoch [125/200], Loss: 0.2364\n",
      "Epoch [126/200], Loss: 0.2356\n",
      "Epoch [127/200], Loss: 0.2355\n",
      "Epoch [128/200], Loss: 0.2367\n",
      "Epoch [129/200], Loss: 0.2359\n",
      "Epoch [130/200], Loss: 0.2344\n",
      "Epoch [131/200], Loss: 0.2346\n",
      "Epoch [132/200], Loss: 0.2348\n",
      "Epoch [133/200], Loss: 0.2337\n",
      "Epoch [134/200], Loss: 0.2312\n",
      "Epoch [135/200], Loss: 0.2300\n",
      "Epoch [136/200], Loss: 0.2292\n",
      "Epoch [137/200], Loss: 0.2278\n",
      "Epoch [138/200], Loss: 0.2265\n",
      "Epoch [139/200], Loss: 0.2265\n",
      "Epoch [140/200], Loss: 0.2266\n",
      "Epoch [141/200], Loss: 0.2262\n",
      "Epoch [142/200], Loss: 0.2259\n",
      "Epoch [143/200], Loss: 0.2252\n",
      "Epoch [144/200], Loss: 0.2247\n",
      "Epoch [145/200], Loss: 0.2240\n",
      "Epoch [146/200], Loss: 0.2235\n",
      "Epoch [147/200], Loss: 0.2230\n",
      "Epoch [148/200], Loss: 0.2220\n",
      "Epoch [149/200], Loss: 0.2214\n",
      "Epoch [150/200], Loss: 0.2209\n",
      "Epoch [151/200], Loss: 0.2206\n",
      "Epoch [152/200], Loss: 0.2201\n",
      "Epoch [153/200], Loss: 0.2197\n",
      "Epoch [154/200], Loss: 0.2202\n",
      "Epoch [155/200], Loss: 0.2202\n",
      "Epoch [156/200], Loss: 0.2205\n",
      "Epoch [157/200], Loss: 0.2202\n",
      "Epoch [158/200], Loss: 0.2194\n",
      "Epoch [159/200], Loss: 0.2193\n",
      "Epoch [160/200], Loss: 0.2187\n",
      "Epoch [161/200], Loss: 0.2187\n",
      "Epoch [162/200], Loss: 0.2180\n",
      "Epoch [163/200], Loss: 0.2175\n",
      "Epoch [164/200], Loss: 0.2172\n",
      "Epoch [165/200], Loss: 0.2164\n",
      "Epoch [166/200], Loss: 0.2160\n",
      "Epoch [167/200], Loss: 0.2155\n",
      "Epoch [168/200], Loss: 0.2158\n",
      "Epoch [169/200], Loss: 0.2162\n",
      "Epoch [170/200], Loss: 0.2168\n",
      "Epoch [171/200], Loss: 0.2174\n",
      "Epoch [172/200], Loss: 0.2166\n",
      "Epoch [173/200], Loss: 0.2161\n",
      "Epoch [174/200], Loss: 0.2152\n",
      "Epoch [175/200], Loss: 0.2150\n",
      "Epoch [176/200], Loss: 0.2144\n",
      "Epoch [177/200], Loss: 0.2140\n",
      "Epoch [178/200], Loss: 0.2137\n",
      "Epoch [179/200], Loss: 0.2131\n",
      "Epoch [180/200], Loss: 0.2129\n",
      "Epoch [181/200], Loss: 0.2128\n",
      "Epoch [182/200], Loss: 0.2134\n",
      "Epoch [183/200], Loss: 0.2142\n",
      "Epoch [184/200], Loss: 0.2160\n",
      "Epoch [185/200], Loss: 0.2165\n",
      "Epoch [186/200], Loss: 0.2152\n",
      "Epoch [187/200], Loss: 0.2149\n",
      "Epoch [188/200], Loss: 0.2170\n",
      "Epoch [189/200], Loss: 0.2220\n",
      "Epoch [190/200], Loss: 0.2238\n",
      "Epoch [191/200], Loss: 0.2235\n",
      "Epoch [192/200], Loss: 0.2264\n",
      "Epoch [193/200], Loss: 0.2214\n",
      "Epoch [194/200], Loss: 0.2151\n",
      "Epoch [195/200], Loss: 0.2127\n",
      "Epoch [196/200], Loss: 0.2114\n",
      "Epoch [197/200], Loss: 0.2094\n",
      "Epoch [198/200], Loss: 0.2093\n",
      "Epoch [199/200], Loss: 0.2096\n",
      "Epoch [200/200], Loss: 0.2089\n",
      "Trainer 0 has finished training in 8.71 seconds.\n",
      "AD 0 has finished test evaluation in 4.23 seconds.\n",
      "Epoch [1/200], Loss: 0.3871\n",
      "Epoch [2/200], Loss: 0.3670\n",
      "Epoch [3/200], Loss: 0.3556\n",
      "Epoch [4/200], Loss: 0.3482\n",
      "Epoch [5/200], Loss: 0.3431\n",
      "Epoch [6/200], Loss: 0.3392\n",
      "Epoch [7/200], Loss: 0.3360\n",
      "Epoch [8/200], Loss: 0.3334\n",
      "Epoch [9/200], Loss: 0.3311\n",
      "Epoch [10/200], Loss: 0.3289\n",
      "Epoch [11/200], Loss: 0.3268\n",
      "Epoch [12/200], Loss: 0.3248\n",
      "Epoch [13/200], Loss: 0.3229\n",
      "Epoch [14/200], Loss: 0.3210\n",
      "Epoch [15/200], Loss: 0.3191\n",
      "Epoch [16/200], Loss: 0.3172\n",
      "Epoch [17/200], Loss: 0.3152\n",
      "Epoch [18/200], Loss: 0.3132\n",
      "Epoch [19/200], Loss: 0.3111\n",
      "Epoch [20/200], Loss: 0.3090\n",
      "Epoch [21/200], Loss: 0.3068\n",
      "Epoch [22/200], Loss: 0.3046\n",
      "Epoch [23/200], Loss: 0.3024\n",
      "Epoch [24/200], Loss: 0.3002\n",
      "Epoch [25/200], Loss: 0.2979\n",
      "Epoch [26/200], Loss: 0.2955\n",
      "Epoch [27/200], Loss: 0.2932\n",
      "Epoch [28/200], Loss: 0.2908\n",
      "Epoch [29/200], Loss: 0.2884\n",
      "Epoch [30/200], Loss: 0.2860\n",
      "Epoch [31/200], Loss: 0.2836\n",
      "Epoch [32/200], Loss: 0.2812\n",
      "Epoch [33/200], Loss: 0.2788\n",
      "Epoch [34/200], Loss: 0.2765\n",
      "Epoch [35/200], Loss: 0.2741\n",
      "Epoch [36/200], Loss: 0.2717\n",
      "Epoch [37/200], Loss: 0.2693\n",
      "Epoch [38/200], Loss: 0.2670\n",
      "Epoch [39/200], Loss: 0.2648\n",
      "Epoch [40/200], Loss: 0.2626\n",
      "Epoch [41/200], Loss: 0.2605\n",
      "Epoch [42/200], Loss: 0.2584\n",
      "Epoch [43/200], Loss: 0.2565\n",
      "Epoch [44/200], Loss: 0.2546\n",
      "Epoch [45/200], Loss: 0.2528\n",
      "Epoch [46/200], Loss: 0.2511\n",
      "Epoch [47/200], Loss: 0.2494\n",
      "Epoch [48/200], Loss: 0.2478\n",
      "Epoch [49/200], Loss: 0.2463\n",
      "Epoch [50/200], Loss: 0.2449\n",
      "Epoch [51/200], Loss: 0.2435\n",
      "Epoch [52/200], Loss: 0.2422\n",
      "Epoch [53/200], Loss: 0.2409\n",
      "Epoch [54/200], Loss: 0.2396\n",
      "Epoch [55/200], Loss: 0.2385\n",
      "Epoch [56/200], Loss: 0.2374\n",
      "Epoch [57/200], Loss: 0.2364\n",
      "Epoch [58/200], Loss: 0.2354\n",
      "Epoch [59/200], Loss: 0.2345\n",
      "Epoch [60/200], Loss: 0.2336\n",
      "Epoch [61/200], Loss: 0.2328\n",
      "Epoch [62/200], Loss: 0.2319\n",
      "Epoch [63/200], Loss: 0.2312\n",
      "Epoch [64/200], Loss: 0.2304\n",
      "Epoch [65/200], Loss: 0.2297\n",
      "Epoch [66/200], Loss: 0.2290\n",
      "Epoch [67/200], Loss: 0.2284\n",
      "Epoch [68/200], Loss: 0.2278\n",
      "Epoch [69/200], Loss: 0.2271\n",
      "Epoch [70/200], Loss: 0.2265\n",
      "Epoch [71/200], Loss: 0.2259\n",
      "Epoch [72/200], Loss: 0.2254\n",
      "Epoch [73/200], Loss: 0.2250\n",
      "Epoch [74/200], Loss: 0.2248\n",
      "Epoch [75/200], Loss: 0.2249\n",
      "Epoch [76/200], Loss: 0.2252\n",
      "Epoch [77/200], Loss: 0.2259\n",
      "Epoch [78/200], Loss: 0.2271\n",
      "Epoch [79/200], Loss: 0.2295\n",
      "Epoch [80/200], Loss: 0.2318\n",
      "Epoch [81/200], Loss: 0.2294\n",
      "Epoch [82/200], Loss: 0.2281\n",
      "Epoch [83/200], Loss: 0.2250\n",
      "Epoch [84/200], Loss: 0.2221\n",
      "Epoch [85/200], Loss: 0.2214\n",
      "Epoch [86/200], Loss: 0.2200\n",
      "Epoch [87/200], Loss: 0.2192\n",
      "Epoch [88/200], Loss: 0.2190\n",
      "Epoch [89/200], Loss: 0.2184\n",
      "Epoch [90/200], Loss: 0.2179\n",
      "Epoch [91/200], Loss: 0.2173\n",
      "Epoch [92/200], Loss: 0.2167\n",
      "Epoch [93/200], Loss: 0.2165\n",
      "Epoch [94/200], Loss: 0.2162\n",
      "Epoch [95/200], Loss: 0.2159\n",
      "Epoch [96/200], Loss: 0.2155\n",
      "Epoch [97/200], Loss: 0.2150\n",
      "Epoch [98/200], Loss: 0.2146\n",
      "Epoch [99/200], Loss: 0.2144\n",
      "Epoch [100/200], Loss: 0.2142\n",
      "Epoch [101/200], Loss: 0.2139\n",
      "Epoch [102/200], Loss: 0.2136\n",
      "Epoch [103/200], Loss: 0.2131\n",
      "Epoch [104/200], Loss: 0.2127\n",
      "Epoch [105/200], Loss: 0.2126\n",
      "Epoch [106/200], Loss: 0.2124\n",
      "Epoch [107/200], Loss: 0.2123\n",
      "Epoch [108/200], Loss: 0.2120\n",
      "Epoch [109/200], Loss: 0.2115\n",
      "Epoch [110/200], Loss: 0.2111\n",
      "Epoch [111/200], Loss: 0.2112\n",
      "Epoch [112/200], Loss: 0.2111\n",
      "Epoch [113/200], Loss: 0.2110\n",
      "Epoch [114/200], Loss: 0.2108\n",
      "Epoch [115/200], Loss: 0.2103\n",
      "Epoch [116/200], Loss: 0.2098\n",
      "Epoch [117/200], Loss: 0.2100\n",
      "Epoch [118/200], Loss: 0.2099\n",
      "Epoch [119/200], Loss: 0.2098\n",
      "Epoch [120/200], Loss: 0.2096\n",
      "Epoch [121/200], Loss: 0.2090\n",
      "Epoch [122/200], Loss: 0.2085\n",
      "Epoch [123/200], Loss: 0.2087\n",
      "Epoch [124/200], Loss: 0.2086\n",
      "Epoch [125/200], Loss: 0.2084\n",
      "Epoch [126/200], Loss: 0.2082\n",
      "Epoch [127/200], Loss: 0.2075\n",
      "Epoch [128/200], Loss: 0.2071\n",
      "Epoch [129/200], Loss: 0.2073\n",
      "Epoch [130/200], Loss: 0.2071\n",
      "Epoch [131/200], Loss: 0.2069\n",
      "Epoch [132/200], Loss: 0.2066\n",
      "Epoch [133/200], Loss: 0.2059\n",
      "Epoch [134/200], Loss: 0.2057\n",
      "Epoch [135/200], Loss: 0.2057\n",
      "Epoch [136/200], Loss: 0.2056\n",
      "Epoch [137/200], Loss: 0.2056\n",
      "Epoch [138/200], Loss: 0.2053\n",
      "Epoch [139/200], Loss: 0.2050\n",
      "Epoch [140/200], Loss: 0.2052\n",
      "Epoch [141/200], Loss: 0.2053\n",
      "Epoch [142/200], Loss: 0.2053\n",
      "Epoch [143/200], Loss: 0.2055\n",
      "Epoch [144/200], Loss: 0.2053\n",
      "Epoch [145/200], Loss: 0.2051\n",
      "Epoch [146/200], Loss: 0.2052\n",
      "Epoch [147/200], Loss: 0.2058\n",
      "Epoch [148/200], Loss: 0.2065\n",
      "Epoch [149/200], Loss: 0.2060\n",
      "Epoch [150/200], Loss: 0.2055\n",
      "Epoch [151/200], Loss: 0.2060\n",
      "Epoch [152/200], Loss: 0.2056\n",
      "Epoch [153/200], Loss: 0.2046\n",
      "Epoch [154/200], Loss: 0.2038\n",
      "Epoch [155/200], Loss: 0.2033\n",
      "Epoch [156/200], Loss: 0.2027\n",
      "Epoch [157/200], Loss: 0.2022\n",
      "Epoch [158/200], Loss: 0.2021\n",
      "Epoch [159/200], Loss: 0.2016\n",
      "Epoch [160/200], Loss: 0.2011\n",
      "Epoch [161/200], Loss: 0.2009\n",
      "Epoch [162/200], Loss: 0.2007\n",
      "Epoch [163/200], Loss: 0.2005\n",
      "Epoch [164/200], Loss: 0.2003\n",
      "Epoch [165/200], Loss: 0.2001\n",
      "Epoch [166/200], Loss: 0.1999\n",
      "Epoch [167/200], Loss: 0.1998\n",
      "Epoch [168/200], Loss: 0.1996\n",
      "Epoch [169/200], Loss: 0.1995\n",
      "Epoch [170/200], Loss: 0.1992\n",
      "Epoch [171/200], Loss: 0.1991\n",
      "Epoch [172/200], Loss: 0.1990\n",
      "Epoch [173/200], Loss: 0.1990\n",
      "Epoch [174/200], Loss: 0.1988\n",
      "Epoch [175/200], Loss: 0.1987\n",
      "Epoch [176/200], Loss: 0.1985\n",
      "Epoch [177/200], Loss: 0.1983\n",
      "Epoch [178/200], Loss: 0.1983\n",
      "Epoch [179/200], Loss: 0.1983\n",
      "Epoch [180/200], Loss: 0.1983\n",
      "Epoch [181/200], Loss: 0.1981\n",
      "Epoch [182/200], Loss: 0.1978\n",
      "Epoch [183/200], Loss: 0.1976\n",
      "Epoch [184/200], Loss: 0.1976\n",
      "Epoch [185/200], Loss: 0.1976\n",
      "Epoch [186/200], Loss: 0.1975\n",
      "Epoch [187/200], Loss: 0.1974\n",
      "Epoch [188/200], Loss: 0.1970\n",
      "Epoch [189/200], Loss: 0.1968\n",
      "Epoch [190/200], Loss: 0.1969\n",
      "Epoch [191/200], Loss: 0.1970\n",
      "Epoch [192/200], Loss: 0.1971\n",
      "Epoch [193/200], Loss: 0.1972\n",
      "Epoch [194/200], Loss: 0.1968\n",
      "Epoch [195/200], Loss: 0.1967\n",
      "Epoch [196/200], Loss: 0.1975\n",
      "Epoch [197/200], Loss: 0.1977\n",
      "Epoch [198/200], Loss: 0.1979\n",
      "Epoch [199/200], Loss: 0.1981\n",
      "Epoch [200/200], Loss: 0.1972\n",
      "Trainer 1 has finished training in 8.04 seconds.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x64 and 32x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has finished training in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the start time\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_decoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record the end time\u001b[39;00m\n\u001b[1;32m     30\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n",
      "File \u001b[0;32m~/Deep_Learning_Final/Deep_Learning_Final/evaluate.py:26\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dl, opt, latents, epochs, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m i \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m x_rec \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m reconstruction_loss(x, x_rec)\n\u001b[1;32m     28\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Deep_Learning_Final/Deep_Learning_Final/AutoDecoder.py:23\u001b[0m, in \u001b[0;36mAutoDecoder.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/AD-Project/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x64 and 32x128)"
     ]
    }
   ],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[5*i].latent_dim).to(device)) for i in range(5) for _ in range(5)]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=200)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=auto_decoders[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Evaluate Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bfd32-1c89-4f4f-9494-9d7f6e7adc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "latents = torch.nn.Parameter(torch.randn(num_test_samples, trainer.latent_dim).to(device))\n",
    "opt = optim.Adam([latents], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evaluate_model(model=auto_decoder, test_dl=test_dl, opt=opt, latents=latents, epochs=100, device=device)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
