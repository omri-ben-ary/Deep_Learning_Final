{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from VariationalAutoDecoder import VariationalAutoDecoder as VAD\n",
    "from VAD_Trainer import VAD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "architectures = [\n",
    "    # Architecture 1: Balanced Depth with Dropout\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        )\n",
    "    },\n",
    "\n",
    "    # Architecture 2: Deeper and Wider Network with Batch Normalization\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        )\n",
    "    },\n",
    "\n",
    "    # Architecture 3: Lightweight Architecture with High Dropout\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        )\n",
    "    },\n",
    "\n",
    "    # Architecture 4: Residual Connections and Dropout\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25)\n",
    "            ),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.25)\n",
    "            ),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        )\n",
    "    },\n",
    "\n",
    "    # Architecture 5: Small Network with Layer Normalization\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)  # Output dimension changed to 128\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# latent_dims = [dim for dim in [64, 32, 128, 16, 10] for _ in range(5)]\n",
    "VADs = [VAD(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device) for arch in architectures]# for _ in range(5)]\n",
    "# learning_rates = [lr for lr in [0.001, 0.0005, 0.0001, 0.002, 0.005] for _ in range(5)]\n",
    "trainers = [VAD_Trainer(var_decoder=VADs[i], dataloader=train_dl, latent_dim=128, device=device, lr=1e-3) for i in range(len(VADs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.4969\n",
      "Epoch [2/200], Loss: 0.4659\n",
      "Epoch [3/200], Loss: 0.4442\n",
      "Epoch [4/200], Loss: 0.4226\n",
      "Epoch [5/200], Loss: 0.4010\n",
      "Epoch [6/200], Loss: 0.3790\n",
      "Epoch [7/200], Loss: 0.3577\n",
      "Epoch [8/200], Loss: 0.3367\n",
      "Epoch [9/200], Loss: 0.3179\n",
      "Epoch [10/200], Loss: 0.3023\n",
      "Epoch [11/200], Loss: 0.2890\n",
      "Epoch [12/200], Loss: 0.2788\n",
      "Epoch [13/200], Loss: 0.2698\n",
      "Epoch [14/200], Loss: 0.2623\n",
      "Epoch [15/200], Loss: 0.2575\n",
      "Epoch [16/200], Loss: 0.2539\n",
      "Epoch [17/200], Loss: 0.2517\n",
      "Epoch [18/200], Loss: 0.2495\n",
      "Epoch [19/200], Loss: 0.2488\n",
      "Epoch [20/200], Loss: 0.2495\n",
      "Epoch [21/200], Loss: 0.2477\n",
      "Epoch [22/200], Loss: 0.2447\n",
      "Epoch [23/200], Loss: 0.2414\n",
      "Epoch [24/200], Loss: 0.2386\n",
      "Epoch [25/200], Loss: 0.2363\n",
      "Epoch [26/200], Loss: 0.2330\n",
      "Epoch [27/200], Loss: 0.2301\n",
      "Epoch [28/200], Loss: 0.2276\n",
      "Epoch [29/200], Loss: 0.2253\n",
      "Epoch [30/200], Loss: 0.2246\n",
      "Epoch [31/200], Loss: 0.2226\n",
      "Epoch [32/200], Loss: 0.2212\n",
      "Epoch [33/200], Loss: 0.2215\n",
      "Epoch [34/200], Loss: 0.2223\n",
      "Epoch [35/200], Loss: 0.2231\n",
      "Epoch [36/200], Loss: 0.2232\n",
      "Epoch [37/200], Loss: 0.2200\n",
      "Epoch [38/200], Loss: 0.2179\n",
      "Epoch [39/200], Loss: 0.2165\n",
      "Epoch [40/200], Loss: 0.2147\n",
      "Epoch [41/200], Loss: 0.2126\n",
      "Epoch [42/200], Loss: 0.2116\n",
      "Epoch [43/200], Loss: 0.2110\n",
      "Epoch [44/200], Loss: 0.2093\n",
      "Epoch [45/200], Loss: 0.2069\n",
      "Epoch [46/200], Loss: 0.2047\n",
      "Epoch [47/200], Loss: 0.2034\n",
      "Epoch [48/200], Loss: 0.2024\n",
      "Epoch [49/200], Loss: 0.2026\n",
      "Epoch [50/200], Loss: 0.2032\n",
      "Epoch [51/200], Loss: 0.2039\n",
      "Epoch [52/200], Loss: 0.2028\n",
      "Epoch [53/200], Loss: 0.2015\n",
      "Epoch [54/200], Loss: 0.1980\n",
      "Epoch [55/200], Loss: 0.1957\n",
      "Epoch [56/200], Loss: 0.1955\n",
      "Epoch [57/200], Loss: 0.1959\n",
      "Epoch [58/200], Loss: 0.1952\n",
      "Epoch [59/200], Loss: 0.1933\n",
      "Epoch [60/200], Loss: 0.1932\n",
      "Epoch [61/200], Loss: 0.1949\n",
      "Epoch [62/200], Loss: 0.1945\n",
      "Epoch [63/200], Loss: 0.1933\n",
      "Epoch [64/200], Loss: 0.1926\n",
      "Epoch [65/200], Loss: 0.1922\n",
      "Epoch [66/200], Loss: 0.1933\n",
      "Epoch [67/200], Loss: 0.1933\n",
      "Epoch [68/200], Loss: 0.1945\n",
      "Epoch [69/200], Loss: 0.1974\n",
      "Epoch [70/200], Loss: 0.1962\n",
      "Epoch [71/200], Loss: 0.1895\n",
      "Epoch [72/200], Loss: 0.1867\n",
      "Epoch [73/200], Loss: 0.1838\n",
      "Epoch [74/200], Loss: 0.1819\n",
      "Epoch [75/200], Loss: 0.1813\n",
      "Epoch [76/200], Loss: 0.1808\n",
      "Epoch [77/200], Loss: 0.1817\n",
      "Epoch [78/200], Loss: 0.1817\n",
      "Epoch [79/200], Loss: 0.1817\n",
      "Epoch [80/200], Loss: 0.1803\n",
      "Epoch [81/200], Loss: 0.1798\n",
      "Epoch [82/200], Loss: 0.1797\n",
      "Epoch [83/200], Loss: 0.1803\n",
      "Epoch [84/200], Loss: 0.1812\n",
      "Epoch [85/200], Loss: 0.1823\n",
      "Epoch [86/200], Loss: 0.1806\n",
      "Epoch [87/200], Loss: 0.1775\n",
      "Epoch [88/200], Loss: 0.1766\n",
      "Epoch [89/200], Loss: 0.1752\n",
      "Epoch [90/200], Loss: 0.1752\n",
      "Epoch [91/200], Loss: 0.1753\n",
      "Epoch [92/200], Loss: 0.1753\n",
      "Epoch [93/200], Loss: 0.1757\n",
      "Epoch [94/200], Loss: 0.1758\n",
      "Epoch [95/200], Loss: 0.1749\n",
      "Epoch [96/200], Loss: 0.1752\n",
      "Epoch [97/200], Loss: 0.1734\n",
      "Epoch [98/200], Loss: 0.1728\n",
      "Epoch [99/200], Loss: 0.1719\n",
      "Epoch [100/200], Loss: 0.1711\n",
      "Epoch [101/200], Loss: 0.1708\n",
      "Epoch [102/200], Loss: 0.1716\n",
      "Epoch [103/200], Loss: 0.1709\n",
      "Epoch [104/200], Loss: 0.1704\n",
      "Epoch [105/200], Loss: 0.1723\n",
      "Epoch [106/200], Loss: 0.1687\n",
      "Epoch [107/200], Loss: 0.1682\n",
      "Epoch [108/200], Loss: 0.1682\n",
      "Epoch [109/200], Loss: 0.1676\n",
      "Epoch [110/200], Loss: 0.1678\n",
      "Epoch [111/200], Loss: 0.1677\n",
      "Epoch [112/200], Loss: 0.1654\n",
      "Epoch [113/200], Loss: 0.1631\n",
      "Epoch [114/200], Loss: 0.1607\n",
      "Epoch [115/200], Loss: 0.1600\n",
      "Epoch [116/200], Loss: 0.1587\n",
      "Epoch [117/200], Loss: 0.1579\n",
      "Epoch [118/200], Loss: 0.1579\n",
      "Epoch [119/200], Loss: 0.1591\n",
      "Epoch [120/200], Loss: 0.1617\n",
      "Epoch [121/200], Loss: 0.1614\n",
      "Epoch [122/200], Loss: 0.1600\n",
      "Epoch [123/200], Loss: 0.1592\n",
      "Epoch [124/200], Loss: 0.1588\n",
      "Epoch [125/200], Loss: 0.1594\n",
      "Epoch [126/200], Loss: 0.1559\n",
      "Epoch [127/200], Loss: 0.1561\n",
      "Epoch [128/200], Loss: 0.1572\n",
      "Epoch [129/200], Loss: 0.1543\n",
      "Epoch [130/200], Loss: 0.1527\n",
      "Epoch [131/200], Loss: 0.1525\n",
      "Epoch [132/200], Loss: 0.1510\n",
      "Epoch [133/200], Loss: 0.1521\n",
      "Epoch [134/200], Loss: 0.1518\n",
      "Epoch [135/200], Loss: 0.1499\n",
      "Epoch [136/200], Loss: 0.1505\n",
      "Epoch [137/200], Loss: 0.1513\n",
      "Epoch [138/200], Loss: 0.1496\n",
      "Epoch [139/200], Loss: 0.1516\n",
      "Epoch [140/200], Loss: 0.1504\n",
      "Epoch [141/200], Loss: 0.1484\n",
      "Epoch [142/200], Loss: 0.1496\n",
      "Epoch [143/200], Loss: 0.1478\n",
      "Epoch [144/200], Loss: 0.1455\n",
      "Epoch [145/200], Loss: 0.1435\n",
      "Epoch [146/200], Loss: 0.1441\n",
      "Epoch [147/200], Loss: 0.1447\n",
      "Epoch [148/200], Loss: 0.1439\n",
      "Epoch [149/200], Loss: 0.1429\n",
      "Epoch [150/200], Loss: 0.1442\n",
      "Epoch [151/200], Loss: 0.1436\n",
      "Epoch [152/200], Loss: 0.1432\n",
      "Epoch [153/200], Loss: 0.1441\n",
      "Epoch [154/200], Loss: 0.1412\n",
      "Epoch [155/200], Loss: 0.1401\n",
      "Epoch [156/200], Loss: 0.1412\n",
      "Epoch [157/200], Loss: 0.1403\n",
      "Epoch [158/200], Loss: 0.1391\n",
      "Epoch [159/200], Loss: 0.1395\n",
      "Epoch [160/200], Loss: 0.1408\n",
      "Epoch [161/200], Loss: 0.1393\n",
      "Epoch [162/200], Loss: 0.1378\n",
      "Epoch [163/200], Loss: 0.1357\n",
      "Epoch [164/200], Loss: 0.1348\n",
      "Epoch [165/200], Loss: 0.1351\n",
      "Epoch [166/200], Loss: 0.1344\n",
      "Epoch [167/200], Loss: 0.1345\n",
      "Epoch [168/200], Loss: 0.1351\n",
      "Epoch [169/200], Loss: 0.1349\n",
      "Epoch [170/200], Loss: 0.1341\n",
      "Epoch [171/200], Loss: 0.1344\n",
      "Epoch [172/200], Loss: 0.1339\n",
      "Epoch [173/200], Loss: 0.1355\n",
      "Epoch [174/200], Loss: 0.1357\n",
      "Epoch [175/200], Loss: 0.1322\n",
      "Epoch [176/200], Loss: 0.1325\n",
      "Epoch [177/200], Loss: 0.1316\n",
      "Epoch [178/200], Loss: 0.1319\n",
      "Epoch [179/200], Loss: 0.1295\n",
      "Epoch [180/200], Loss: 0.1294\n",
      "Epoch [181/200], Loss: 0.1300\n",
      "Epoch [182/200], Loss: 0.1321\n",
      "Epoch [183/200], Loss: 0.1299\n",
      "Epoch [184/200], Loss: 0.1326\n",
      "Epoch [185/200], Loss: 0.1341\n",
      "Epoch [186/200], Loss: 0.1368\n",
      "Epoch [187/200], Loss: 0.1390\n",
      "Epoch [188/200], Loss: 0.1371\n",
      "Epoch [189/200], Loss: 0.1311\n",
      "Epoch [190/200], Loss: 0.1276\n",
      "Epoch [191/200], Loss: 0.1249\n",
      "Epoch [192/200], Loss: 0.1241\n",
      "Epoch [193/200], Loss: 0.1270\n",
      "Epoch [194/200], Loss: 0.1251\n",
      "Epoch [195/200], Loss: 0.1265\n",
      "Epoch [196/200], Loss: 0.1281\n",
      "Epoch [197/200], Loss: 0.1273\n",
      "Epoch [198/200], Loss: 0.1261\n",
      "Epoch [199/200], Loss: 0.1288\n",
      "Epoch [200/200], Loss: 0.1284\n",
      "Trainer 0 has finished training in 19.61 seconds.\n",
      "AD 0 has finished test evaluation in 7.67 seconds.\n",
      "Epoch [1/200], Loss: 0.5036\n",
      "Epoch [2/200], Loss: 0.4771\n",
      "Epoch [3/200], Loss: 0.4600\n",
      "Epoch [4/200], Loss: 0.4416\n",
      "Epoch [5/200], Loss: 0.4208\n",
      "Epoch [6/200], Loss: 0.3983\n",
      "Epoch [7/200], Loss: 0.3744\n",
      "Epoch [8/200], Loss: 0.3512\n",
      "Epoch [9/200], Loss: 0.3298\n",
      "Epoch [10/200], Loss: 0.3120\n",
      "Epoch [11/200], Loss: 0.2953\n",
      "Epoch [12/200], Loss: 0.2821\n",
      "Epoch [13/200], Loss: 0.2669\n",
      "Epoch [14/200], Loss: 0.2535\n",
      "Epoch [15/200], Loss: 0.2505\n",
      "Epoch [16/200], Loss: 0.2453\n",
      "Epoch [17/200], Loss: 0.2432\n",
      "Epoch [18/200], Loss: 0.2470\n",
      "Epoch [19/200], Loss: 0.2434\n",
      "Epoch [20/200], Loss: 0.2376\n",
      "Epoch [21/200], Loss: 0.2375\n",
      "Epoch [22/200], Loss: 0.2383\n",
      "Epoch [23/200], Loss: 0.2330\n",
      "Epoch [24/200], Loss: 0.2253\n",
      "Epoch [25/200], Loss: 0.2241\n",
      "Epoch [26/200], Loss: 0.2225\n",
      "Epoch [27/200], Loss: 0.2214\n",
      "Epoch [28/200], Loss: 0.2195\n",
      "Epoch [29/200], Loss: 0.2189\n",
      "Epoch [30/200], Loss: 0.2196\n",
      "Epoch [31/200], Loss: 0.2185\n",
      "Epoch [32/200], Loss: 0.2177\n",
      "Epoch [33/200], Loss: 0.2156\n",
      "Epoch [34/200], Loss: 0.2129\n",
      "Epoch [35/200], Loss: 0.2111\n",
      "Epoch [36/200], Loss: 0.2098\n",
      "Epoch [37/200], Loss: 0.2091\n",
      "Epoch [38/200], Loss: 0.2074\n",
      "Epoch [39/200], Loss: 0.2064\n",
      "Epoch [40/200], Loss: 0.2067\n",
      "Epoch [41/200], Loss: 0.2053\n",
      "Epoch [42/200], Loss: 0.2047\n",
      "Epoch [43/200], Loss: 0.2067\n",
      "Epoch [44/200], Loss: 0.2055\n",
      "Epoch [45/200], Loss: 0.2056\n",
      "Epoch [46/200], Loss: 0.2034\n",
      "Epoch [47/200], Loss: 0.2012\n",
      "Epoch [48/200], Loss: 0.1993\n",
      "Epoch [49/200], Loss: 0.1981\n",
      "Epoch [50/200], Loss: 0.1963\n",
      "Epoch [51/200], Loss: 0.1980\n",
      "Epoch [52/200], Loss: 0.1974\n",
      "Epoch [53/200], Loss: 0.1954\n",
      "Epoch [54/200], Loss: 0.1959\n",
      "Epoch [55/200], Loss: 0.1954\n",
      "Epoch [56/200], Loss: 0.1932\n",
      "Epoch [57/200], Loss: 0.1908\n",
      "Epoch [58/200], Loss: 0.1907\n",
      "Epoch [59/200], Loss: 0.1897\n",
      "Epoch [60/200], Loss: 0.1893\n",
      "Epoch [61/200], Loss: 0.1884\n",
      "Epoch [62/200], Loss: 0.1885\n",
      "Epoch [63/200], Loss: 0.1863\n",
      "Epoch [64/200], Loss: 0.1837\n",
      "Epoch [65/200], Loss: 0.1822\n",
      "Epoch [66/200], Loss: 0.1818\n",
      "Epoch [67/200], Loss: 0.1807\n",
      "Epoch [68/200], Loss: 0.1805\n",
      "Epoch [69/200], Loss: 0.1810\n",
      "Epoch [70/200], Loss: 0.1814\n",
      "Epoch [71/200], Loss: 0.1811\n",
      "Epoch [72/200], Loss: 0.1808\n",
      "Epoch [73/200], Loss: 0.1801\n",
      "Epoch [74/200], Loss: 0.1783\n",
      "Epoch [75/200], Loss: 0.1803\n",
      "Epoch [76/200], Loss: 0.1776\n",
      "Epoch [77/200], Loss: 0.1785\n",
      "Epoch [78/200], Loss: 0.1756\n",
      "Epoch [79/200], Loss: 0.1723\n",
      "Epoch [80/200], Loss: 0.1706\n",
      "Epoch [81/200], Loss: 0.1699\n",
      "Epoch [82/200], Loss: 0.1692\n",
      "Epoch [83/200], Loss: 0.1710\n",
      "Epoch [84/200], Loss: 0.1695\n",
      "Epoch [85/200], Loss: 0.1705\n",
      "Epoch [86/200], Loss: 0.1692\n",
      "Epoch [87/200], Loss: 0.1646\n",
      "Epoch [88/200], Loss: 0.1638\n",
      "Epoch [89/200], Loss: 0.1631\n",
      "Epoch [90/200], Loss: 0.1624\n",
      "Epoch [91/200], Loss: 0.1627\n",
      "Epoch [92/200], Loss: 0.1622\n",
      "Epoch [93/200], Loss: 0.1597\n",
      "Epoch [94/200], Loss: 0.1594\n",
      "Epoch [95/200], Loss: 0.1604\n",
      "Epoch [96/200], Loss: 0.1599\n",
      "Epoch [97/200], Loss: 0.1596\n",
      "Epoch [98/200], Loss: 0.1595\n",
      "Epoch [99/200], Loss: 0.1600\n",
      "Epoch [100/200], Loss: 0.1593\n",
      "Epoch [101/200], Loss: 0.1607\n",
      "Epoch [102/200], Loss: 0.1591\n",
      "Epoch [103/200], Loss: 0.1583\n",
      "Epoch [104/200], Loss: 0.1559\n",
      "Epoch [105/200], Loss: 0.1546\n",
      "Epoch [106/200], Loss: 0.1559\n",
      "Epoch [107/200], Loss: 0.1580\n",
      "Epoch [108/200], Loss: 0.1607\n",
      "Epoch [109/200], Loss: 0.1570\n",
      "Epoch [110/200], Loss: 0.1542\n",
      "Epoch [111/200], Loss: 0.1550\n",
      "Epoch [112/200], Loss: 0.1533\n",
      "Epoch [113/200], Loss: 0.1532\n",
      "Epoch [114/200], Loss: 0.1519\n",
      "Epoch [115/200], Loss: 0.1500\n",
      "Epoch [116/200], Loss: 0.1505\n",
      "Epoch [117/200], Loss: 0.1499\n",
      "Epoch [118/200], Loss: 0.1490\n",
      "Epoch [119/200], Loss: 0.1490\n",
      "Epoch [120/200], Loss: 0.1488\n",
      "Epoch [121/200], Loss: 0.1512\n",
      "Epoch [122/200], Loss: 0.1455\n",
      "Epoch [123/200], Loss: 0.1431\n",
      "Epoch [124/200], Loss: 0.1424\n",
      "Epoch [125/200], Loss: 0.1431\n",
      "Epoch [126/200], Loss: 0.1425\n",
      "Epoch [127/200], Loss: 0.1433\n",
      "Epoch [128/200], Loss: 0.1433\n",
      "Epoch [129/200], Loss: 0.1415\n",
      "Epoch [130/200], Loss: 0.1421\n",
      "Epoch [131/200], Loss: 0.1401\n",
      "Epoch [132/200], Loss: 0.1393\n",
      "Epoch [133/200], Loss: 0.1380\n",
      "Epoch [134/200], Loss: 0.1358\n",
      "Epoch [135/200], Loss: 0.1352\n",
      "Epoch [136/200], Loss: 0.1328\n",
      "Epoch [137/200], Loss: 0.1322\n",
      "Epoch [138/200], Loss: 0.1322\n",
      "Epoch [139/200], Loss: 0.1330\n",
      "Epoch [140/200], Loss: 0.1344\n",
      "Epoch [141/200], Loss: 0.1370\n",
      "Epoch [142/200], Loss: 0.1368\n",
      "Epoch [143/200], Loss: 0.1362\n",
      "Epoch [144/200], Loss: 0.1344\n",
      "Epoch [145/200], Loss: 0.1313\n",
      "Epoch [146/200], Loss: 0.1300\n",
      "Epoch [147/200], Loss: 0.1299\n",
      "Epoch [148/200], Loss: 0.1281\n",
      "Epoch [149/200], Loss: 0.1278\n",
      "Epoch [150/200], Loss: 0.1280\n",
      "Epoch [151/200], Loss: 0.1274\n",
      "Epoch [152/200], Loss: 0.1297\n",
      "Epoch [153/200], Loss: 0.1299\n",
      "Epoch [154/200], Loss: 0.1272\n",
      "Epoch [155/200], Loss: 0.1259\n",
      "Epoch [156/200], Loss: 0.1244\n",
      "Epoch [157/200], Loss: 0.1229\n",
      "Epoch [158/200], Loss: 0.1225\n",
      "Epoch [159/200], Loss: 0.1231\n",
      "Epoch [160/200], Loss: 0.1241\n",
      "Epoch [161/200], Loss: 0.1232\n",
      "Epoch [162/200], Loss: 0.1235\n",
      "Epoch [163/200], Loss: 0.1224\n",
      "Epoch [164/200], Loss: 0.1204\n",
      "Epoch [165/200], Loss: 0.1198\n",
      "Epoch [166/200], Loss: 0.1219\n",
      "Epoch [167/200], Loss: 0.1222\n",
      "Epoch [168/200], Loss: 0.1233\n",
      "Epoch [169/200], Loss: 0.1230\n",
      "Epoch [170/200], Loss: 0.1250\n",
      "Epoch [171/200], Loss: 0.1251\n",
      "Epoch [172/200], Loss: 0.1282\n",
      "Epoch [173/200], Loss: 0.1260\n",
      "Epoch [174/200], Loss: 0.1266\n",
      "Epoch [175/200], Loss: 0.1219\n",
      "Epoch [176/200], Loss: 0.1168\n",
      "Epoch [177/200], Loss: 0.1140\n",
      "Epoch [178/200], Loss: 0.1125\n",
      "Epoch [179/200], Loss: 0.1120\n",
      "Epoch [180/200], Loss: 0.1122\n",
      "Epoch [181/200], Loss: 0.1133\n",
      "Epoch [182/200], Loss: 0.1159\n",
      "Epoch [183/200], Loss: 0.1159\n",
      "Epoch [184/200], Loss: 0.1161\n",
      "Epoch [185/200], Loss: 0.1165\n",
      "Epoch [186/200], Loss: 0.1157\n",
      "Epoch [187/200], Loss: 0.1140\n",
      "Epoch [188/200], Loss: 0.1126\n",
      "Epoch [189/200], Loss: 0.1102\n",
      "Epoch [190/200], Loss: 0.1092\n",
      "Epoch [191/200], Loss: 0.1091\n",
      "Epoch [192/200], Loss: 0.1093\n",
      "Epoch [193/200], Loss: 0.1091\n",
      "Epoch [194/200], Loss: 0.1103\n",
      "Epoch [195/200], Loss: 0.1118\n",
      "Epoch [196/200], Loss: 0.1114\n",
      "Epoch [197/200], Loss: 0.1127\n",
      "Epoch [198/200], Loss: 0.1111\n",
      "Epoch [199/200], Loss: 0.1103\n",
      "Epoch [200/200], Loss: 0.1098\n",
      "Trainer 1 has finished training in 25.61 seconds.\n",
      "AD 1 has finished test evaluation in 12.20 seconds.\n",
      "Epoch [1/200], Loss: 0.4897\n",
      "Epoch [2/200], Loss: 0.4591\n",
      "Epoch [3/200], Loss: 0.4389\n",
      "Epoch [4/200], Loss: 0.4175\n",
      "Epoch [5/200], Loss: 0.3961\n",
      "Epoch [6/200], Loss: 0.3733\n",
      "Epoch [7/200], Loss: 0.3507\n",
      "Epoch [8/200], Loss: 0.3303\n",
      "Epoch [9/200], Loss: 0.3125\n",
      "Epoch [10/200], Loss: 0.2984\n",
      "Epoch [11/200], Loss: 0.2888\n",
      "Epoch [12/200], Loss: 0.2812\n",
      "Epoch [13/200], Loss: 0.2768\n",
      "Epoch [14/200], Loss: 0.2712\n",
      "Epoch [15/200], Loss: 0.2681\n",
      "Epoch [16/200], Loss: 0.2609\n",
      "Epoch [17/200], Loss: 0.2595\n",
      "Epoch [18/200], Loss: 0.2548\n",
      "Epoch [19/200], Loss: 0.2527\n",
      "Epoch [20/200], Loss: 0.2493\n",
      "Epoch [21/200], Loss: 0.2490\n",
      "Epoch [22/200], Loss: 0.2467\n",
      "Epoch [23/200], Loss: 0.2437\n",
      "Epoch [24/200], Loss: 0.2408\n",
      "Epoch [25/200], Loss: 0.2372\n",
      "Epoch [26/200], Loss: 0.2364\n",
      "Epoch [27/200], Loss: 0.2356\n",
      "Epoch [28/200], Loss: 0.2349\n",
      "Epoch [29/200], Loss: 0.2323\n",
      "Epoch [30/200], Loss: 0.2298\n",
      "Epoch [31/200], Loss: 0.2287\n",
      "Epoch [32/200], Loss: 0.2261\n",
      "Epoch [33/200], Loss: 0.2247\n",
      "Epoch [34/200], Loss: 0.2244\n",
      "Epoch [35/200], Loss: 0.2250\n",
      "Epoch [36/200], Loss: 0.2229\n",
      "Epoch [37/200], Loss: 0.2212\n",
      "Epoch [38/200], Loss: 0.2179\n",
      "Epoch [39/200], Loss: 0.2167\n",
      "Epoch [40/200], Loss: 0.2136\n",
      "Epoch [41/200], Loss: 0.2124\n",
      "Epoch [42/200], Loss: 0.2107\n",
      "Epoch [43/200], Loss: 0.2103\n",
      "Epoch [44/200], Loss: 0.2084\n",
      "Epoch [45/200], Loss: 0.2085\n",
      "Epoch [46/200], Loss: 0.2089\n",
      "Epoch [47/200], Loss: 0.2110\n",
      "Epoch [48/200], Loss: 0.2097\n",
      "Epoch [49/200], Loss: 0.2095\n",
      "Epoch [50/200], Loss: 0.2067\n",
      "Epoch [51/200], Loss: 0.2041\n",
      "Epoch [52/200], Loss: 0.2030\n",
      "Epoch [53/200], Loss: 0.2021\n",
      "Epoch [54/200], Loss: 0.2018\n",
      "Epoch [55/200], Loss: 0.2035\n",
      "Epoch [56/200], Loss: 0.2042\n",
      "Epoch [57/200], Loss: 0.2048\n",
      "Epoch [58/200], Loss: 0.2068\n",
      "Epoch [59/200], Loss: 0.2029\n",
      "Epoch [60/200], Loss: 0.1991\n",
      "Epoch [61/200], Loss: 0.1974\n",
      "Epoch [62/200], Loss: 0.1959\n",
      "Epoch [63/200], Loss: 0.1957\n",
      "Epoch [64/200], Loss: 0.1951\n",
      "Epoch [65/200], Loss: 0.1959\n",
      "Epoch [66/200], Loss: 0.1957\n",
      "Epoch [67/200], Loss: 0.1946\n",
      "Epoch [68/200], Loss: 0.1953\n",
      "Epoch [69/200], Loss: 0.1959\n",
      "Epoch [70/200], Loss: 0.1956\n",
      "Epoch [71/200], Loss: 0.1954\n",
      "Epoch [72/200], Loss: 0.1946\n",
      "Epoch [73/200], Loss: 0.1927\n",
      "Epoch [74/200], Loss: 0.1921\n",
      "Epoch [75/200], Loss: 0.1911\n",
      "Epoch [76/200], Loss: 0.1900\n",
      "Epoch [77/200], Loss: 0.1897\n",
      "Epoch [78/200], Loss: 0.1890\n",
      "Epoch [79/200], Loss: 0.1897\n",
      "Epoch [80/200], Loss: 0.1890\n",
      "Epoch [81/200], Loss: 0.1888\n",
      "Epoch [82/200], Loss: 0.1872\n",
      "Epoch [83/200], Loss: 0.1854\n",
      "Epoch [84/200], Loss: 0.1847\n",
      "Epoch [85/200], Loss: 0.1840\n",
      "Epoch [86/200], Loss: 0.1842\n",
      "Epoch [87/200], Loss: 0.1849\n",
      "Epoch [88/200], Loss: 0.1836\n",
      "Epoch [89/200], Loss: 0.1828\n",
      "Epoch [90/200], Loss: 0.1827\n",
      "Epoch [91/200], Loss: 0.1822\n",
      "Epoch [92/200], Loss: 0.1816\n",
      "Epoch [93/200], Loss: 0.1816\n",
      "Epoch [94/200], Loss: 0.1817\n",
      "Epoch [95/200], Loss: 0.1826\n",
      "Epoch [96/200], Loss: 0.1826\n",
      "Epoch [97/200], Loss: 0.1833\n",
      "Epoch [98/200], Loss: 0.1834\n",
      "Epoch [99/200], Loss: 0.1817\n",
      "Epoch [100/200], Loss: 0.1814\n",
      "Epoch [101/200], Loss: 0.1805\n",
      "Epoch [102/200], Loss: 0.1797\n",
      "Epoch [103/200], Loss: 0.1792\n",
      "Epoch [104/200], Loss: 0.1790\n",
      "Epoch [105/200], Loss: 0.1786\n",
      "Epoch [106/200], Loss: 0.1770\n",
      "Epoch [107/200], Loss: 0.1771\n",
      "Epoch [108/200], Loss: 0.1759\n",
      "Epoch [109/200], Loss: 0.1770\n",
      "Epoch [110/200], Loss: 0.1760\n",
      "Epoch [111/200], Loss: 0.1755\n",
      "Epoch [112/200], Loss: 0.1748\n",
      "Epoch [113/200], Loss: 0.1740\n",
      "Epoch [114/200], Loss: 0.1733\n",
      "Epoch [115/200], Loss: 0.1722\n",
      "Epoch [116/200], Loss: 0.1730\n",
      "Epoch [117/200], Loss: 0.1726\n",
      "Epoch [118/200], Loss: 0.1715\n",
      "Epoch [119/200], Loss: 0.1713\n",
      "Epoch [120/200], Loss: 0.1718\n",
      "Epoch [121/200], Loss: 0.1725\n",
      "Epoch [122/200], Loss: 0.1712\n",
      "Epoch [123/200], Loss: 0.1708\n",
      "Epoch [124/200], Loss: 0.1683\n",
      "Epoch [125/200], Loss: 0.1682\n",
      "Epoch [126/200], Loss: 0.1674\n",
      "Epoch [127/200], Loss: 0.1676\n",
      "Epoch [128/200], Loss: 0.1673\n",
      "Epoch [129/200], Loss: 0.1657\n",
      "Epoch [130/200], Loss: 0.1668\n",
      "Epoch [131/200], Loss: 0.1644\n",
      "Epoch [132/200], Loss: 0.1641\n",
      "Epoch [133/200], Loss: 0.1637\n",
      "Epoch [134/200], Loss: 0.1640\n",
      "Epoch [135/200], Loss: 0.1644\n",
      "Epoch [136/200], Loss: 0.1654\n",
      "Epoch [137/200], Loss: 0.1664\n",
      "Epoch [138/200], Loss: 0.1654\n",
      "Epoch [139/200], Loss: 0.1668\n",
      "Epoch [140/200], Loss: 0.1661\n",
      "Epoch [141/200], Loss: 0.1669\n",
      "Epoch [142/200], Loss: 0.1661\n",
      "Epoch [143/200], Loss: 0.1643\n",
      "Epoch [144/200], Loss: 0.1626\n",
      "Epoch [145/200], Loss: 0.1612\n",
      "Epoch [146/200], Loss: 0.1610\n",
      "Epoch [147/200], Loss: 0.1611\n",
      "Epoch [148/200], Loss: 0.1605\n",
      "Epoch [149/200], Loss: 0.1583\n",
      "Epoch [150/200], Loss: 0.1581\n",
      "Epoch [151/200], Loss: 0.1582\n",
      "Epoch [152/200], Loss: 0.1562\n",
      "Epoch [153/200], Loss: 0.1559\n",
      "Epoch [154/200], Loss: 0.1544\n",
      "Epoch [155/200], Loss: 0.1555\n",
      "Epoch [156/200], Loss: 0.1562\n",
      "Epoch [157/200], Loss: 0.1544\n",
      "Epoch [158/200], Loss: 0.1546\n",
      "Epoch [159/200], Loss: 0.1540\n",
      "Epoch [160/200], Loss: 0.1555\n",
      "Epoch [161/200], Loss: 0.1561\n",
      "Epoch [162/200], Loss: 0.1586\n",
      "Epoch [163/200], Loss: 0.1573\n",
      "Epoch [164/200], Loss: 0.1575\n",
      "Epoch [165/200], Loss: 0.1597\n",
      "Epoch [166/200], Loss: 0.1566\n",
      "Epoch [167/200], Loss: 0.1541\n",
      "Epoch [168/200], Loss: 0.1526\n",
      "Epoch [169/200], Loss: 0.1497\n",
      "Epoch [170/200], Loss: 0.1483\n",
      "Epoch [171/200], Loss: 0.1488\n",
      "Epoch [172/200], Loss: 0.1483\n",
      "Epoch [173/200], Loss: 0.1478\n",
      "Epoch [174/200], Loss: 0.1486\n",
      "Epoch [175/200], Loss: 0.1496\n",
      "Epoch [176/200], Loss: 0.1493\n",
      "Epoch [177/200], Loss: 0.1491\n",
      "Epoch [178/200], Loss: 0.1488\n",
      "Epoch [179/200], Loss: 0.1485\n",
      "Epoch [180/200], Loss: 0.1492\n",
      "Epoch [181/200], Loss: 0.1482\n",
      "Epoch [182/200], Loss: 0.1489\n",
      "Epoch [183/200], Loss: 0.1471\n",
      "Epoch [184/200], Loss: 0.1486\n",
      "Epoch [185/200], Loss: 0.1464\n",
      "Epoch [186/200], Loss: 0.1463\n",
      "Epoch [187/200], Loss: 0.1448\n",
      "Epoch [188/200], Loss: 0.1433\n",
      "Epoch [189/200], Loss: 0.1424\n",
      "Epoch [190/200], Loss: 0.1421\n",
      "Epoch [191/200], Loss: 0.1415\n",
      "Epoch [192/200], Loss: 0.1420\n",
      "Epoch [193/200], Loss: 0.1421\n",
      "Epoch [194/200], Loss: 0.1417\n",
      "Epoch [195/200], Loss: 0.1434\n",
      "Epoch [196/200], Loss: 0.1421\n",
      "Epoch [197/200], Loss: 0.1412\n",
      "Epoch [198/200], Loss: 0.1423\n",
      "Epoch [199/200], Loss: 0.1436\n",
      "Epoch [200/200], Loss: 0.1417\n",
      "Trainer 2 has finished training in 17.77 seconds.\n",
      "AD 2 has finished test evaluation in 9.25 seconds.\n",
      "Epoch [1/200], Loss: 0.5016\n",
      "Epoch [2/200], Loss: 0.4759\n",
      "Epoch [3/200], Loss: 0.4543\n",
      "Epoch [4/200], Loss: 0.4315\n",
      "Epoch [5/200], Loss: 0.4080\n",
      "Epoch [6/200], Loss: 0.3835\n",
      "Epoch [7/200], Loss: 0.3598\n",
      "Epoch [8/200], Loss: 0.3371\n",
      "Epoch [9/200], Loss: 0.3165\n",
      "Epoch [10/200], Loss: 0.2999\n",
      "Epoch [11/200], Loss: 0.2859\n",
      "Epoch [12/200], Loss: 0.2735\n",
      "Epoch [13/200], Loss: 0.2656\n",
      "Epoch [14/200], Loss: 0.2622\n",
      "Epoch [15/200], Loss: 0.2628\n",
      "Epoch [16/200], Loss: 0.2865\n",
      "Epoch [17/200], Loss: 0.2624\n",
      "Epoch [18/200], Loss: 0.2573\n",
      "Epoch [19/200], Loss: 0.2520\n",
      "Epoch [20/200], Loss: 0.2483\n",
      "Epoch [21/200], Loss: 0.2457\n",
      "Epoch [22/200], Loss: 0.2427\n",
      "Epoch [23/200], Loss: 0.2414\n",
      "Epoch [24/200], Loss: 0.2392\n",
      "Epoch [25/200], Loss: 0.2370\n",
      "Epoch [26/200], Loss: 0.2335\n",
      "Epoch [27/200], Loss: 0.2293\n",
      "Epoch [28/200], Loss: 0.2269\n",
      "Epoch [29/200], Loss: 0.2258\n",
      "Epoch [30/200], Loss: 0.2256\n",
      "Epoch [31/200], Loss: 0.2258\n",
      "Epoch [32/200], Loss: 0.2253\n",
      "Epoch [33/200], Loss: 0.2261\n",
      "Epoch [34/200], Loss: 0.2275\n",
      "Epoch [35/200], Loss: 0.2282\n",
      "Epoch [36/200], Loss: 0.2251\n",
      "Epoch [37/200], Loss: 0.2223\n",
      "Epoch [38/200], Loss: 0.2180\n",
      "Epoch [39/200], Loss: 0.2149\n",
      "Epoch [40/200], Loss: 0.2142\n",
      "Epoch [41/200], Loss: 0.2141\n",
      "Epoch [42/200], Loss: 0.2136\n",
      "Epoch [43/200], Loss: 0.2131\n",
      "Epoch [44/200], Loss: 0.2129\n",
      "Epoch [45/200], Loss: 0.2123\n",
      "Epoch [46/200], Loss: 0.2104\n",
      "Epoch [47/200], Loss: 0.2105\n",
      "Epoch [48/200], Loss: 0.2096\n",
      "Epoch [49/200], Loss: 0.2109\n",
      "Epoch [50/200], Loss: 0.2110\n",
      "Epoch [51/200], Loss: 0.2107\n",
      "Epoch [52/200], Loss: 0.2101\n",
      "Epoch [53/200], Loss: 0.2062\n",
      "Epoch [54/200], Loss: 0.2047\n",
      "Epoch [55/200], Loss: 0.2050\n",
      "Epoch [56/200], Loss: 0.2057\n",
      "Epoch [57/200], Loss: 0.2038\n",
      "Epoch [58/200], Loss: 0.2061\n",
      "Epoch [59/200], Loss: 0.2078\n",
      "Epoch [60/200], Loss: 0.2055\n",
      "Epoch [61/200], Loss: 0.2040\n",
      "Epoch [62/200], Loss: 0.2034\n",
      "Epoch [63/200], Loss: 0.2056\n",
      "Epoch [64/200], Loss: 0.2082\n",
      "Epoch [65/200], Loss: 0.2097\n",
      "Epoch [66/200], Loss: 0.2042\n",
      "Epoch [67/200], Loss: 0.2002\n",
      "Epoch [68/200], Loss: 0.1964\n",
      "Epoch [69/200], Loss: 0.1956\n",
      "Epoch [70/200], Loss: 0.1946\n",
      "Epoch [71/200], Loss: 0.1938\n",
      "Epoch [72/200], Loss: 0.1944\n",
      "Epoch [73/200], Loss: 0.1965\n",
      "Epoch [74/200], Loss: 0.1970\n",
      "Epoch [75/200], Loss: 0.1985\n",
      "Epoch [76/200], Loss: 0.1991\n",
      "Epoch [77/200], Loss: 0.1991\n",
      "Epoch [78/200], Loss: 0.1962\n",
      "Epoch [79/200], Loss: 0.1945\n",
      "Epoch [80/200], Loss: 0.1932\n",
      "Epoch [81/200], Loss: 0.1905\n",
      "Epoch [82/200], Loss: 0.1900\n",
      "Epoch [83/200], Loss: 0.1897\n",
      "Epoch [84/200], Loss: 0.1894\n",
      "Epoch [85/200], Loss: 0.1892\n",
      "Epoch [86/200], Loss: 0.1886\n",
      "Epoch [87/200], Loss: 0.1889\n",
      "Epoch [88/200], Loss: 0.1889\n",
      "Epoch [89/200], Loss: 0.1886\n",
      "Epoch [90/200], Loss: 0.1879\n",
      "Epoch [91/200], Loss: 0.1888\n",
      "Epoch [92/200], Loss: 0.1890\n",
      "Epoch [93/200], Loss: 0.1878\n",
      "Epoch [94/200], Loss: 0.1877\n",
      "Epoch [95/200], Loss: 0.1869\n",
      "Epoch [96/200], Loss: 0.1845\n",
      "Epoch [97/200], Loss: 0.1848\n",
      "Epoch [98/200], Loss: 0.1852\n",
      "Epoch [99/200], Loss: 0.1868\n",
      "Epoch [100/200], Loss: 0.1867\n",
      "Epoch [101/200], Loss: 0.1883\n",
      "Epoch [102/200], Loss: 0.1914\n",
      "Epoch [103/200], Loss: 0.1892\n",
      "Epoch [104/200], Loss: 0.1856\n",
      "Epoch [105/200], Loss: 0.1850\n",
      "Epoch [106/200], Loss: 0.1832\n",
      "Epoch [107/200], Loss: 0.1819\n",
      "Epoch [108/200], Loss: 0.1812\n",
      "Epoch [109/200], Loss: 0.1802\n",
      "Epoch [110/200], Loss: 0.1787\n",
      "Epoch [111/200], Loss: 0.1796\n",
      "Epoch [112/200], Loss: 0.1807\n",
      "Epoch [113/200], Loss: 0.1797\n",
      "Epoch [114/200], Loss: 0.1807\n",
      "Epoch [115/200], Loss: 0.1803\n",
      "Epoch [116/200], Loss: 0.1815\n",
      "Epoch [117/200], Loss: 0.1800\n",
      "Epoch [118/200], Loss: 0.1797\n",
      "Epoch [119/200], Loss: 0.1781\n",
      "Epoch [120/200], Loss: 0.1790\n",
      "Epoch [121/200], Loss: 0.1785\n",
      "Epoch [122/200], Loss: 0.1777\n",
      "Epoch [123/200], Loss: 0.1773\n",
      "Epoch [124/200], Loss: 0.1772\n",
      "Epoch [125/200], Loss: 0.1769\n",
      "Epoch [126/200], Loss: 0.1762\n",
      "Epoch [127/200], Loss: 0.1761\n",
      "Epoch [128/200], Loss: 0.1755\n",
      "Epoch [129/200], Loss: 0.1735\n",
      "Epoch [130/200], Loss: 0.1732\n",
      "Epoch [131/200], Loss: 0.1733\n",
      "Epoch [132/200], Loss: 0.1730\n",
      "Epoch [133/200], Loss: 0.1735\n",
      "Epoch [134/200], Loss: 0.1757\n",
      "Epoch [135/200], Loss: 0.1773\n",
      "Epoch [136/200], Loss: 0.1756\n",
      "Epoch [137/200], Loss: 0.1755\n",
      "Epoch [138/200], Loss: 0.1728\n",
      "Epoch [139/200], Loss: 0.1706\n",
      "Epoch [140/200], Loss: 0.1691\n",
      "Epoch [141/200], Loss: 0.1681\n",
      "Epoch [142/200], Loss: 0.1688\n",
      "Epoch [143/200], Loss: 0.1691\n",
      "Epoch [144/200], Loss: 0.1702\n",
      "Epoch [145/200], Loss: 0.1697\n",
      "Epoch [146/200], Loss: 0.1699\n",
      "Epoch [147/200], Loss: 0.1702\n",
      "Epoch [148/200], Loss: 0.1675\n",
      "Epoch [149/200], Loss: 0.1663\n",
      "Epoch [150/200], Loss: 0.1653\n",
      "Epoch [151/200], Loss: 0.1656\n",
      "Epoch [152/200], Loss: 0.1646\n",
      "Epoch [153/200], Loss: 0.1649\n",
      "Epoch [154/200], Loss: 0.1651\n",
      "Epoch [155/200], Loss: 0.1661\n",
      "Epoch [156/200], Loss: 0.1656\n",
      "Epoch [157/200], Loss: 0.1674\n",
      "Epoch [158/200], Loss: 0.1673\n",
      "Epoch [159/200], Loss: 0.1649\n",
      "Epoch [160/200], Loss: 0.1644\n",
      "Epoch [161/200], Loss: 0.1637\n",
      "Epoch [162/200], Loss: 0.1609\n",
      "Epoch [163/200], Loss: 0.1606\n",
      "Epoch [164/200], Loss: 0.1599\n",
      "Epoch [165/200], Loss: 0.1601\n",
      "Epoch [166/200], Loss: 0.1614\n",
      "Epoch [167/200], Loss: 0.1606\n",
      "Epoch [168/200], Loss: 0.1580\n",
      "Epoch [169/200], Loss: 0.1568\n",
      "Epoch [170/200], Loss: 0.1563\n",
      "Epoch [171/200], Loss: 0.1555\n",
      "Epoch [172/200], Loss: 0.1550\n",
      "Epoch [173/200], Loss: 0.1551\n",
      "Epoch [174/200], Loss: 0.1556\n",
      "Epoch [175/200], Loss: 0.1553\n",
      "Epoch [176/200], Loss: 0.1538\n",
      "Epoch [177/200], Loss: 0.1538\n",
      "Epoch [178/200], Loss: 0.1541\n",
      "Epoch [179/200], Loss: 0.1526\n",
      "Epoch [180/200], Loss: 0.1525\n",
      "Epoch [181/200], Loss: 0.1535\n",
      "Epoch [182/200], Loss: 0.1551\n",
      "Epoch [183/200], Loss: 0.1537\n",
      "Epoch [184/200], Loss: 0.1516\n",
      "Epoch [185/200], Loss: 0.1527\n",
      "Epoch [186/200], Loss: 0.1523\n",
      "Epoch [187/200], Loss: 0.1532\n",
      "Epoch [188/200], Loss: 0.1535\n",
      "Epoch [189/200], Loss: 0.1515\n",
      "Epoch [190/200], Loss: 0.1514\n",
      "Epoch [191/200], Loss: 0.1492\n",
      "Epoch [192/200], Loss: 0.1494\n",
      "Epoch [193/200], Loss: 0.1484\n",
      "Epoch [194/200], Loss: 0.1493\n",
      "Epoch [195/200], Loss: 0.1474\n",
      "Epoch [196/200], Loss: 0.1440\n",
      "Epoch [197/200], Loss: 0.1439\n",
      "Epoch [198/200], Loss: 0.1448\n",
      "Epoch [199/200], Loss: 0.1456\n",
      "Epoch [200/200], Loss: 0.1452\n",
      "Trainer 3 has finished training in 23.04 seconds.\n",
      "AD 3 has finished test evaluation in 10.53 seconds.\n",
      "Epoch [1/200], Loss: 0.5072\n",
      "Epoch [2/200], Loss: 0.4805\n",
      "Epoch [3/200], Loss: 0.4637\n",
      "Epoch [4/200], Loss: 0.4465\n",
      "Epoch [5/200], Loss: 0.4272\n",
      "Epoch [6/200], Loss: 0.4066\n",
      "Epoch [7/200], Loss: 0.3846\n",
      "Epoch [8/200], Loss: 0.3615\n",
      "Epoch [9/200], Loss: 0.3384\n",
      "Epoch [10/200], Loss: 0.3156\n",
      "Epoch [11/200], Loss: 0.2943\n",
      "Epoch [12/200], Loss: 0.2779\n",
      "Epoch [13/200], Loss: 0.2690\n",
      "Epoch [14/200], Loss: 0.2574\n",
      "Epoch [15/200], Loss: 0.2572\n",
      "Epoch [16/200], Loss: 0.2607\n",
      "Epoch [17/200], Loss: 0.2531\n",
      "Epoch [18/200], Loss: 0.2474\n",
      "Epoch [19/200], Loss: 0.2462\n",
      "Epoch [20/200], Loss: 0.2415\n",
      "Epoch [21/200], Loss: 0.2367\n",
      "Epoch [22/200], Loss: 0.2326\n",
      "Epoch [23/200], Loss: 0.2291\n",
      "Epoch [24/200], Loss: 0.2268\n",
      "Epoch [25/200], Loss: 0.2258\n",
      "Epoch [26/200], Loss: 0.2249\n",
      "Epoch [27/200], Loss: 0.2272\n",
      "Epoch [28/200], Loss: 0.2296\n",
      "Epoch [29/200], Loss: 0.2304\n",
      "Epoch [30/200], Loss: 0.2279\n",
      "Epoch [31/200], Loss: 0.2250\n",
      "Epoch [32/200], Loss: 0.2234\n",
      "Epoch [33/200], Loss: 0.2207\n",
      "Epoch [34/200], Loss: 0.2171\n",
      "Epoch [35/200], Loss: 0.2148\n",
      "Epoch [36/200], Loss: 0.2135\n",
      "Epoch [37/200], Loss: 0.2121\n",
      "Epoch [38/200], Loss: 0.2109\n",
      "Epoch [39/200], Loss: 0.2111\n",
      "Epoch [40/200], Loss: 0.2107\n",
      "Epoch [41/200], Loss: 0.2086\n",
      "Epoch [42/200], Loss: 0.2070\n",
      "Epoch [43/200], Loss: 0.2061\n",
      "Epoch [44/200], Loss: 0.2058\n",
      "Epoch [45/200], Loss: 0.2053\n",
      "Epoch [46/200], Loss: 0.2044\n",
      "Epoch [47/200], Loss: 0.2037\n",
      "Epoch [48/200], Loss: 0.2010\n",
      "Epoch [49/200], Loss: 0.1998\n",
      "Epoch [50/200], Loss: 0.1997\n",
      "Epoch [51/200], Loss: 0.1992\n",
      "Epoch [52/200], Loss: 0.1999\n",
      "Epoch [53/200], Loss: 0.2014\n",
      "Epoch [54/200], Loss: 0.2024\n",
      "Epoch [55/200], Loss: 0.1995\n",
      "Epoch [56/200], Loss: 0.1959\n",
      "Epoch [57/200], Loss: 0.1930\n",
      "Epoch [58/200], Loss: 0.1915\n",
      "Epoch [59/200], Loss: 0.1901\n",
      "Epoch [60/200], Loss: 0.1903\n",
      "Epoch [61/200], Loss: 0.1905\n",
      "Epoch [62/200], Loss: 0.1905\n",
      "Epoch [63/200], Loss: 0.1911\n",
      "Epoch [64/200], Loss: 0.1929\n",
      "Epoch [65/200], Loss: 0.1943\n",
      "Epoch [66/200], Loss: 0.1937\n",
      "Epoch [67/200], Loss: 0.1926\n",
      "Epoch [68/200], Loss: 0.1915\n",
      "Epoch [69/200], Loss: 0.1869\n",
      "Epoch [70/200], Loss: 0.1838\n",
      "Epoch [71/200], Loss: 0.1818\n",
      "Epoch [72/200], Loss: 0.1796\n",
      "Epoch [73/200], Loss: 0.1793\n",
      "Epoch [74/200], Loss: 0.1800\n",
      "Epoch [75/200], Loss: 0.1800\n",
      "Epoch [76/200], Loss: 0.1809\n",
      "Epoch [77/200], Loss: 0.1813\n",
      "Epoch [78/200], Loss: 0.1804\n",
      "Epoch [79/200], Loss: 0.1814\n",
      "Epoch [80/200], Loss: 0.1812\n",
      "Epoch [81/200], Loss: 0.1807\n",
      "Epoch [82/200], Loss: 0.1791\n",
      "Epoch [83/200], Loss: 0.1773\n",
      "Epoch [84/200], Loss: 0.1778\n",
      "Epoch [85/200], Loss: 0.1767\n",
      "Epoch [86/200], Loss: 0.1748\n",
      "Epoch [87/200], Loss: 0.1739\n",
      "Epoch [88/200], Loss: 0.1728\n",
      "Epoch [89/200], Loss: 0.1721\n",
      "Epoch [90/200], Loss: 0.1716\n",
      "Epoch [91/200], Loss: 0.1713\n",
      "Epoch [92/200], Loss: 0.1715\n",
      "Epoch [93/200], Loss: 0.1732\n",
      "Epoch [94/200], Loss: 0.1719\n",
      "Epoch [95/200], Loss: 0.1687\n",
      "Epoch [96/200], Loss: 0.1666\n",
      "Epoch [97/200], Loss: 0.1650\n",
      "Epoch [98/200], Loss: 0.1655\n",
      "Epoch [99/200], Loss: 0.1661\n",
      "Epoch [100/200], Loss: 0.1659\n",
      "Epoch [101/200], Loss: 0.1664\n",
      "Epoch [102/200], Loss: 0.1670\n",
      "Epoch [103/200], Loss: 0.1670\n",
      "Epoch [104/200], Loss: 0.1667\n",
      "Epoch [105/200], Loss: 0.1651\n",
      "Epoch [106/200], Loss: 0.1643\n",
      "Epoch [107/200], Loss: 0.1636\n",
      "Epoch [108/200], Loss: 0.1637\n",
      "Epoch [109/200], Loss: 0.1637\n",
      "Epoch [110/200], Loss: 0.1641\n",
      "Epoch [111/200], Loss: 0.1629\n",
      "Epoch [112/200], Loss: 0.1619\n",
      "Epoch [113/200], Loss: 0.1614\n",
      "Epoch [114/200], Loss: 0.1608\n",
      "Epoch [115/200], Loss: 0.1604\n",
      "Epoch [116/200], Loss: 0.1623\n",
      "Epoch [117/200], Loss: 0.1629\n",
      "Epoch [118/200], Loss: 0.1589\n",
      "Epoch [119/200], Loss: 0.1560\n",
      "Epoch [120/200], Loss: 0.1551\n",
      "Epoch [121/200], Loss: 0.1534\n",
      "Epoch [122/200], Loss: 0.1536\n",
      "Epoch [123/200], Loss: 0.1541\n",
      "Epoch [124/200], Loss: 0.1524\n",
      "Epoch [125/200], Loss: 0.1519\n",
      "Epoch [126/200], Loss: 0.1502\n",
      "Epoch [127/200], Loss: 0.1502\n",
      "Epoch [128/200], Loss: 0.1500\n",
      "Epoch [129/200], Loss: 0.1493\n",
      "Epoch [130/200], Loss: 0.1485\n",
      "Epoch [131/200], Loss: 0.1505\n",
      "Epoch [132/200], Loss: 0.1507\n",
      "Epoch [133/200], Loss: 0.1499\n",
      "Epoch [134/200], Loss: 0.1498\n",
      "Epoch [135/200], Loss: 0.1478\n",
      "Epoch [136/200], Loss: 0.1496\n",
      "Epoch [137/200], Loss: 0.1525\n",
      "Epoch [138/200], Loss: 0.1519\n",
      "Epoch [139/200], Loss: 0.1474\n",
      "Epoch [140/200], Loss: 0.1427\n",
      "Epoch [141/200], Loss: 0.1400\n",
      "Epoch [142/200], Loss: 0.1387\n",
      "Epoch [143/200], Loss: 0.1386\n",
      "Epoch [144/200], Loss: 0.1422\n",
      "Epoch [145/200], Loss: 0.1421\n",
      "Epoch [146/200], Loss: 0.1412\n",
      "Epoch [147/200], Loss: 0.1423\n",
      "Epoch [148/200], Loss: 0.1415\n",
      "Epoch [149/200], Loss: 0.1400\n",
      "Epoch [150/200], Loss: 0.1409\n",
      "Epoch [151/200], Loss: 0.1370\n",
      "Epoch [152/200], Loss: 0.1360\n",
      "Epoch [153/200], Loss: 0.1352\n",
      "Epoch [154/200], Loss: 0.1375\n",
      "Epoch [155/200], Loss: 0.1341\n",
      "Epoch [156/200], Loss: 0.1332\n",
      "Epoch [157/200], Loss: 0.1323\n",
      "Epoch [158/200], Loss: 0.1343\n",
      "Epoch [159/200], Loss: 0.1307\n",
      "Epoch [160/200], Loss: 0.1301\n",
      "Epoch [161/200], Loss: 0.1330\n",
      "Epoch [162/200], Loss: 0.1360\n",
      "Epoch [163/200], Loss: 0.1382\n",
      "Epoch [164/200], Loss: 0.1408\n",
      "Epoch [165/200], Loss: 0.1404\n",
      "Epoch [166/200], Loss: 0.1418\n",
      "Epoch [167/200], Loss: 0.1374\n",
      "Epoch [168/200], Loss: 0.1342\n",
      "Epoch [169/200], Loss: 0.1323\n",
      "Epoch [170/200], Loss: 0.1288\n",
      "Epoch [171/200], Loss: 0.1285\n",
      "Epoch [172/200], Loss: 0.1260\n",
      "Epoch [173/200], Loss: 0.1252\n",
      "Epoch [174/200], Loss: 0.1256\n",
      "Epoch [175/200], Loss: 0.1279\n",
      "Epoch [176/200], Loss: 0.1233\n",
      "Epoch [177/200], Loss: 0.1203\n",
      "Epoch [178/200], Loss: 0.1208\n",
      "Epoch [179/200], Loss: 0.1207\n",
      "Epoch [180/200], Loss: 0.1237\n",
      "Epoch [181/200], Loss: 0.1257\n",
      "Epoch [182/200], Loss: 0.1288\n",
      "Epoch [183/200], Loss: 0.1348\n",
      "Epoch [184/200], Loss: 0.1285\n",
      "Epoch [185/200], Loss: 0.1229\n",
      "Epoch [186/200], Loss: 0.1223\n",
      "Epoch [187/200], Loss: 0.1209\n",
      "Epoch [188/200], Loss: 0.1190\n",
      "Epoch [189/200], Loss: 0.1201\n",
      "Epoch [190/200], Loss: 0.1162\n",
      "Epoch [191/200], Loss: 0.1135\n",
      "Epoch [192/200], Loss: 0.1126\n",
      "Epoch [193/200], Loss: 0.1135\n",
      "Epoch [194/200], Loss: 0.1131\n",
      "Epoch [195/200], Loss: 0.1159\n",
      "Epoch [196/200], Loss: 0.1139\n",
      "Epoch [197/200], Loss: 0.1137\n",
      "Epoch [198/200], Loss: 0.1146\n",
      "Epoch [199/200], Loss: 0.1169\n",
      "Epoch [200/200], Loss: 0.1169\n",
      "Trainer 4 has finished training in 18.81 seconds.\n",
      "AD 4 has finished test evaluation in 7.68 seconds.\n",
      "Results saved to results_VAD_temp.csv.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "latents_list = [torch.nn.Parameter(torch.randn(num_test_samples, trainers[i].latent_dim).to(device)) for i in range(5)]# for _ in range(5)]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results_VAD_temp.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(200)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=200)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=VADs[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=100, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27147e64-6c63-4528-bc12-99750ba8ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    utils.plot_tsne(train_ds, trainers[i].latents, f\"tsne_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Sample specific vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebb7ef-611d-40c0-a8d6-159c778b684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = nn.Sequential(\n",
    "        nn.Linear(256, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 2048),\n",
    "        nn.BatchNorm1d(2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 784)\n",
    "    )\n",
    "decoder = AutoDecoder.AutoDecoder(arch)\n",
    "trainer = AD_Trainer.AD_Trainer(decoder=decoder, dataloader=train_dl, latent_dim=256, device=device, lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bfd32-1c89-4f4f-9494-9d7f6e7adc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=200)\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "latents = torch.nn.Parameter(torch.randn(num_test_samples, trainer.latent_dim).to(device))\n",
    "opt = optim.Adam([latents], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568406b-c6c5-40af-9339-9825107b7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate_model(model=decoder, test_dl=test_dl, opt=opt, latents=latents, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(latents)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [latents[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "print(\"Sampled Vectors Shape:\", sampled_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "print(\"Random Vectors Shape:\", random_latents_tensor.shape)  # Should be (5, *) depending on your data shape\n",
    "\n",
    "sampled_test_images = decoder(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = decoder(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "print(\"Sampled Images Shape:\", sampled_test_images.shape)  # Should be (5, *) depending on your data shape\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
