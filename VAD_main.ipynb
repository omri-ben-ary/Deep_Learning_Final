{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from VariationalAutoDecoder import VariationalAutoDecoder as VAD\n",
    "from VAD_Trainer import VAD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = [16, 32, 64, 128]\n",
    "betas = [1e5, 5e5, 1e6, 5e6]\n",
    "VADs = [VAD(latent_dim=dim, device=device) for (dim,_) in list(itertools.product(latent_dims, betas))]\n",
    "trainers = [VAD_Trainer(var_decoder=VADs[i], dataloader=train_dl, latent_dim=dim, beta=beta, device=device, lr=1e-3)\n",
    "            for i,(dim,beta) in enumerate(list(itertools.product(latent_dims, betas)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(train_dl.dataset)\n",
    "csv_file_path = 'results_VAD.csv'\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] + ['Final Train Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "for index, trainer in enumerate(trainers):\n",
    "    optimizer = optim.Adam([trainer.latents], lr=1e-3)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = trainer.train(num_epochs=500)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_eval_loss = evaluate_model(model=VADs[index], test_dl=train_dl, opt=optimizer, latents=trainer.latents, epochs=500, device=device) \n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"AD {index} has finished train evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    row = [index] + train_loss + [train_eval_loss]\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27147e64-6c63-4528-bc12-99750ba8ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(trainers)):\n",
    "    latents = VADs[i].reparameterize(trainers[i].latents)\n",
    "    utils.plot_tsne(train_ds, latents, f\"tsne_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033aaf-061c-4bd6-b32e-40466dd1027f",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc862a48-c47c-49ce-b233-71192301ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 154753928.0000\n",
      "Epoch [2/1000], Loss: 150863846.0000\n",
      "Epoch [3/1000], Loss: 148002060.0000\n",
      "Epoch [4/1000], Loss: 145408609.0000\n",
      "Epoch [5/1000], Loss: 142970976.0000\n",
      "Epoch [6/1000], Loss: 140679386.0000\n",
      "Epoch [7/1000], Loss: 138511162.0000\n",
      "Epoch [8/1000], Loss: 136502866.5000\n",
      "Epoch [9/1000], Loss: 134545313.0000\n",
      "Epoch [10/1000], Loss: 132728968.5000\n",
      "Epoch [11/1000], Loss: 131003507.5000\n",
      "Epoch [12/1000], Loss: 129384099.5000\n",
      "Epoch [13/1000], Loss: 127802410.0000\n",
      "Epoch [14/1000], Loss: 126322324.0000\n",
      "Epoch [15/1000], Loss: 124925971.0000\n",
      "Epoch [16/1000], Loss: 123510945.0000\n",
      "Epoch [17/1000], Loss: 122186918.0000\n",
      "Epoch [18/1000], Loss: 120894334.5000\n",
      "Epoch [19/1000], Loss: 119693608.5000\n",
      "Epoch [20/1000], Loss: 118491207.5000\n",
      "Epoch [21/1000], Loss: 117327397.5000\n",
      "Epoch [22/1000], Loss: 116145090.5000\n",
      "Epoch [23/1000], Loss: 114978266.0000\n",
      "Epoch [24/1000], Loss: 113842454.5000\n",
      "Epoch [25/1000], Loss: 112697620.0000\n",
      "Epoch [26/1000], Loss: 111688411.0000\n",
      "Epoch [27/1000], Loss: 110667234.0000\n",
      "Epoch [28/1000], Loss: 109661745.5000\n",
      "Epoch [29/1000], Loss: 108600265.5000\n",
      "Epoch [30/1000], Loss: 107659330.0000\n",
      "Epoch [31/1000], Loss: 106627930.5000\n",
      "Epoch [32/1000], Loss: 105691984.5000\n",
      "Epoch [33/1000], Loss: 104759303.0000\n",
      "Epoch [34/1000], Loss: 103763514.5000\n",
      "Epoch [35/1000], Loss: 102926055.0000\n",
      "Epoch [36/1000], Loss: 101976647.5000\n",
      "Epoch [37/1000], Loss: 101048330.0000\n",
      "Epoch [38/1000], Loss: 100169654.0000\n",
      "Epoch [39/1000], Loss: 99283153.0000\n",
      "Epoch [40/1000], Loss: 98459024.5000\n",
      "Epoch [41/1000], Loss: 97543195.0000\n",
      "Epoch [42/1000], Loss: 96720264.0000\n",
      "Epoch [43/1000], Loss: 95960871.5000\n",
      "Epoch [44/1000], Loss: 95133704.0000\n",
      "Epoch [45/1000], Loss: 94343812.0000\n",
      "Epoch [46/1000], Loss: 93583356.0000\n",
      "Epoch [47/1000], Loss: 92738063.5000\n",
      "Epoch [48/1000], Loss: 92009498.5000\n",
      "Epoch [49/1000], Loss: 91273566.0000\n",
      "Epoch [50/1000], Loss: 90447752.0000\n",
      "Epoch [51/1000], Loss: 89761106.5000\n",
      "Epoch [52/1000], Loss: 88989152.0000\n",
      "Epoch [53/1000], Loss: 88226584.0000\n",
      "Epoch [54/1000], Loss: 87549277.0000\n",
      "Epoch [55/1000], Loss: 86778903.5000\n",
      "Epoch [56/1000], Loss: 86097341.0000\n",
      "Epoch [57/1000], Loss: 85489467.0000\n",
      "Epoch [58/1000], Loss: 84791045.0000\n",
      "Epoch [59/1000], Loss: 84018040.5000\n",
      "Epoch [60/1000], Loss: 83407430.5000\n",
      "Epoch [61/1000], Loss: 82807582.5000\n",
      "Epoch [62/1000], Loss: 82029319.5000\n",
      "Epoch [63/1000], Loss: 81432563.0000\n",
      "Epoch [64/1000], Loss: 80827208.0000\n",
      "Epoch [65/1000], Loss: 80175911.0000\n",
      "Epoch [66/1000], Loss: 79482478.0000\n",
      "Epoch [67/1000], Loss: 78942206.0000\n",
      "Epoch [68/1000], Loss: 78233641.5000\n",
      "Epoch [69/1000], Loss: 77673276.0000\n",
      "Epoch [70/1000], Loss: 77036764.0000\n",
      "Epoch [71/1000], Loss: 76457018.0000\n",
      "Epoch [72/1000], Loss: 75886038.0000\n",
      "Epoch [73/1000], Loss: 75266409.0000\n",
      "Epoch [74/1000], Loss: 74746293.5000\n",
      "Epoch [75/1000], Loss: 74140270.5000\n",
      "Epoch [76/1000], Loss: 73578748.5000\n",
      "Epoch [77/1000], Loss: 72980020.5000\n",
      "Epoch [78/1000], Loss: 72455020.0000\n",
      "Epoch [79/1000], Loss: 71885731.5000\n",
      "Epoch [80/1000], Loss: 71331462.0000\n",
      "Epoch [81/1000], Loss: 70802168.5000\n",
      "Epoch [82/1000], Loss: 70265401.0000\n",
      "Epoch [83/1000], Loss: 69740246.5000\n",
      "Epoch [84/1000], Loss: 69158838.7500\n",
      "Epoch [85/1000], Loss: 68678654.2500\n",
      "Epoch [86/1000], Loss: 68132859.7500\n",
      "Epoch [87/1000], Loss: 67609139.2500\n",
      "Epoch [88/1000], Loss: 67098533.7500\n",
      "Epoch [89/1000], Loss: 66684863.7500\n",
      "Epoch [90/1000], Loss: 66101822.0000\n",
      "Epoch [91/1000], Loss: 65589041.5000\n",
      "Epoch [92/1000], Loss: 65092171.7500\n",
      "Epoch [93/1000], Loss: 64650663.2500\n",
      "Epoch [94/1000], Loss: 64120810.0000\n",
      "Epoch [95/1000], Loss: 63696501.5000\n",
      "Epoch [96/1000], Loss: 63204422.0000\n",
      "Epoch [97/1000], Loss: 62744653.0000\n",
      "Epoch [98/1000], Loss: 62298668.7500\n",
      "Epoch [99/1000], Loss: 61807921.7500\n",
      "Epoch [100/1000], Loss: 61404535.7500\n",
      "Epoch [101/1000], Loss: 60935153.0000\n",
      "Epoch [102/1000], Loss: 60451127.2500\n",
      "Epoch [103/1000], Loss: 60089367.2500\n",
      "Epoch [104/1000], Loss: 59587651.7500\n",
      "Epoch [105/1000], Loss: 59112226.0000\n",
      "Epoch [106/1000], Loss: 58713855.0000\n",
      "Epoch [107/1000], Loss: 58314574.5000\n",
      "Epoch [108/1000], Loss: 57862008.2500\n",
      "Epoch [109/1000], Loss: 57426898.0000\n",
      "Epoch [110/1000], Loss: 57019846.2500\n",
      "Epoch [111/1000], Loss: 56572712.5000\n",
      "Epoch [112/1000], Loss: 56187002.0000\n",
      "Epoch [113/1000], Loss: 55831753.5000\n",
      "Epoch [114/1000], Loss: 55434820.7500\n",
      "Epoch [115/1000], Loss: 55006001.7500\n",
      "Epoch [116/1000], Loss: 54594018.0000\n",
      "Epoch [117/1000], Loss: 54206276.0000\n",
      "Epoch [118/1000], Loss: 53767166.5000\n",
      "Epoch [119/1000], Loss: 53407599.5000\n",
      "Epoch [120/1000], Loss: 53056531.5000\n",
      "Epoch [121/1000], Loss: 52620694.0000\n",
      "Epoch [122/1000], Loss: 52290310.2500\n",
      "Epoch [123/1000], Loss: 51870551.2500\n",
      "Epoch [124/1000], Loss: 51622009.7500\n",
      "Epoch [125/1000], Loss: 51176620.5000\n",
      "Epoch [126/1000], Loss: 50832711.7500\n",
      "Epoch [127/1000], Loss: 50477588.2500\n",
      "Epoch [128/1000], Loss: 50026048.0000\n",
      "Epoch [129/1000], Loss: 49711800.5000\n",
      "Epoch [130/1000], Loss: 49370815.7500\n",
      "Epoch [131/1000], Loss: 49009669.7500\n",
      "Epoch [132/1000], Loss: 48716219.2500\n",
      "Epoch [133/1000], Loss: 48318670.0000\n",
      "Epoch [134/1000], Loss: 47972166.7500\n",
      "Epoch [135/1000], Loss: 47676500.7500\n",
      "Epoch [136/1000], Loss: 47334894.0000\n",
      "Epoch [137/1000], Loss: 46981351.7500\n",
      "Epoch [138/1000], Loss: 46594361.0000\n",
      "Epoch [139/1000], Loss: 46299233.0000\n",
      "Epoch [140/1000], Loss: 45985299.2500\n",
      "Epoch [141/1000], Loss: 45690005.7500\n",
      "Epoch [142/1000], Loss: 45336659.0000\n",
      "Epoch [143/1000], Loss: 44995683.7500\n",
      "Epoch [144/1000], Loss: 44668856.7500\n",
      "Epoch [145/1000], Loss: 44412085.7500\n",
      "Epoch [146/1000], Loss: 44041394.5000\n",
      "Epoch [147/1000], Loss: 43769492.0000\n",
      "Epoch [148/1000], Loss: 43438692.7500\n",
      "Epoch [149/1000], Loss: 43109542.7500\n",
      "Epoch [150/1000], Loss: 42855392.7500\n",
      "Epoch [151/1000], Loss: 42590882.0000\n",
      "Epoch [152/1000], Loss: 42274198.5000\n",
      "Epoch [153/1000], Loss: 41975723.0000\n",
      "Epoch [154/1000], Loss: 41692744.2500\n",
      "Epoch [155/1000], Loss: 41391760.2500\n",
      "Epoch [156/1000], Loss: 41070553.0000\n",
      "Epoch [157/1000], Loss: 40838192.2500\n",
      "Epoch [158/1000], Loss: 40571721.5000\n",
      "Epoch [159/1000], Loss: 40241631.0000\n",
      "Epoch [160/1000], Loss: 39978988.2500\n",
      "Epoch [161/1000], Loss: 39671152.0000\n",
      "Epoch [162/1000], Loss: 39500774.5000\n",
      "Epoch [163/1000], Loss: 39180040.5000\n",
      "Epoch [164/1000], Loss: 38858048.0000\n",
      "Epoch [165/1000], Loss: 38653084.2500\n",
      "Epoch [166/1000], Loss: 38362334.5000\n",
      "Epoch [167/1000], Loss: 38142616.7500\n",
      "Epoch [168/1000], Loss: 37851829.0000\n",
      "Epoch [169/1000], Loss: 37632466.0000\n",
      "Epoch [170/1000], Loss: 37378982.2500\n",
      "Epoch [171/1000], Loss: 37093477.2500\n",
      "Epoch [172/1000], Loss: 36854069.7500\n",
      "Epoch [173/1000], Loss: 36526064.7500\n",
      "Epoch [174/1000], Loss: 36357224.7500\n",
      "Epoch [175/1000], Loss: 36017514.7500\n",
      "Epoch [176/1000], Loss: 35798744.7500\n",
      "Epoch [177/1000], Loss: 35527626.5000\n",
      "Epoch [178/1000], Loss: 35341864.5000\n",
      "Epoch [179/1000], Loss: 35180178.1250\n",
      "Epoch [180/1000], Loss: 34825619.5000\n",
      "Epoch [181/1000], Loss: 34584655.1250\n",
      "Epoch [182/1000], Loss: 34420806.8750\n",
      "Epoch [183/1000], Loss: 34165116.2500\n",
      "Epoch [184/1000], Loss: 33909181.1250\n",
      "Epoch [185/1000], Loss: 33666731.7500\n",
      "Epoch [186/1000], Loss: 33483640.0000\n",
      "Epoch [187/1000], Loss: 33262240.3750\n",
      "Epoch [188/1000], Loss: 32956016.6250\n",
      "Epoch [189/1000], Loss: 32828659.6250\n",
      "Epoch [190/1000], Loss: 32570874.3750\n",
      "Epoch [191/1000], Loss: 32389384.0000\n",
      "Epoch [192/1000], Loss: 32171239.8750\n",
      "Epoch [193/1000], Loss: 31881431.1250\n",
      "Epoch [194/1000], Loss: 31681968.7500\n",
      "Epoch [195/1000], Loss: 31515685.6250\n",
      "Epoch [196/1000], Loss: 31249664.5000\n",
      "Epoch [197/1000], Loss: 31065749.1250\n",
      "Epoch [198/1000], Loss: 30843138.3750\n",
      "Epoch [199/1000], Loss: 30674011.0000\n",
      "Epoch [200/1000], Loss: 30561758.7500\n",
      "Epoch [201/1000], Loss: 30230282.0000\n",
      "Epoch [202/1000], Loss: 30017841.3750\n",
      "Epoch [203/1000], Loss: 29829643.6250\n",
      "Epoch [204/1000], Loss: 29643173.5000\n",
      "Epoch [205/1000], Loss: 29452783.0000\n",
      "Epoch [206/1000], Loss: 29310070.2500\n",
      "Epoch [207/1000], Loss: 29030698.3750\n",
      "Epoch [208/1000], Loss: 28904913.1250\n",
      "Epoch [209/1000], Loss: 28638501.7500\n",
      "Epoch [210/1000], Loss: 28498337.0000\n",
      "Epoch [211/1000], Loss: 28306412.7500\n",
      "Epoch [212/1000], Loss: 28110134.1250\n",
      "Epoch [213/1000], Loss: 27892081.6250\n",
      "Epoch [214/1000], Loss: 27691131.3750\n",
      "Epoch [215/1000], Loss: 27606606.5000\n",
      "Epoch [216/1000], Loss: 27402355.5000\n",
      "Epoch [217/1000], Loss: 27226550.2500\n",
      "Epoch [218/1000], Loss: 27008661.2500\n",
      "Epoch [219/1000], Loss: 26875314.1250\n",
      "Epoch [220/1000], Loss: 26663223.6250\n",
      "Epoch [221/1000], Loss: 26524575.5000\n",
      "Epoch [222/1000], Loss: 26277296.1250\n",
      "Epoch [223/1000], Loss: 26091725.3750\n",
      "Epoch [224/1000], Loss: 25982627.7500\n",
      "Epoch [225/1000], Loss: 25800506.8750\n",
      "Epoch [226/1000], Loss: 25644283.7500\n",
      "Epoch [227/1000], Loss: 25463107.7500\n",
      "Epoch [228/1000], Loss: 25303807.8750\n",
      "Epoch [229/1000], Loss: 25105358.2500\n",
      "Epoch [230/1000], Loss: 24978336.8750\n",
      "Epoch [231/1000], Loss: 24847949.3750\n",
      "Epoch [232/1000], Loss: 24624543.0000\n",
      "Epoch [233/1000], Loss: 24467027.3750\n",
      "Epoch [234/1000], Loss: 24292985.2500\n",
      "Epoch [235/1000], Loss: 24190966.0000\n",
      "Epoch [236/1000], Loss: 24003505.8750\n",
      "Epoch [237/1000], Loss: 23838793.5000\n",
      "Epoch [238/1000], Loss: 23698466.2500\n",
      "Epoch [239/1000], Loss: 23587178.7500\n",
      "Epoch [240/1000], Loss: 23445390.7500\n",
      "Epoch [241/1000], Loss: 23297990.0000\n",
      "Epoch [242/1000], Loss: 23119028.8750\n",
      "Epoch [243/1000], Loss: 22957827.0000\n",
      "Epoch [244/1000], Loss: 22846630.8750\n",
      "Epoch [245/1000], Loss: 22686940.2500\n",
      "Epoch [246/1000], Loss: 22518055.2500\n",
      "Epoch [247/1000], Loss: 22336940.0000\n",
      "Epoch [248/1000], Loss: 22213611.2500\n",
      "Epoch [249/1000], Loss: 22092766.5000\n",
      "Epoch [250/1000], Loss: 21939026.7500\n",
      "Epoch [251/1000], Loss: 21814057.8750\n",
      "Epoch [252/1000], Loss: 21673642.8750\n",
      "Epoch [253/1000], Loss: 21563804.8750\n",
      "Epoch [254/1000], Loss: 21336447.7500\n",
      "Epoch [255/1000], Loss: 21215963.2500\n",
      "Epoch [256/1000], Loss: 21051428.2500\n",
      "Epoch [257/1000], Loss: 21040945.2500\n",
      "Epoch [258/1000], Loss: 20870775.6250\n",
      "Epoch [259/1000], Loss: 20724263.3750\n",
      "Epoch [260/1000], Loss: 20634205.5000\n",
      "Epoch [261/1000], Loss: 20520085.7500\n",
      "Epoch [262/1000], Loss: 20386846.0000\n",
      "Epoch [263/1000], Loss: 20175633.3750\n",
      "Epoch [264/1000], Loss: 20103087.6250\n",
      "Epoch [265/1000], Loss: 19915555.1250\n",
      "Epoch [266/1000], Loss: 19793112.5000\n",
      "Epoch [267/1000], Loss: 19663549.7500\n",
      "Epoch [268/1000], Loss: 19560249.3750\n",
      "Epoch [269/1000], Loss: 19499600.8750\n",
      "Epoch [270/1000], Loss: 19301802.2500\n",
      "Epoch [271/1000], Loss: 19219750.3750\n",
      "Epoch [272/1000], Loss: 19156792.2500\n",
      "Epoch [273/1000], Loss: 18984490.1250\n",
      "Epoch [274/1000], Loss: 18872292.0000\n",
      "Epoch [275/1000], Loss: 18714671.6250\n",
      "Epoch [276/1000], Loss: 18589130.7500\n",
      "Epoch [277/1000], Loss: 18461997.1250\n",
      "Epoch [278/1000], Loss: 18325977.0000\n",
      "Epoch [279/1000], Loss: 18299235.3750\n",
      "Epoch [280/1000], Loss: 18111336.2500\n",
      "Epoch [281/1000], Loss: 18040616.0000\n",
      "Epoch [282/1000], Loss: 17918826.0000\n",
      "Epoch [283/1000], Loss: 17874784.7500\n",
      "Epoch [284/1000], Loss: 17692834.6250\n",
      "Epoch [285/1000], Loss: 17529625.3750\n",
      "Epoch [286/1000], Loss: 17494179.1875\n",
      "Epoch [287/1000], Loss: 17336995.1250\n",
      "Epoch [288/1000], Loss: 17307230.7500\n",
      "Epoch [289/1000], Loss: 17091453.1875\n",
      "Epoch [290/1000], Loss: 17017898.3125\n",
      "Epoch [291/1000], Loss: 16950361.5625\n",
      "Epoch [292/1000], Loss: 16820381.9375\n",
      "Epoch [293/1000], Loss: 16751295.5000\n",
      "Epoch [294/1000], Loss: 16699134.0000\n",
      "Epoch [295/1000], Loss: 16519193.5625\n",
      "Epoch [296/1000], Loss: 16406320.8125\n",
      "Epoch [297/1000], Loss: 16341165.6250\n",
      "Epoch [298/1000], Loss: 16249158.2500\n",
      "Epoch [299/1000], Loss: 16160395.1875\n",
      "Epoch [300/1000], Loss: 16040163.8125\n",
      "Epoch [301/1000], Loss: 15920277.7500\n",
      "Epoch [302/1000], Loss: 15864774.2500\n",
      "Epoch [303/1000], Loss: 15760969.6875\n",
      "Epoch [304/1000], Loss: 15644389.0625\n",
      "Epoch [305/1000], Loss: 15609360.2500\n",
      "Epoch [306/1000], Loss: 15478343.7500\n",
      "Epoch [307/1000], Loss: 15294498.7500\n",
      "Epoch [308/1000], Loss: 15342045.7500\n",
      "Epoch [309/1000], Loss: 15172808.0000\n",
      "Epoch [310/1000], Loss: 15070072.7500\n",
      "Epoch [311/1000], Loss: 14988876.1875\n",
      "Epoch [312/1000], Loss: 14918522.6875\n",
      "Epoch [313/1000], Loss: 14821063.8125\n",
      "Epoch [314/1000], Loss: 14721705.2500\n",
      "Epoch [315/1000], Loss: 14666539.7500\n",
      "Epoch [316/1000], Loss: 14560333.6250\n",
      "Epoch [317/1000], Loss: 14499591.2500\n",
      "Epoch [318/1000], Loss: 14349844.9375\n",
      "Epoch [319/1000], Loss: 14318400.8125\n",
      "Epoch [320/1000], Loss: 14235428.3750\n",
      "Epoch [321/1000], Loss: 14133536.3750\n",
      "Epoch [322/1000], Loss: 14069714.5000\n",
      "Epoch [323/1000], Loss: 13914616.6250\n",
      "Epoch [324/1000], Loss: 13924945.2500\n",
      "Epoch [325/1000], Loss: 13790771.3125\n",
      "Epoch [326/1000], Loss: 13777763.3750\n",
      "Epoch [327/1000], Loss: 13640770.8125\n",
      "Epoch [328/1000], Loss: 13625270.0000\n",
      "Epoch [329/1000], Loss: 13509012.5000\n",
      "Epoch [330/1000], Loss: 13415228.0000\n",
      "Epoch [331/1000], Loss: 13294680.9375\n",
      "Epoch [332/1000], Loss: 13242000.0000\n",
      "Epoch [333/1000], Loss: 13179994.8125\n",
      "Epoch [334/1000], Loss: 13073145.0625\n",
      "Epoch [335/1000], Loss: 13034113.4375\n",
      "Epoch [336/1000], Loss: 12938260.2500\n",
      "Epoch [337/1000], Loss: 12887086.6250\n",
      "Epoch [338/1000], Loss: 12794517.6875\n",
      "Epoch [339/1000], Loss: 12776456.8750\n",
      "Epoch [340/1000], Loss: 12622709.5625\n",
      "Epoch [341/1000], Loss: 12596698.6250\n",
      "Epoch [342/1000], Loss: 12549798.9375\n",
      "Epoch [343/1000], Loss: 12436150.8125\n",
      "Epoch [344/1000], Loss: 12334623.0000\n",
      "Epoch [345/1000], Loss: 12218472.8125\n",
      "Epoch [346/1000], Loss: 12199217.5000\n",
      "Epoch [347/1000], Loss: 12183796.6875\n",
      "Epoch [348/1000], Loss: 12033692.3125\n",
      "Epoch [349/1000], Loss: 11985928.9375\n",
      "Epoch [350/1000], Loss: 11926669.1875\n",
      "Epoch [351/1000], Loss: 11881950.8750\n",
      "Epoch [352/1000], Loss: 11693856.9375\n",
      "Epoch [353/1000], Loss: 11827169.3125\n",
      "Epoch [354/1000], Loss: 11690422.2500\n",
      "Epoch [355/1000], Loss: 11626259.9375\n",
      "Epoch [356/1000], Loss: 11510060.8750\n",
      "Epoch [357/1000], Loss: 11468006.2500\n",
      "Epoch [358/1000], Loss: 11395447.0625\n",
      "Epoch [359/1000], Loss: 11381897.4375\n",
      "Epoch [360/1000], Loss: 11286060.1250\n",
      "Epoch [361/1000], Loss: 11206093.1875\n",
      "Epoch [362/1000], Loss: 11174518.9375\n",
      "Epoch [363/1000], Loss: 11088347.1875\n",
      "Epoch [364/1000], Loss: 11094754.2500\n",
      "Epoch [365/1000], Loss: 11045026.5625\n",
      "Epoch [366/1000], Loss: 10991697.7500\n",
      "Epoch [367/1000], Loss: 10879902.5625\n",
      "Epoch [368/1000], Loss: 10837198.6250\n",
      "Epoch [369/1000], Loss: 10688740.0000\n",
      "Epoch [370/1000], Loss: 10721203.1250\n",
      "Epoch [371/1000], Loss: 10652906.5625\n",
      "Epoch [372/1000], Loss: 10617848.5625\n",
      "Epoch [373/1000], Loss: 10542015.7500\n",
      "Epoch [374/1000], Loss: 10522164.6250\n",
      "Epoch [375/1000], Loss: 10374086.3750\n",
      "Epoch [376/1000], Loss: 10344115.2500\n",
      "Epoch [377/1000], Loss: 10306835.8750\n",
      "Epoch [378/1000], Loss: 10232274.9375\n",
      "Epoch [379/1000], Loss: 10208629.5625\n",
      "Epoch [380/1000], Loss: 10108232.2500\n",
      "Epoch [381/1000], Loss: 10075012.6250\n",
      "Epoch [382/1000], Loss: 10038134.6875\n",
      "Epoch [383/1000], Loss: 9975713.0000\n",
      "Epoch [384/1000], Loss: 9897066.7500\n",
      "Epoch [385/1000], Loss: 9900950.8125\n",
      "Epoch [386/1000], Loss: 9836915.5625\n",
      "Epoch [387/1000], Loss: 9759935.4375\n",
      "Epoch [388/1000], Loss: 9703615.6250\n",
      "Epoch [389/1000], Loss: 9712703.6875\n",
      "Epoch [390/1000], Loss: 9701571.8125\n",
      "Epoch [391/1000], Loss: 9566724.4375\n",
      "Epoch [392/1000], Loss: 9523093.3750\n",
      "Epoch [393/1000], Loss: 9446111.4375\n",
      "Epoch [394/1000], Loss: 9469641.4375\n",
      "Epoch [395/1000], Loss: 9350210.5625\n",
      "Epoch [396/1000], Loss: 9388484.4375\n",
      "Epoch [397/1000], Loss: 9327378.6250\n",
      "Epoch [398/1000], Loss: 9193925.8750\n",
      "Epoch [399/1000], Loss: 9221193.8125\n",
      "Epoch [400/1000], Loss: 9174817.7500\n",
      "Epoch [401/1000], Loss: 9126317.4375\n",
      "Epoch [402/1000], Loss: 9037996.0000\n",
      "Epoch [403/1000], Loss: 9094456.6875\n",
      "Epoch [404/1000], Loss: 8986042.1875\n",
      "Epoch [405/1000], Loss: 8881533.0625\n",
      "Epoch [406/1000], Loss: 8904020.4375\n",
      "Epoch [407/1000], Loss: 8785670.6250\n",
      "Epoch [408/1000], Loss: 8763029.9062\n",
      "Epoch [409/1000], Loss: 8805988.5938\n",
      "Epoch [410/1000], Loss: 8712258.9375\n",
      "Epoch [411/1000], Loss: 8657466.1250\n",
      "Epoch [412/1000], Loss: 8622955.4688\n",
      "Epoch [413/1000], Loss: 8608408.4688\n",
      "Epoch [414/1000], Loss: 8545587.6250\n",
      "Epoch [415/1000], Loss: 8562708.8750\n",
      "Epoch [416/1000], Loss: 8452125.0625\n",
      "Epoch [417/1000], Loss: 8428624.7188\n",
      "Epoch [418/1000], Loss: 8379917.8125\n",
      "Epoch [419/1000], Loss: 8397165.8750\n",
      "Epoch [420/1000], Loss: 8283820.9688\n",
      "Epoch [421/1000], Loss: 8422854.0312\n",
      "Epoch [422/1000], Loss: 8186473.0938\n",
      "Epoch [423/1000], Loss: 8202881.4375\n",
      "Epoch [424/1000], Loss: 8120129.4062\n",
      "Epoch [425/1000], Loss: 8144824.9688\n",
      "Epoch [426/1000], Loss: 8053113.1250\n",
      "Epoch [427/1000], Loss: 8055025.0938\n",
      "Epoch [428/1000], Loss: 8050974.1250\n",
      "Epoch [429/1000], Loss: 8049135.7188\n",
      "Epoch [430/1000], Loss: 7935242.7188\n",
      "Epoch [431/1000], Loss: 7970018.9688\n",
      "Epoch [432/1000], Loss: 7882674.5000\n",
      "Epoch [433/1000], Loss: 7907158.6562\n",
      "Epoch [434/1000], Loss: 7877802.3438\n",
      "Epoch [435/1000], Loss: 7793285.2188\n",
      "Epoch [436/1000], Loss: 7695072.5312\n",
      "Epoch [437/1000], Loss: 7760865.6250\n",
      "Epoch [438/1000], Loss: 7733700.5312\n",
      "Epoch [439/1000], Loss: 7615085.4062\n",
      "Epoch [440/1000], Loss: 7621346.4688\n",
      "Epoch [441/1000], Loss: 7564162.3750\n",
      "Epoch [442/1000], Loss: 7537888.2812\n",
      "Epoch [443/1000], Loss: 7452035.8438\n",
      "Epoch [444/1000], Loss: 7473866.5625\n",
      "Epoch [445/1000], Loss: 7408020.0000\n",
      "Epoch [446/1000], Loss: 7487107.9375\n",
      "Epoch [447/1000], Loss: 7510555.1562\n",
      "Epoch [448/1000], Loss: 7326377.7188\n",
      "Epoch [449/1000], Loss: 7334599.2500\n",
      "Epoch [450/1000], Loss: 7350517.4062\n",
      "Epoch [451/1000], Loss: 7279432.7500\n",
      "Epoch [452/1000], Loss: 7261364.9375\n",
      "Epoch [453/1000], Loss: 7240206.2188\n",
      "Epoch [454/1000], Loss: 7192440.2188\n",
      "Epoch [455/1000], Loss: 7260720.8125\n",
      "Epoch [456/1000], Loss: 7089716.4062\n",
      "Epoch [457/1000], Loss: 7078233.0625\n",
      "Epoch [458/1000], Loss: 7078130.5625\n",
      "Epoch [459/1000], Loss: 7010443.9062\n",
      "Epoch [460/1000], Loss: 7078483.3438\n",
      "Epoch [461/1000], Loss: 6995276.8125\n",
      "Epoch [462/1000], Loss: 7002070.5938\n",
      "Epoch [463/1000], Loss: 6936616.4375\n",
      "Epoch [464/1000], Loss: 6926214.5312\n",
      "Epoch [465/1000], Loss: 6956844.7812\n",
      "Epoch [466/1000], Loss: 6886858.8438\n",
      "Epoch [467/1000], Loss: 6859040.0938\n",
      "Epoch [468/1000], Loss: 6820811.4062\n",
      "Epoch [469/1000], Loss: 6816012.2500\n",
      "Epoch [470/1000], Loss: 6746110.2188\n",
      "Epoch [471/1000], Loss: 6758465.4375\n",
      "Epoch [472/1000], Loss: 6767406.5938\n",
      "Epoch [473/1000], Loss: 6679030.2188\n",
      "Epoch [474/1000], Loss: 6593414.2812\n",
      "Epoch [475/1000], Loss: 6618436.9688\n",
      "Epoch [476/1000], Loss: 6598490.9688\n",
      "Epoch [477/1000], Loss: 6609976.6875\n",
      "Epoch [478/1000], Loss: 6563027.0312\n",
      "Epoch [479/1000], Loss: 6501465.0938\n",
      "Epoch [480/1000], Loss: 6499313.2188\n",
      "Epoch [481/1000], Loss: 6504009.8125\n",
      "Epoch [482/1000], Loss: 6479415.0938\n",
      "Epoch [483/1000], Loss: 6478238.2188\n",
      "Epoch [484/1000], Loss: 6417645.0312\n",
      "Epoch [485/1000], Loss: 6437138.8750\n",
      "Epoch [486/1000], Loss: 6332104.1250\n",
      "Epoch [487/1000], Loss: 6378114.4062\n",
      "Epoch [488/1000], Loss: 6311684.4375\n",
      "Epoch [489/1000], Loss: 6377580.8125\n",
      "Epoch [490/1000], Loss: 6279541.1250\n",
      "Epoch [491/1000], Loss: 6300064.1875\n",
      "Epoch [492/1000], Loss: 6204538.3750\n",
      "Epoch [493/1000], Loss: 6270780.6250\n",
      "Epoch [494/1000], Loss: 6228176.8438\n",
      "Epoch [495/1000], Loss: 6189040.2812\n",
      "Epoch [496/1000], Loss: 6174108.0000\n",
      "Epoch [497/1000], Loss: 6148019.5625\n",
      "Epoch [498/1000], Loss: 6103582.0000\n",
      "Epoch [499/1000], Loss: 6193477.6562\n",
      "Epoch [500/1000], Loss: 6085391.2500\n",
      "Epoch [501/1000], Loss: 6024148.7188\n",
      "Epoch [502/1000], Loss: 6020906.3125\n",
      "Epoch [503/1000], Loss: 6071713.4062\n",
      "Epoch [504/1000], Loss: 6075948.1562\n",
      "Epoch [505/1000], Loss: 6041773.6875\n",
      "Epoch [506/1000], Loss: 5932571.6562\n",
      "Epoch [507/1000], Loss: 5959291.5625\n",
      "Epoch [508/1000], Loss: 5902380.8750\n",
      "Epoch [509/1000], Loss: 5958644.4062\n",
      "Epoch [510/1000], Loss: 5951873.6250\n",
      "Epoch [511/1000], Loss: 5909866.2500\n",
      "Epoch [512/1000], Loss: 5868412.2500\n",
      "Epoch [513/1000], Loss: 5817603.9062\n",
      "Epoch [514/1000], Loss: 5870867.9688\n",
      "Epoch [515/1000], Loss: 5825697.4375\n",
      "Epoch [516/1000], Loss: 5894520.3125\n",
      "Epoch [517/1000], Loss: 5835840.4062\n",
      "Epoch [518/1000], Loss: 5826878.0312\n",
      "Epoch [519/1000], Loss: 5758214.2812\n",
      "Epoch [520/1000], Loss: 5787376.7500\n",
      "Epoch [521/1000], Loss: 5739607.4688\n",
      "Epoch [522/1000], Loss: 5739346.4688\n",
      "Epoch [523/1000], Loss: 5802003.3125\n",
      "Epoch [524/1000], Loss: 5679858.3125\n",
      "Epoch [525/1000], Loss: 5650821.3438\n",
      "Epoch [526/1000], Loss: 5698232.0938\n",
      "Epoch [527/1000], Loss: 5668560.6250\n",
      "Epoch [528/1000], Loss: 5610026.3750\n",
      "Epoch [529/1000], Loss: 5646786.3750\n",
      "Epoch [530/1000], Loss: 5599703.5938\n",
      "Epoch [531/1000], Loss: 5583274.6250\n",
      "Epoch [532/1000], Loss: 5577580.1250\n",
      "Epoch [533/1000], Loss: 5565727.2188\n",
      "Epoch [534/1000], Loss: 5577271.6250\n",
      "Epoch [535/1000], Loss: 5542685.1875\n",
      "Epoch [536/1000], Loss: 5562789.4688\n",
      "Epoch [537/1000], Loss: 5525042.4375\n",
      "Epoch [538/1000], Loss: 5499866.3750\n",
      "Epoch [539/1000], Loss: 5478172.3125\n",
      "Epoch [540/1000], Loss: 5482838.2500\n",
      "Epoch [541/1000], Loss: 5455431.4062\n",
      "Epoch [542/1000], Loss: 5478657.1250\n",
      "Epoch [543/1000], Loss: 5440882.2812\n",
      "Epoch [544/1000], Loss: 5355220.5625\n",
      "Epoch [545/1000], Loss: 5387476.7500\n",
      "Epoch [546/1000], Loss: 5357218.4375\n",
      "Epoch [547/1000], Loss: 5380661.7188\n",
      "Epoch [548/1000], Loss: 5301851.7500\n",
      "Epoch [549/1000], Loss: 5343086.5625\n",
      "Epoch [550/1000], Loss: 5325796.1562\n",
      "Epoch [551/1000], Loss: 5398231.8750\n",
      "Epoch [552/1000], Loss: 5378063.0312\n",
      "Epoch [553/1000], Loss: 5312873.8438\n",
      "Epoch [554/1000], Loss: 5312459.9688\n",
      "Epoch [555/1000], Loss: 5251420.8438\n",
      "Epoch [556/1000], Loss: 5306774.7500\n",
      "Epoch [557/1000], Loss: 5209246.5938\n",
      "Epoch [558/1000], Loss: 5269779.0000\n",
      "Epoch [559/1000], Loss: 5251394.9688\n",
      "Epoch [560/1000], Loss: 5224474.5312\n",
      "Epoch [561/1000], Loss: 5179224.7812\n",
      "Epoch [562/1000], Loss: 5162144.5312\n",
      "Epoch [563/1000], Loss: 5198662.2500\n",
      "Epoch [564/1000], Loss: 5151624.0938\n",
      "Epoch [565/1000], Loss: 5122874.9375\n",
      "Epoch [566/1000], Loss: 5122050.1562\n",
      "Epoch [567/1000], Loss: 5170624.9062\n",
      "Epoch [568/1000], Loss: 5136657.5938\n",
      "Epoch [569/1000], Loss: 5101075.0625\n",
      "Epoch [570/1000], Loss: 5141158.6562\n",
      "Epoch [571/1000], Loss: 5077195.9375\n",
      "Epoch [572/1000], Loss: 5012293.1875\n",
      "Epoch [573/1000], Loss: 5124616.8438\n",
      "Epoch [574/1000], Loss: 5108271.0000\n",
      "Epoch [575/1000], Loss: 5062229.1250\n",
      "Epoch [576/1000], Loss: 5133198.0938\n",
      "Epoch [577/1000], Loss: 5113464.1250\n",
      "Epoch [578/1000], Loss: 5075100.4375\n",
      "Epoch [579/1000], Loss: 5093099.5625\n",
      "Epoch [580/1000], Loss: 5078349.8438\n",
      "Epoch [581/1000], Loss: 5006518.2188\n",
      "Epoch [582/1000], Loss: 4945518.2188\n",
      "Epoch [583/1000], Loss: 4980935.2500\n",
      "Epoch [584/1000], Loss: 4980891.4375\n",
      "Epoch [585/1000], Loss: 5020940.6875\n",
      "Epoch [586/1000], Loss: 4995478.9688\n",
      "Epoch [587/1000], Loss: 4970480.0000\n",
      "Epoch [588/1000], Loss: 4948802.0000\n",
      "Epoch [589/1000], Loss: 5007317.8125\n",
      "Epoch [590/1000], Loss: 5017880.6250\n",
      "Epoch [591/1000], Loss: 4937887.2812\n",
      "Epoch [592/1000], Loss: 4961885.9688\n",
      "Epoch [593/1000], Loss: 4859569.0625\n",
      "Epoch [594/1000], Loss: 4977070.3125\n",
      "Epoch [595/1000], Loss: 4888238.1562\n",
      "Epoch [596/1000], Loss: 4943621.2500\n",
      "Epoch [597/1000], Loss: 4895439.4375\n",
      "Epoch [598/1000], Loss: 4905496.0938\n",
      "Epoch [599/1000], Loss: 4920806.2812\n",
      "Epoch [600/1000], Loss: 4860390.0312\n",
      "Epoch [601/1000], Loss: 4862126.6875\n",
      "Epoch [602/1000], Loss: 4925747.4375\n",
      "Epoch [603/1000], Loss: 4840326.0312\n",
      "Epoch [604/1000], Loss: 4814716.8438\n",
      "Epoch [605/1000], Loss: 4828042.1562\n",
      "Epoch [606/1000], Loss: 4876837.8750\n",
      "Epoch [607/1000], Loss: 4809324.2812\n",
      "Epoch [608/1000], Loss: 4876220.2812\n",
      "Epoch [609/1000], Loss: 4878172.2500\n",
      "Epoch [610/1000], Loss: 4800793.9375\n",
      "Epoch [611/1000], Loss: 4761930.5625\n",
      "Epoch [612/1000], Loss: 4786472.7812\n",
      "Epoch [613/1000], Loss: 4715312.8125\n",
      "Epoch [614/1000], Loss: 4778695.3438\n",
      "Epoch [615/1000], Loss: 4736125.1875\n",
      "Epoch [616/1000], Loss: 4716206.2812\n",
      "Epoch [617/1000], Loss: 4739018.8438\n",
      "Epoch [618/1000], Loss: 4835333.0625\n",
      "Epoch [619/1000], Loss: 4748851.1875\n",
      "Epoch [620/1000], Loss: 4769536.1250\n",
      "Epoch [621/1000], Loss: 4754766.4375\n",
      "Epoch [622/1000], Loss: 4741707.1562\n",
      "Epoch [623/1000], Loss: 4719352.1875\n",
      "Epoch [624/1000], Loss: 4671242.8750\n",
      "Epoch [625/1000], Loss: 4706006.5938\n",
      "Epoch [626/1000], Loss: 4684322.6875\n",
      "Epoch [627/1000], Loss: 4644147.3750\n",
      "Epoch [628/1000], Loss: 4667761.4062\n",
      "Epoch [629/1000], Loss: 4738436.8438\n",
      "Epoch [630/1000], Loss: 4669168.3125\n",
      "Epoch [631/1000], Loss: 4633899.7500\n",
      "Epoch [632/1000], Loss: 4670335.0625\n",
      "Epoch [633/1000], Loss: 4637448.7500\n",
      "Epoch [634/1000], Loss: 4721592.7031\n",
      "Epoch [635/1000], Loss: 4625372.8438\n",
      "Epoch [636/1000], Loss: 4683428.4062\n",
      "Epoch [637/1000], Loss: 4672091.6562\n",
      "Epoch [638/1000], Loss: 4681893.0000\n",
      "Epoch [639/1000], Loss: 4589980.3750\n",
      "Epoch [640/1000], Loss: 4602758.0625\n",
      "Epoch [641/1000], Loss: 4602996.0625\n",
      "Epoch [642/1000], Loss: 4586664.5312\n",
      "Epoch [643/1000], Loss: 4618519.7188\n",
      "Epoch [644/1000], Loss: 4596961.9531\n",
      "Epoch [645/1000], Loss: 4665556.5625\n",
      "Epoch [646/1000], Loss: 4630579.5781\n",
      "Epoch [647/1000], Loss: 4578935.4688\n",
      "Epoch [648/1000], Loss: 4552459.0156\n",
      "Epoch [649/1000], Loss: 4674233.1250\n",
      "Epoch [650/1000], Loss: 4550590.7812\n",
      "Epoch [651/1000], Loss: 4580199.3750\n",
      "Epoch [652/1000], Loss: 4574037.2188\n",
      "Epoch [653/1000], Loss: 4583333.3438\n",
      "Epoch [654/1000], Loss: 4560189.3750\n",
      "Epoch [655/1000], Loss: 4571504.9375\n",
      "Epoch [656/1000], Loss: 4549128.3125\n",
      "Epoch [657/1000], Loss: 4541309.6406\n",
      "Epoch [658/1000], Loss: 4507986.5625\n",
      "Epoch [659/1000], Loss: 4570622.1875\n",
      "Epoch [660/1000], Loss: 4578932.9375\n",
      "Epoch [661/1000], Loss: 4528316.9844\n",
      "Epoch [662/1000], Loss: 4505195.7500\n",
      "Epoch [663/1000], Loss: 4522383.3125\n",
      "Epoch [664/1000], Loss: 4515755.0469\n",
      "Epoch [665/1000], Loss: 4587910.7500\n",
      "Epoch [666/1000], Loss: 4532861.3906\n",
      "Epoch [667/1000], Loss: 4489674.4688\n",
      "Epoch [668/1000], Loss: 4520947.0469\n",
      "Epoch [669/1000], Loss: 4446890.0000\n",
      "Epoch [670/1000], Loss: 4514093.4219\n",
      "Epoch [671/1000], Loss: 4525995.7500\n",
      "Epoch [672/1000], Loss: 4476371.8750\n",
      "Epoch [673/1000], Loss: 4475652.5312\n",
      "Epoch [674/1000], Loss: 4498304.4688\n",
      "Epoch [675/1000], Loss: 4525385.6250\n",
      "Epoch [676/1000], Loss: 4434403.8906\n",
      "Epoch [677/1000], Loss: 4414961.7031\n",
      "Epoch [678/1000], Loss: 4499617.7812\n",
      "Epoch [679/1000], Loss: 4381057.0312\n",
      "Epoch [680/1000], Loss: 4515832.3281\n",
      "Epoch [681/1000], Loss: 4525222.2500\n",
      "Epoch [682/1000], Loss: 4480721.6250\n",
      "Epoch [683/1000], Loss: 4414778.1250\n",
      "Epoch [684/1000], Loss: 4453609.9062\n",
      "Epoch [685/1000], Loss: 4421884.5000\n",
      "Epoch [686/1000], Loss: 4392427.6250\n",
      "Epoch [687/1000], Loss: 4425119.2969\n",
      "Epoch [688/1000], Loss: 4368121.1875\n",
      "Epoch [689/1000], Loss: 4357943.7188\n",
      "Epoch [690/1000], Loss: 4441804.8125\n",
      "Epoch [691/1000], Loss: 4367374.0312\n",
      "Epoch [692/1000], Loss: 4393342.4688\n",
      "Epoch [693/1000], Loss: 4421764.8594\n",
      "Epoch [694/1000], Loss: 4433843.7500\n",
      "Epoch [695/1000], Loss: 4489829.2656\n",
      "Epoch [696/1000], Loss: 4405269.9062\n",
      "Epoch [697/1000], Loss: 4444322.9219\n",
      "Epoch [698/1000], Loss: 4331740.4219\n",
      "Epoch [699/1000], Loss: 4428744.7188\n",
      "Epoch [700/1000], Loss: 4395950.0000\n",
      "Epoch [701/1000], Loss: 4307717.9844\n",
      "Epoch [702/1000], Loss: 4394842.3281\n",
      "Epoch [703/1000], Loss: 4400622.2812\n",
      "Epoch [704/1000], Loss: 4348958.2812\n",
      "Epoch [705/1000], Loss: 4318995.1562\n",
      "Epoch [706/1000], Loss: 4373079.7344\n",
      "Epoch [707/1000], Loss: 4339412.5781\n",
      "Epoch [708/1000], Loss: 4328629.1094\n",
      "Epoch [709/1000], Loss: 4362289.8750\n",
      "Epoch [710/1000], Loss: 4431088.0312\n",
      "Epoch [711/1000], Loss: 4403084.9531\n",
      "Epoch [712/1000], Loss: 4320570.1562\n",
      "Epoch [713/1000], Loss: 4308800.0625\n",
      "Epoch [714/1000], Loss: 4346977.0469\n",
      "Epoch [715/1000], Loss: 4343095.5156\n",
      "Epoch [716/1000], Loss: 4315274.6875\n",
      "Epoch [717/1000], Loss: 4345513.8906\n",
      "Epoch [718/1000], Loss: 4362098.1719\n",
      "Epoch [719/1000], Loss: 4388520.9219\n",
      "Epoch [720/1000], Loss: 4316821.5469\n",
      "Epoch [721/1000], Loss: 4348333.1719\n",
      "Epoch [722/1000], Loss: 4287454.1250\n",
      "Epoch [723/1000], Loss: 4319889.4531\n",
      "Epoch [724/1000], Loss: 4391976.0938\n",
      "Epoch [725/1000], Loss: 4276397.7344\n",
      "Epoch [726/1000], Loss: 4356188.4062\n",
      "Epoch [727/1000], Loss: 4326876.6562\n",
      "Epoch [728/1000], Loss: 4299971.5938\n",
      "Epoch [729/1000], Loss: 4386616.2188\n",
      "Epoch [730/1000], Loss: 4268319.2500\n",
      "Epoch [731/1000], Loss: 4292510.7812\n",
      "Epoch [732/1000], Loss: 4332099.5312\n",
      "Epoch [733/1000], Loss: 4303889.1719\n",
      "Epoch [734/1000], Loss: 4329948.7344\n",
      "Epoch [735/1000], Loss: 4301191.8281\n",
      "Epoch [736/1000], Loss: 4339866.2656\n",
      "Epoch [737/1000], Loss: 4299967.9688\n",
      "Epoch [738/1000], Loss: 4320111.4531\n",
      "Epoch [739/1000], Loss: 4347428.4062\n",
      "Epoch [740/1000], Loss: 4229814.4688\n",
      "Epoch [741/1000], Loss: 4330522.1406\n",
      "Epoch [742/1000], Loss: 4288320.4844\n",
      "Epoch [743/1000], Loss: 4326159.3281\n",
      "Epoch [744/1000], Loss: 4267374.1875\n",
      "Epoch [745/1000], Loss: 4338387.9531\n",
      "Epoch [746/1000], Loss: 4266619.3906\n",
      "Epoch [747/1000], Loss: 4344753.7031\n",
      "Epoch [748/1000], Loss: 4251456.2344\n",
      "Epoch [749/1000], Loss: 4314824.3750\n",
      "Epoch [750/1000], Loss: 4355760.4062\n",
      "Epoch [751/1000], Loss: 4274410.3281\n",
      "Epoch [752/1000], Loss: 4304564.1719\n",
      "Epoch [753/1000], Loss: 4239951.4219\n",
      "Epoch [754/1000], Loss: 4217701.0625\n",
      "Epoch [755/1000], Loss: 4292887.3438\n",
      "Epoch [756/1000], Loss: 4337003.3438\n",
      "Epoch [757/1000], Loss: 4330749.2500\n",
      "Epoch [758/1000], Loss: 4260467.0312\n",
      "Epoch [759/1000], Loss: 4260802.5781\n",
      "Epoch [760/1000], Loss: 4322551.2656\n",
      "Epoch [761/1000], Loss: 4313366.7969\n",
      "Epoch [762/1000], Loss: 4249784.5312\n",
      "Epoch [763/1000], Loss: 4261981.8594\n",
      "Epoch [764/1000], Loss: 4194394.6562\n",
      "Epoch [765/1000], Loss: 4276197.5156\n",
      "Epoch [766/1000], Loss: 4230857.6094\n",
      "Epoch [767/1000], Loss: 4219601.8281\n",
      "Epoch [768/1000], Loss: 4314899.7188\n",
      "Epoch [769/1000], Loss: 4309270.6719\n",
      "Epoch [770/1000], Loss: 4256657.8125\n",
      "Epoch [771/1000], Loss: 4263386.8906\n",
      "Epoch [772/1000], Loss: 4329047.0938\n",
      "Epoch [773/1000], Loss: 4249369.1094\n",
      "Epoch [774/1000], Loss: 4241701.4844\n",
      "Epoch [775/1000], Loss: 4308711.7344\n",
      "Epoch [776/1000], Loss: 4208677.8594\n",
      "Epoch [777/1000], Loss: 4224028.0000\n",
      "Epoch [778/1000], Loss: 4184645.1719\n",
      "Epoch [779/1000], Loss: 4260358.7656\n",
      "Epoch [780/1000], Loss: 4238478.6875\n",
      "Epoch [781/1000], Loss: 4241478.2812\n",
      "Epoch [782/1000], Loss: 4285230.0156\n",
      "Epoch [783/1000], Loss: 4228597.2656\n",
      "Epoch [784/1000], Loss: 4297439.9531\n",
      "Epoch [785/1000], Loss: 4256938.8906\n",
      "Epoch [786/1000], Loss: 4267507.0000\n",
      "Epoch [787/1000], Loss: 4245909.4219\n",
      "Epoch [788/1000], Loss: 4215771.3750\n",
      "Epoch [789/1000], Loss: 4297430.5469\n",
      "Epoch [790/1000], Loss: 4232902.7188\n",
      "Epoch [791/1000], Loss: 4235942.4688\n",
      "Epoch [792/1000], Loss: 4252318.4844\n",
      "Epoch [793/1000], Loss: 4285440.7031\n",
      "Epoch [794/1000], Loss: 4235069.4688\n",
      "Epoch [795/1000], Loss: 4216586.3594\n",
      "Epoch [796/1000], Loss: 4232663.9375\n",
      "Epoch [797/1000], Loss: 4250929.3281\n",
      "Epoch [798/1000], Loss: 4188895.0625\n",
      "Epoch [799/1000], Loss: 4218084.5781\n",
      "Epoch [800/1000], Loss: 4173489.9062\n",
      "Epoch [801/1000], Loss: 4222433.2656\n",
      "Epoch [802/1000], Loss: 4177192.7188\n",
      "Epoch [803/1000], Loss: 4203942.7031\n",
      "Epoch [804/1000], Loss: 4183217.1250\n",
      "Epoch [805/1000], Loss: 4170259.3125\n",
      "Epoch [806/1000], Loss: 4233295.5000\n",
      "Epoch [807/1000], Loss: 4188199.1406\n",
      "Epoch [808/1000], Loss: 4214207.0000\n",
      "Epoch [809/1000], Loss: 4229095.0625\n",
      "Epoch [810/1000], Loss: 4293842.1562\n",
      "Epoch [811/1000], Loss: 4221227.1250\n",
      "Epoch [812/1000], Loss: 4219462.0625\n",
      "Epoch [813/1000], Loss: 4181824.1094\n",
      "Epoch [814/1000], Loss: 4223941.4531\n",
      "Epoch [815/1000], Loss: 4273216.2656\n",
      "Epoch [816/1000], Loss: 4199058.0625\n",
      "Epoch [817/1000], Loss: 4169073.1875\n",
      "Epoch [818/1000], Loss: 4200319.5000\n",
      "Epoch [819/1000], Loss: 4221354.5469\n",
      "Epoch [820/1000], Loss: 4256528.2500\n",
      "Epoch [821/1000], Loss: 4245055.4375\n",
      "Epoch [822/1000], Loss: 4206597.7969\n",
      "Epoch [823/1000], Loss: 4198042.1875\n",
      "Epoch [824/1000], Loss: 4268512.4062\n",
      "Epoch [825/1000], Loss: 4222358.0469\n",
      "Epoch [826/1000], Loss: 4224665.5781\n",
      "Epoch [827/1000], Loss: 4230213.6094\n",
      "Epoch [828/1000], Loss: 4184494.4844\n",
      "Epoch [829/1000], Loss: 4222852.4219\n",
      "Epoch [830/1000], Loss: 4201185.3438\n",
      "Epoch [831/1000], Loss: 4227455.5781\n",
      "Epoch [832/1000], Loss: 4179834.1250\n",
      "Epoch [833/1000], Loss: 4215764.2969\n",
      "Epoch [834/1000], Loss: 4212239.2656\n",
      "Epoch [835/1000], Loss: 4200327.5469\n",
      "Epoch [836/1000], Loss: 4140592.6250\n",
      "Epoch [837/1000], Loss: 4217711.6250\n",
      "Epoch [838/1000], Loss: 4237783.0469\n",
      "Epoch [839/1000], Loss: 4224271.2344\n",
      "Epoch [840/1000], Loss: 4155959.5000\n",
      "Epoch [841/1000], Loss: 4227398.5938\n",
      "Epoch [842/1000], Loss: 4092387.7188\n",
      "Epoch [843/1000], Loss: 4288176.3281\n",
      "Epoch [844/1000], Loss: 4163552.3438\n",
      "Epoch [845/1000], Loss: 4175776.5000\n",
      "Epoch [846/1000], Loss: 4169465.5312\n",
      "Epoch [847/1000], Loss: 4138451.0469\n",
      "Epoch [848/1000], Loss: 4259670.3281\n",
      "Epoch [849/1000], Loss: 4208310.2812\n",
      "Epoch [850/1000], Loss: 4166405.5938\n",
      "Epoch [851/1000], Loss: 4251284.0000\n",
      "Epoch [852/1000], Loss: 4186177.0938\n",
      "Epoch [853/1000], Loss: 4187062.7969\n",
      "Epoch [854/1000], Loss: 4171616.8750\n",
      "Epoch [855/1000], Loss: 4175187.6094\n",
      "Epoch [856/1000], Loss: 4179266.2656\n",
      "Epoch [857/1000], Loss: 4170502.0469\n",
      "Epoch [858/1000], Loss: 4203056.7188\n",
      "Epoch [859/1000], Loss: 4209817.3906\n",
      "Epoch [860/1000], Loss: 4199905.3281\n",
      "Epoch [861/1000], Loss: 4149627.5781\n",
      "Epoch [862/1000], Loss: 4229183.2656\n",
      "Epoch [863/1000], Loss: 4157821.9062\n",
      "Epoch [864/1000], Loss: 4213189.3438\n",
      "Epoch [865/1000], Loss: 4213339.6875\n",
      "Epoch [866/1000], Loss: 4192861.2344\n",
      "Epoch [867/1000], Loss: 4140492.5312\n",
      "Epoch [868/1000], Loss: 4205524.7812\n",
      "Epoch [869/1000], Loss: 4239489.3281\n",
      "Epoch [870/1000], Loss: 4132705.9688\n",
      "Epoch [871/1000], Loss: 4202135.5625\n",
      "Epoch [872/1000], Loss: 4201848.9375\n",
      "Epoch [873/1000], Loss: 4121826.3906\n",
      "Epoch [874/1000], Loss: 4197885.8750\n",
      "Epoch [875/1000], Loss: 4130452.8750\n",
      "Epoch [876/1000], Loss: 4162552.7656\n",
      "Epoch [877/1000], Loss: 4167718.6875\n",
      "Epoch [878/1000], Loss: 4191085.1406\n",
      "Epoch [879/1000], Loss: 4199023.6875\n",
      "Epoch [880/1000], Loss: 4205390.5312\n",
      "Epoch [881/1000], Loss: 4192335.1875\n",
      "Epoch [882/1000], Loss: 4180355.7656\n",
      "Epoch [883/1000], Loss: 4189564.0938\n",
      "Epoch [884/1000], Loss: 4179658.0000\n",
      "Epoch [885/1000], Loss: 4195451.2812\n",
      "Epoch [886/1000], Loss: 4204077.7031\n",
      "Epoch [887/1000], Loss: 4210564.4844\n",
      "Epoch [888/1000], Loss: 4198293.3281\n",
      "Epoch [889/1000], Loss: 4262934.3906\n",
      "Epoch [890/1000], Loss: 4091083.9844\n",
      "Epoch [891/1000], Loss: 4179000.8125\n",
      "Epoch [892/1000], Loss: 4208575.8750\n",
      "Epoch [893/1000], Loss: 4210291.1875\n",
      "Epoch [894/1000], Loss: 4206065.9844\n",
      "Epoch [895/1000], Loss: 4138345.9219\n",
      "Epoch [896/1000], Loss: 4152073.5000\n",
      "Epoch [897/1000], Loss: 4173449.2031\n",
      "Epoch [898/1000], Loss: 4146288.0469\n",
      "Epoch [899/1000], Loss: 4202310.7188\n",
      "Epoch [900/1000], Loss: 4151856.3594\n",
      "Epoch [901/1000], Loss: 4269836.5469\n",
      "Epoch [902/1000], Loss: 4128339.2656\n",
      "Epoch [903/1000], Loss: 4224679.0625\n",
      "Epoch [904/1000], Loss: 4153880.5312\n",
      "Epoch [905/1000], Loss: 4181567.0625\n",
      "Epoch [906/1000], Loss: 4152075.6250\n",
      "Epoch [907/1000], Loss: 4187130.8281\n",
      "Epoch [908/1000], Loss: 4200363.0469\n",
      "Epoch [909/1000], Loss: 4097357.2500\n",
      "Epoch [910/1000], Loss: 4144094.3281\n",
      "Epoch [911/1000], Loss: 4125255.2500\n",
      "Epoch [912/1000], Loss: 4128150.8125\n",
      "Epoch [913/1000], Loss: 4172937.9844\n",
      "Epoch [914/1000], Loss: 4206034.3750\n",
      "Epoch [915/1000], Loss: 4163792.1406\n",
      "Epoch [916/1000], Loss: 4161898.9688\n",
      "Epoch [917/1000], Loss: 4201221.2188\n",
      "Epoch [918/1000], Loss: 4203788.7188\n",
      "Epoch [919/1000], Loss: 4150657.4375\n",
      "Epoch [920/1000], Loss: 4206010.7188\n",
      "Epoch [921/1000], Loss: 4149463.9062\n",
      "Epoch [922/1000], Loss: 4143616.4062\n",
      "Epoch [923/1000], Loss: 4202250.0000\n",
      "Epoch [924/1000], Loss: 4203624.0938\n",
      "Epoch [925/1000], Loss: 4223762.7031\n",
      "Epoch [926/1000], Loss: 4140974.0312\n",
      "Epoch [927/1000], Loss: 4220849.7500\n",
      "Epoch [928/1000], Loss: 4289701.4844\n",
      "Epoch [929/1000], Loss: 4158577.8281\n",
      "Epoch [930/1000], Loss: 4157963.4531\n",
      "Epoch [931/1000], Loss: 4199399.1250\n",
      "Epoch [932/1000], Loss: 4151102.8750\n",
      "Epoch [933/1000], Loss: 4116129.1250\n",
      "Epoch [934/1000], Loss: 4221412.1719\n",
      "Epoch [935/1000], Loss: 4216402.8750\n",
      "Epoch [936/1000], Loss: 4118441.4375\n",
      "Epoch [937/1000], Loss: 4128740.2031\n",
      "Epoch [938/1000], Loss: 4166421.3281\n",
      "Epoch [939/1000], Loss: 4068605.5781\n",
      "Epoch [940/1000], Loss: 4213525.7969\n",
      "Epoch [941/1000], Loss: 4198143.7969\n",
      "Epoch [942/1000], Loss: 4198010.2500\n",
      "Epoch [943/1000], Loss: 4105705.5156\n",
      "Epoch [944/1000], Loss: 4130679.7500\n",
      "Epoch [945/1000], Loss: 4179985.3125\n",
      "Epoch [946/1000], Loss: 4139534.9844\n",
      "Epoch [947/1000], Loss: 4214601.0781\n",
      "Epoch [948/1000], Loss: 4141214.2656\n",
      "Epoch [949/1000], Loss: 4189698.1094\n",
      "Epoch [950/1000], Loss: 4193918.1562\n",
      "Epoch [951/1000], Loss: 4113298.9844\n",
      "Epoch [952/1000], Loss: 4227228.0781\n",
      "Epoch [953/1000], Loss: 4203056.7812\n",
      "Epoch [954/1000], Loss: 4163611.5625\n",
      "Epoch [955/1000], Loss: 4150188.2812\n",
      "Epoch [956/1000], Loss: 4103386.1875\n",
      "Epoch [957/1000], Loss: 4121247.3125\n",
      "Epoch [958/1000], Loss: 4211433.3750\n",
      "Epoch [959/1000], Loss: 4158053.4375\n",
      "Epoch [960/1000], Loss: 4182535.0938\n",
      "Epoch [961/1000], Loss: 4164510.6719\n",
      "Epoch [962/1000], Loss: 4240236.6719\n",
      "Epoch [963/1000], Loss: 4171555.2031\n",
      "Epoch [964/1000], Loss: 4139635.1094\n",
      "Epoch [965/1000], Loss: 4160290.9844\n",
      "Epoch [966/1000], Loss: 4155461.1719\n",
      "Epoch [967/1000], Loss: 4150877.1406\n",
      "Epoch [968/1000], Loss: 4144682.5312\n",
      "Epoch [969/1000], Loss: 4144690.4531\n",
      "Epoch [970/1000], Loss: 4144791.9531\n",
      "Epoch [971/1000], Loss: 4178024.5156\n",
      "Epoch [972/1000], Loss: 4145713.8906\n",
      "Epoch [973/1000], Loss: 4174520.9375\n",
      "Epoch [974/1000], Loss: 4191245.2656\n",
      "Epoch [975/1000], Loss: 4110022.3906\n",
      "Epoch [976/1000], Loss: 4177904.7344\n",
      "Epoch [977/1000], Loss: 4175993.7188\n",
      "Epoch [978/1000], Loss: 4148314.1875\n",
      "Epoch [979/1000], Loss: 4196321.4688\n",
      "Epoch [980/1000], Loss: 4180364.6719\n",
      "Epoch [981/1000], Loss: 4140975.5312\n",
      "Epoch [982/1000], Loss: 4185023.5000\n",
      "Epoch [983/1000], Loss: 4123343.0938\n",
      "Epoch [984/1000], Loss: 4159218.2969\n",
      "Epoch [985/1000], Loss: 4116505.7188\n",
      "Epoch [986/1000], Loss: 4262040.0156\n",
      "Epoch [987/1000], Loss: 4110042.8594\n",
      "Epoch [988/1000], Loss: 4098404.5625\n",
      "Epoch [989/1000], Loss: 4151639.7812\n",
      "Epoch [990/1000], Loss: 4159300.2344\n",
      "Epoch [991/1000], Loss: 4152752.5312\n",
      "Epoch [992/1000], Loss: 4228064.2969\n",
      "Epoch [993/1000], Loss: 4170129.4844\n",
      "Epoch [994/1000], Loss: 4180279.1094\n",
      "Epoch [995/1000], Loss: 4162770.6406\n",
      "Epoch [996/1000], Loss: 4258751.9219\n",
      "Epoch [997/1000], Loss: 4108402.9531\n",
      "Epoch [998/1000], Loss: 4105842.2031\n",
      "Epoch [999/1000], Loss: 4167740.1562\n",
      "Epoch [1000/1000], Loss: 4143303.5156\n"
     ]
    }
   ],
   "source": [
    "VAD_best = VAD(latent_dim=128, device=device)\n",
    "trainer_best = VAD_Trainer(var_decoder=VAD_best, dataloader=train_dl, latent_dim=128, beta=1e6, device=device, lr=0.001)\n",
    "_ = trainer_best.train(num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edda5b7e-e9d7-4b24-b253-566316440be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21835661493241787\n"
     ]
    }
   ],
   "source": [
    "num_test_samples = len(train_dl.dataset)\n",
    "opt = optim.Adam([trainer_best.latents], lr=1e-3)\n",
    "evaluate_loss = evaluate_model(model=VAD_best, test_dl=train_dl, opt=opt, latents=trainer_best.latents, epochs=1000, device=device)\n",
    "print(evaluate_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9360261-02b1-4d2c-96b3-c102c9c66b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latents = VAD_best.reparameterize(trainer_best.latents)\n",
    "utils.plot_tsne(train_ds, latents, f\"tsne_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Sample specific vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2bfd32-1c89-4f4f-9494-9d7f6e7adc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "mu_test = torch.randn(num_test_samples, VAD_best.latent_dim, device=device, requires_grad=True)\n",
    "sigma_test = torch.randn(num_test_samples, VAD_best.latent_dim, device=device, requires_grad=True)\n",
    "test_latents = torch.nn.parameter.Parameter(torch.stack([mu_test, sigma_test], dim=1)).to(device)\n",
    "opt = optim.Adam([test_latents], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6568406b-c6c5-40af-9339-9825107b7bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD has finished test evaluation with a test loss of 0.2232640078291297.\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(model=VAD_best, test_dl=test_dl, opt=opt, latents=test_latents, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a5d3b54-cf39-4e45-af3f-8a573b9b3007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_test_latents = VAD_best.reparameterize(test_latents)\n",
    "utils.plot_tsne(test_ds, final_test_latents, f\"tsne_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(49)\n",
    "sampled_indices = random.sample(range(1000), 5)\n",
    "random_latents_tensor = torch.randn(5,VAD_best.latent_dim, device=device)\n",
    "\n",
    "\n",
    "sampled_test_images = VAD_best(test_latents[sampled_indices]).view(-1, 1, 28, 28)\n",
    "random_test_images = VAD_best.decoder(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images_VAD.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images_VAD.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e7d41-cb83-46b6-95b0-33a7258e5fee",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e96f6f8-17b2-40b8-8f4e-4b8998ad729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sampled_indices = [1, 25]\n",
    "sampled_latents = [final_test_latents[i] for i in sampled_indices]\n",
    "weights = np.linspace(0, 1, 7)\n",
    "interpolated_latents = [w * sampled_latents[0] + (1 - w) * sampled_latents[1] for w in weights]\n",
    "interpolated_latents_tensor = torch.stack(interpolated_latents)\n",
    "interpolated_images = VAD_best.decoder(interpolated_latents_tensor).view(-1, 1, 28, 28)\n",
    "utils.save_images(interpolated_images, \"interpolated_images_VAD.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
