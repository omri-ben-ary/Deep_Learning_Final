{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from VariationalAutoDecoder_Beta import VariationalAutoDecoder as VAD_Beta\n",
    "from VAD_Trainer import VAD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "architectures = [\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# latent_dims = [dim for dim in [64, 32, 128, 16, 10] for _ in range(5)]\n",
    "VADs = [VAD(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device) for arch in architectures]# for _ in range(5)]\n",
    "# learning_rates = [lr for lr in [0.001, 0.0005, 0.0001, 0.002, 0.005] for _ in range(5)]\n",
    "trainers = [VAD_Trainer(var_decoder=VADs[i], dataloader=train_dl, latent_dim=128, device=device, lr=1e-2) for i in range(len(VADs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "temp_latents = torch.randn(10, 128).to(device)\n",
    "latents_list = [torch.nn.Parameter(torch.stack([temp_latents[label,:] for label in train_dl.dataset.y])).to(device) for i in range(len(VADs))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results_VAD_temp.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=500)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=VADs[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=500, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27147e64-6c63-4528-bc12-99750ba8ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainers)):\n",
    "    utils.plot_tsne(train_ds, trainers[i].latents, f\"tsne_kl_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64685dc-0a81-42d3-b67b-cf78d1e34e9e",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801bf72-57eb-4d39-9a97-0330af95e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "\n",
    "learning_rates = [0.001, 0.0001, 0.005]\n",
    "VADs_ft = [VAD(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device) for _ in range(len(learning_rates))]# for _ in range(5)]\n",
    "trainers_ft = [VAD_Trainer(var_decoder=VADs_ft[i], dataloader=train_dl, latent_dim=128, device=device, lr=learning_rates[i]) for i in range(len(VADs_ft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d52bc6-5d45-4c1b-8c4a-ec606d33ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "temp_latents_ft = torch.randn(10, 128).to(device)\n",
    "latents_list_ft = [torch.nn.Parameter(torch.stack([temp_latents_ft[label,:] for label in train_dl.dataset.y])).to(device) for i in range(len(VADs_ft))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list_ft]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results_VAD_ft.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers_ft):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=500)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=VADs_ft[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list_ft[index], epochs=500, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afa67a-6bc0-4802-8dfd-d3cf9350207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainers_ft)):\n",
    "    utils.plot_tsne(train_ds, trainers_ft[i].latents, f\"tsne_kl_ft_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033aaf-061c-4bd6-b32e-40466dd1027f",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc862a48-c47c-49ce-b233-71192301ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.4978\n",
      "Epoch [2/1000], Loss: 0.4725\n",
      "Epoch [3/1000], Loss: 0.4522\n",
      "Epoch [4/1000], Loss: 0.4314\n",
      "Epoch [5/1000], Loss: 0.4100\n",
      "Epoch [6/1000], Loss: 0.3879\n",
      "Epoch [7/1000], Loss: 0.3662\n",
      "Epoch [8/1000], Loss: 0.3450\n",
      "Epoch [9/1000], Loss: 0.3245\n",
      "Epoch [10/1000], Loss: 0.3072\n",
      "Epoch [11/1000], Loss: 0.2919\n",
      "Epoch [12/1000], Loss: 0.2776\n",
      "Epoch [13/1000], Loss: 0.2651\n",
      "Epoch [14/1000], Loss: 0.2579\n",
      "Epoch [15/1000], Loss: 0.2544\n",
      "Epoch [16/1000], Loss: 0.2565\n",
      "Epoch [17/1000], Loss: 0.2574\n",
      "Epoch [18/1000], Loss: 0.2505\n",
      "Epoch [19/1000], Loss: 0.2469\n",
      "Epoch [20/1000], Loss: 0.2436\n",
      "Epoch [21/1000], Loss: 0.2407\n",
      "Epoch [22/1000], Loss: 0.2368\n",
      "Epoch [23/1000], Loss: 0.2328\n",
      "Epoch [24/1000], Loss: 0.2311\n",
      "Epoch [25/1000], Loss: 0.2308\n",
      "Epoch [26/1000], Loss: 0.2305\n",
      "Epoch [27/1000], Loss: 0.2299\n",
      "Epoch [28/1000], Loss: 0.2272\n",
      "Epoch [29/1000], Loss: 0.2246\n",
      "Epoch [30/1000], Loss: 0.2223\n",
      "Epoch [31/1000], Loss: 0.2210\n",
      "Epoch [32/1000], Loss: 0.2190\n",
      "Epoch [33/1000], Loss: 0.2174\n",
      "Epoch [34/1000], Loss: 0.2161\n",
      "Epoch [35/1000], Loss: 0.2154\n",
      "Epoch [36/1000], Loss: 0.2145\n",
      "Epoch [37/1000], Loss: 0.2138\n",
      "Epoch [38/1000], Loss: 0.2132\n",
      "Epoch [39/1000], Loss: 0.2122\n",
      "Epoch [40/1000], Loss: 0.2115\n",
      "Epoch [41/1000], Loss: 0.2110\n",
      "Epoch [42/1000], Loss: 0.2096\n",
      "Epoch [43/1000], Loss: 0.2104\n",
      "Epoch [44/1000], Loss: 0.2121\n",
      "Epoch [45/1000], Loss: 0.2103\n",
      "Epoch [46/1000], Loss: 0.2086\n",
      "Epoch [47/1000], Loss: 0.2074\n",
      "Epoch [48/1000], Loss: 0.2068\n",
      "Epoch [49/1000], Loss: 0.2057\n",
      "Epoch [50/1000], Loss: 0.2049\n",
      "Epoch [51/1000], Loss: 0.2038\n",
      "Epoch [52/1000], Loss: 0.2036\n",
      "Epoch [53/1000], Loss: 0.2044\n",
      "Epoch [54/1000], Loss: 0.2029\n",
      "Epoch [55/1000], Loss: 0.2021\n",
      "Epoch [56/1000], Loss: 0.2007\n",
      "Epoch [57/1000], Loss: 0.1996\n",
      "Epoch [58/1000], Loss: 0.1997\n",
      "Epoch [59/1000], Loss: 0.1988\n",
      "Epoch [60/1000], Loss: 0.1989\n",
      "Epoch [61/1000], Loss: 0.1987\n",
      "Epoch [62/1000], Loss: 0.1985\n",
      "Epoch [63/1000], Loss: 0.1982\n",
      "Epoch [64/1000], Loss: 0.1980\n",
      "Epoch [65/1000], Loss: 0.1963\n",
      "Epoch [66/1000], Loss: 0.1940\n",
      "Epoch [67/1000], Loss: 0.1934\n",
      "Epoch [68/1000], Loss: 0.1926\n",
      "Epoch [69/1000], Loss: 0.1925\n",
      "Epoch [70/1000], Loss: 0.1920\n",
      "Epoch [71/1000], Loss: 0.1914\n",
      "Epoch [72/1000], Loss: 0.1911\n",
      "Epoch [73/1000], Loss: 0.1904\n",
      "Epoch [74/1000], Loss: 0.1906\n",
      "Epoch [75/1000], Loss: 0.1919\n",
      "Epoch [76/1000], Loss: 0.1916\n",
      "Epoch [77/1000], Loss: 0.1913\n",
      "Epoch [78/1000], Loss: 0.1917\n",
      "Epoch [79/1000], Loss: 0.1910\n",
      "Epoch [80/1000], Loss: 0.1906\n",
      "Epoch [81/1000], Loss: 0.1904\n",
      "Epoch [82/1000], Loss: 0.1874\n",
      "Epoch [83/1000], Loss: 0.1857\n",
      "Epoch [84/1000], Loss: 0.1853\n",
      "Epoch [85/1000], Loss: 0.1842\n",
      "Epoch [86/1000], Loss: 0.1831\n",
      "Epoch [87/1000], Loss: 0.1833\n",
      "Epoch [88/1000], Loss: 0.1832\n",
      "Epoch [89/1000], Loss: 0.1819\n",
      "Epoch [90/1000], Loss: 0.1806\n",
      "Epoch [91/1000], Loss: 0.1803\n",
      "Epoch [92/1000], Loss: 0.1807\n",
      "Epoch [93/1000], Loss: 0.1809\n",
      "Epoch [94/1000], Loss: 0.1821\n",
      "Epoch [95/1000], Loss: 0.1821\n",
      "Epoch [96/1000], Loss: 0.1803\n",
      "Epoch [97/1000], Loss: 0.1788\n",
      "Epoch [98/1000], Loss: 0.1783\n",
      "Epoch [99/1000], Loss: 0.1772\n",
      "Epoch [100/1000], Loss: 0.1764\n",
      "Epoch [101/1000], Loss: 0.1757\n",
      "Epoch [102/1000], Loss: 0.1752\n",
      "Epoch [103/1000], Loss: 0.1758\n",
      "Epoch [104/1000], Loss: 0.1735\n",
      "Epoch [105/1000], Loss: 0.1743\n",
      "Epoch [106/1000], Loss: 0.1729\n",
      "Epoch [107/1000], Loss: 0.1726\n",
      "Epoch [108/1000], Loss: 0.1720\n",
      "Epoch [109/1000], Loss: 0.1716\n",
      "Epoch [110/1000], Loss: 0.1704\n",
      "Epoch [111/1000], Loss: 0.1712\n",
      "Epoch [112/1000], Loss: 0.1707\n",
      "Epoch [113/1000], Loss: 0.1716\n",
      "Epoch [114/1000], Loss: 0.1715\n",
      "Epoch [115/1000], Loss: 0.1700\n",
      "Epoch [116/1000], Loss: 0.1696\n",
      "Epoch [117/1000], Loss: 0.1692\n",
      "Epoch [118/1000], Loss: 0.1679\n",
      "Epoch [119/1000], Loss: 0.1662\n",
      "Epoch [120/1000], Loss: 0.1674\n",
      "Epoch [121/1000], Loss: 0.1674\n",
      "Epoch [122/1000], Loss: 0.1670\n",
      "Epoch [123/1000], Loss: 0.1643\n",
      "Epoch [124/1000], Loss: 0.1642\n",
      "Epoch [125/1000], Loss: 0.1624\n",
      "Epoch [126/1000], Loss: 0.1625\n",
      "Epoch [127/1000], Loss: 0.1626\n",
      "Epoch [128/1000], Loss: 0.1611\n",
      "Epoch [129/1000], Loss: 0.1614\n",
      "Epoch [130/1000], Loss: 0.1606\n",
      "Epoch [131/1000], Loss: 0.1603\n",
      "Epoch [132/1000], Loss: 0.1592\n",
      "Epoch [133/1000], Loss: 0.1598\n",
      "Epoch [134/1000], Loss: 0.1604\n",
      "Epoch [135/1000], Loss: 0.1602\n",
      "Epoch [136/1000], Loss: 0.1600\n",
      "Epoch [137/1000], Loss: 0.1589\n",
      "Epoch [138/1000], Loss: 0.1588\n",
      "Epoch [139/1000], Loss: 0.1569\n",
      "Epoch [140/1000], Loss: 0.1560\n",
      "Epoch [141/1000], Loss: 0.1535\n",
      "Epoch [142/1000], Loss: 0.1526\n",
      "Epoch [143/1000], Loss: 0.1513\n",
      "Epoch [144/1000], Loss: 0.1509\n",
      "Epoch [145/1000], Loss: 0.1511\n",
      "Epoch [146/1000], Loss: 0.1503\n",
      "Epoch [147/1000], Loss: 0.1503\n",
      "Epoch [148/1000], Loss: 0.1496\n",
      "Epoch [149/1000], Loss: 0.1501\n",
      "Epoch [150/1000], Loss: 0.1497\n",
      "Epoch [151/1000], Loss: 0.1506\n",
      "Epoch [152/1000], Loss: 0.1464\n",
      "Epoch [153/1000], Loss: 0.1442\n",
      "Epoch [154/1000], Loss: 0.1441\n",
      "Epoch [155/1000], Loss: 0.1442\n",
      "Epoch [156/1000], Loss: 0.1443\n",
      "Epoch [157/1000], Loss: 0.1488\n",
      "Epoch [158/1000], Loss: 0.1433\n",
      "Epoch [159/1000], Loss: 0.1407\n",
      "Epoch [160/1000], Loss: 0.1402\n",
      "Epoch [161/1000], Loss: 0.1394\n",
      "Epoch [162/1000], Loss: 0.1379\n",
      "Epoch [163/1000], Loss: 0.1404\n",
      "Epoch [164/1000], Loss: 0.1389\n",
      "Epoch [165/1000], Loss: 0.1379\n",
      "Epoch [166/1000], Loss: 0.1362\n",
      "Epoch [167/1000], Loss: 0.1343\n",
      "Epoch [168/1000], Loss: 0.1326\n",
      "Epoch [169/1000], Loss: 0.1339\n",
      "Epoch [170/1000], Loss: 0.1331\n",
      "Epoch [171/1000], Loss: 0.1377\n",
      "Epoch [172/1000], Loss: 0.1334\n",
      "Epoch [173/1000], Loss: 0.1297\n",
      "Epoch [174/1000], Loss: 0.1284\n",
      "Epoch [175/1000], Loss: 0.1292\n",
      "Epoch [176/1000], Loss: 0.1295\n",
      "Epoch [177/1000], Loss: 0.1324\n",
      "Epoch [178/1000], Loss: 0.1292\n",
      "Epoch [179/1000], Loss: 0.1270\n",
      "Epoch [180/1000], Loss: 0.1257\n",
      "Epoch [181/1000], Loss: 0.1274\n",
      "Epoch [182/1000], Loss: 0.1263\n",
      "Epoch [183/1000], Loss: 0.1265\n",
      "Epoch [184/1000], Loss: 0.1259\n",
      "Epoch [185/1000], Loss: 0.1280\n",
      "Epoch [186/1000], Loss: 0.1261\n",
      "Epoch [187/1000], Loss: 0.1271\n",
      "Epoch [188/1000], Loss: 0.1251\n",
      "Epoch [189/1000], Loss: 0.1260\n",
      "Epoch [190/1000], Loss: 0.1241\n",
      "Epoch [191/1000], Loss: 0.1211\n",
      "Epoch [192/1000], Loss: 0.1214\n",
      "Epoch [193/1000], Loss: 0.1205\n",
      "Epoch [194/1000], Loss: 0.1244\n",
      "Epoch [195/1000], Loss: 0.1216\n",
      "Epoch [196/1000], Loss: 0.1196\n",
      "Epoch [197/1000], Loss: 0.1180\n",
      "Epoch [198/1000], Loss: 0.1174\n",
      "Epoch [199/1000], Loss: 0.1158\n",
      "Epoch [200/1000], Loss: 0.1162\n",
      "Epoch [201/1000], Loss: 0.1155\n",
      "Epoch [202/1000], Loss: 0.1166\n",
      "Epoch [203/1000], Loss: 0.1155\n",
      "Epoch [204/1000], Loss: 0.1137\n",
      "Epoch [205/1000], Loss: 0.1135\n",
      "Epoch [206/1000], Loss: 0.1126\n",
      "Epoch [207/1000], Loss: 0.1106\n",
      "Epoch [208/1000], Loss: 0.1119\n",
      "Epoch [209/1000], Loss: 0.1108\n",
      "Epoch [210/1000], Loss: 0.1104\n",
      "Epoch [211/1000], Loss: 0.1102\n",
      "Epoch [212/1000], Loss: 0.1094\n",
      "Epoch [213/1000], Loss: 0.1079\n",
      "Epoch [214/1000], Loss: 0.1096\n",
      "Epoch [215/1000], Loss: 0.1075\n",
      "Epoch [216/1000], Loss: 0.1090\n",
      "Epoch [217/1000], Loss: 0.1085\n",
      "Epoch [218/1000], Loss: 0.1083\n",
      "Epoch [219/1000], Loss: 0.1065\n",
      "Epoch [220/1000], Loss: 0.1057\n",
      "Epoch [221/1000], Loss: 0.1055\n",
      "Epoch [222/1000], Loss: 0.1059\n",
      "Epoch [223/1000], Loss: 0.1045\n",
      "Epoch [224/1000], Loss: 0.1051\n",
      "Epoch [225/1000], Loss: 0.1029\n",
      "Epoch [226/1000], Loss: 0.1019\n",
      "Epoch [227/1000], Loss: 0.1009\n",
      "Epoch [228/1000], Loss: 0.1005\n",
      "Epoch [229/1000], Loss: 0.1004\n",
      "Epoch [230/1000], Loss: 0.1003\n",
      "Epoch [231/1000], Loss: 0.1007\n",
      "Epoch [232/1000], Loss: 0.1005\n",
      "Epoch [233/1000], Loss: 0.0989\n",
      "Epoch [234/1000], Loss: 0.0992\n",
      "Epoch [235/1000], Loss: 0.0984\n",
      "Epoch [236/1000], Loss: 0.0989\n",
      "Epoch [237/1000], Loss: 0.0988\n",
      "Epoch [238/1000], Loss: 0.0993\n",
      "Epoch [239/1000], Loss: 0.0986\n",
      "Epoch [240/1000], Loss: 0.0988\n",
      "Epoch [241/1000], Loss: 0.0983\n",
      "Epoch [242/1000], Loss: 0.0976\n",
      "Epoch [243/1000], Loss: 0.0969\n",
      "Epoch [244/1000], Loss: 0.0980\n",
      "Epoch [245/1000], Loss: 0.0969\n",
      "Epoch [246/1000], Loss: 0.0971\n",
      "Epoch [247/1000], Loss: 0.0972\n",
      "Epoch [248/1000], Loss: 0.0960\n",
      "Epoch [249/1000], Loss: 0.0935\n",
      "Epoch [250/1000], Loss: 0.0938\n",
      "Epoch [251/1000], Loss: 0.0930\n",
      "Epoch [252/1000], Loss: 0.0941\n",
      "Epoch [253/1000], Loss: 0.0938\n",
      "Epoch [254/1000], Loss: 0.0941\n",
      "Epoch [255/1000], Loss: 0.0942\n",
      "Epoch [256/1000], Loss: 0.0945\n",
      "Epoch [257/1000], Loss: 0.0939\n",
      "Epoch [258/1000], Loss: 0.0954\n",
      "Epoch [259/1000], Loss: 0.0943\n",
      "Epoch [260/1000], Loss: 0.0935\n",
      "Epoch [261/1000], Loss: 0.0929\n",
      "Epoch [262/1000], Loss: 0.0906\n",
      "Epoch [263/1000], Loss: 0.0911\n",
      "Epoch [264/1000], Loss: 0.0907\n",
      "Epoch [265/1000], Loss: 0.0902\n",
      "Epoch [266/1000], Loss: 0.0921\n",
      "Epoch [267/1000], Loss: 0.0937\n",
      "Epoch [268/1000], Loss: 0.0918\n",
      "Epoch [269/1000], Loss: 0.0912\n",
      "Epoch [270/1000], Loss: 0.0906\n",
      "Epoch [271/1000], Loss: 0.0908\n",
      "Epoch [272/1000], Loss: 0.0925\n",
      "Epoch [273/1000], Loss: 0.0930\n",
      "Epoch [274/1000], Loss: 0.0919\n",
      "Epoch [275/1000], Loss: 0.0913\n",
      "Epoch [276/1000], Loss: 0.0935\n",
      "Epoch [277/1000], Loss: 0.0919\n",
      "Epoch [278/1000], Loss: 0.0900\n",
      "Epoch [279/1000], Loss: 0.0903\n",
      "Epoch [280/1000], Loss: 0.0895\n",
      "Epoch [281/1000], Loss: 0.0886\n",
      "Epoch [282/1000], Loss: 0.0872\n",
      "Epoch [283/1000], Loss: 0.0849\n",
      "Epoch [284/1000], Loss: 0.0856\n",
      "Epoch [285/1000], Loss: 0.0855\n",
      "Epoch [286/1000], Loss: 0.0851\n",
      "Epoch [287/1000], Loss: 0.0853\n",
      "Epoch [288/1000], Loss: 0.0839\n",
      "Epoch [289/1000], Loss: 0.0834\n",
      "Epoch [290/1000], Loss: 0.0843\n",
      "Epoch [291/1000], Loss: 0.0844\n",
      "Epoch [292/1000], Loss: 0.0845\n",
      "Epoch [293/1000], Loss: 0.0846\n",
      "Epoch [294/1000], Loss: 0.0838\n",
      "Epoch [295/1000], Loss: 0.0828\n",
      "Epoch [296/1000], Loss: 0.0833\n",
      "Epoch [297/1000], Loss: 0.0816\n",
      "Epoch [298/1000], Loss: 0.0822\n",
      "Epoch [299/1000], Loss: 0.0820\n",
      "Epoch [300/1000], Loss: 0.0832\n",
      "Epoch [301/1000], Loss: 0.0845\n",
      "Epoch [302/1000], Loss: 0.0829\n",
      "Epoch [303/1000], Loss: 0.0826\n",
      "Epoch [304/1000], Loss: 0.0831\n",
      "Epoch [305/1000], Loss: 0.0816\n",
      "Epoch [306/1000], Loss: 0.0830\n",
      "Epoch [307/1000], Loss: 0.0802\n",
      "Epoch [308/1000], Loss: 0.0810\n",
      "Epoch [309/1000], Loss: 0.0813\n",
      "Epoch [310/1000], Loss: 0.0827\n",
      "Epoch [311/1000], Loss: 0.0856\n",
      "Epoch [312/1000], Loss: 0.0878\n",
      "Epoch [313/1000], Loss: 0.0885\n",
      "Epoch [314/1000], Loss: 0.0903\n",
      "Epoch [315/1000], Loss: 0.0885\n",
      "Epoch [316/1000], Loss: 0.0904\n",
      "Epoch [317/1000], Loss: 0.0870\n",
      "Epoch [318/1000], Loss: 0.0834\n",
      "Epoch [319/1000], Loss: 0.0812\n",
      "Epoch [320/1000], Loss: 0.0801\n",
      "Epoch [321/1000], Loss: 0.0801\n",
      "Epoch [322/1000], Loss: 0.0797\n",
      "Epoch [323/1000], Loss: 0.0800\n",
      "Epoch [324/1000], Loss: 0.0804\n",
      "Epoch [325/1000], Loss: 0.0815\n",
      "Epoch [326/1000], Loss: 0.0829\n",
      "Epoch [327/1000], Loss: 0.0840\n",
      "Epoch [328/1000], Loss: 0.0846\n",
      "Epoch [329/1000], Loss: 0.0853\n",
      "Epoch [330/1000], Loss: 0.0873\n",
      "Epoch [331/1000], Loss: 0.0874\n",
      "Epoch [332/1000], Loss: 0.0862\n",
      "Epoch [333/1000], Loss: 0.0804\n",
      "Epoch [334/1000], Loss: 0.0776\n",
      "Epoch [335/1000], Loss: 0.0756\n",
      "Epoch [336/1000], Loss: 0.0812\n",
      "Epoch [337/1000], Loss: 0.0779\n",
      "Epoch [338/1000], Loss: 0.0772\n",
      "Epoch [339/1000], Loss: 0.0769\n",
      "Epoch [340/1000], Loss: 0.0768\n",
      "Epoch [341/1000], Loss: 0.0782\n",
      "Epoch [342/1000], Loss: 0.0790\n",
      "Epoch [343/1000], Loss: 0.0765\n",
      "Epoch [344/1000], Loss: 0.0780\n",
      "Epoch [345/1000], Loss: 0.0770\n",
      "Epoch [346/1000], Loss: 0.0756\n",
      "Epoch [347/1000], Loss: 0.0752\n",
      "Epoch [348/1000], Loss: 0.0764\n",
      "Epoch [349/1000], Loss: 0.0762\n",
      "Epoch [350/1000], Loss: 0.0779\n",
      "Epoch [351/1000], Loss: 0.0775\n",
      "Epoch [352/1000], Loss: 0.0796\n",
      "Epoch [353/1000], Loss: 0.0806\n",
      "Epoch [354/1000], Loss: 0.0789\n",
      "Epoch [355/1000], Loss: 0.0793\n",
      "Epoch [356/1000], Loss: 0.0769\n",
      "Epoch [357/1000], Loss: 0.0744\n",
      "Epoch [358/1000], Loss: 0.0772\n",
      "Epoch [359/1000], Loss: 0.0766\n",
      "Epoch [360/1000], Loss: 0.0770\n",
      "Epoch [361/1000], Loss: 0.0767\n",
      "Epoch [362/1000], Loss: 0.0761\n",
      "Epoch [363/1000], Loss: 0.0736\n",
      "Epoch [364/1000], Loss: 0.0727\n",
      "Epoch [365/1000], Loss: 0.0712\n",
      "Epoch [366/1000], Loss: 0.0712\n",
      "Epoch [367/1000], Loss: 0.0699\n",
      "Epoch [368/1000], Loss: 0.0694\n",
      "Epoch [369/1000], Loss: 0.0684\n",
      "Epoch [370/1000], Loss: 0.0683\n",
      "Epoch [371/1000], Loss: 0.0676\n",
      "Epoch [372/1000], Loss: 0.0667\n",
      "Epoch [373/1000], Loss: 0.0675\n",
      "Epoch [374/1000], Loss: 0.0670\n",
      "Epoch [375/1000], Loss: 0.0681\n",
      "Epoch [376/1000], Loss: 0.0689\n",
      "Epoch [377/1000], Loss: 0.0699\n",
      "Epoch [378/1000], Loss: 0.0687\n",
      "Epoch [379/1000], Loss: 0.0689\n",
      "Epoch [380/1000], Loss: 0.0692\n",
      "Epoch [381/1000], Loss: 0.0690\n",
      "Epoch [382/1000], Loss: 0.0692\n",
      "Epoch [383/1000], Loss: 0.0709\n",
      "Epoch [384/1000], Loss: 0.0689\n",
      "Epoch [385/1000], Loss: 0.0692\n",
      "Epoch [386/1000], Loss: 0.0683\n",
      "Epoch [387/1000], Loss: 0.0683\n",
      "Epoch [388/1000], Loss: 0.0688\n",
      "Epoch [389/1000], Loss: 0.0682\n",
      "Epoch [390/1000], Loss: 0.0684\n",
      "Epoch [391/1000], Loss: 0.0675\n",
      "Epoch [392/1000], Loss: 0.0664\n",
      "Epoch [393/1000], Loss: 0.0655\n",
      "Epoch [394/1000], Loss: 0.0669\n",
      "Epoch [395/1000], Loss: 0.0672\n",
      "Epoch [396/1000], Loss: 0.0664\n",
      "Epoch [397/1000], Loss: 0.0661\n",
      "Epoch [398/1000], Loss: 0.0664\n",
      "Epoch [399/1000], Loss: 0.0661\n",
      "Epoch [400/1000], Loss: 0.0656\n",
      "Epoch [401/1000], Loss: 0.0657\n",
      "Epoch [402/1000], Loss: 0.0659\n",
      "Epoch [403/1000], Loss: 0.0672\n",
      "Epoch [404/1000], Loss: 0.0674\n",
      "Epoch [405/1000], Loss: 0.0663\n",
      "Epoch [406/1000], Loss: 0.0669\n",
      "Epoch [407/1000], Loss: 0.0672\n",
      "Epoch [408/1000], Loss: 0.0665\n",
      "Epoch [409/1000], Loss: 0.0657\n",
      "Epoch [410/1000], Loss: 0.0651\n",
      "Epoch [411/1000], Loss: 0.0667\n",
      "Epoch [412/1000], Loss: 0.0659\n",
      "Epoch [413/1000], Loss: 0.0673\n",
      "Epoch [414/1000], Loss: 0.0677\n",
      "Epoch [415/1000], Loss: 0.0672\n",
      "Epoch [416/1000], Loss: 0.0681\n",
      "Epoch [417/1000], Loss: 0.0706\n",
      "Epoch [418/1000], Loss: 0.0710\n",
      "Epoch [419/1000], Loss: 0.0719\n",
      "Epoch [420/1000], Loss: 0.0705\n",
      "Epoch [421/1000], Loss: 0.0706\n",
      "Epoch [422/1000], Loss: 0.0697\n",
      "Epoch [423/1000], Loss: 0.0689\n",
      "Epoch [424/1000], Loss: 0.0703\n",
      "Epoch [425/1000], Loss: 0.0696\n",
      "Epoch [426/1000], Loss: 0.0685\n",
      "Epoch [427/1000], Loss: 0.0665\n",
      "Epoch [428/1000], Loss: 0.0656\n",
      "Epoch [429/1000], Loss: 0.0649\n",
      "Epoch [430/1000], Loss: 0.0651\n",
      "Epoch [431/1000], Loss: 0.0634\n",
      "Epoch [432/1000], Loss: 0.0639\n",
      "Epoch [433/1000], Loss: 0.0647\n",
      "Epoch [434/1000], Loss: 0.0636\n",
      "Epoch [435/1000], Loss: 0.0630\n",
      "Epoch [436/1000], Loss: 0.0648\n",
      "Epoch [437/1000], Loss: 0.0649\n",
      "Epoch [438/1000], Loss: 0.0660\n",
      "Epoch [439/1000], Loss: 0.0657\n",
      "Epoch [440/1000], Loss: 0.0660\n",
      "Epoch [441/1000], Loss: 0.0658\n",
      "Epoch [442/1000], Loss: 0.0639\n",
      "Epoch [443/1000], Loss: 0.0626\n",
      "Epoch [444/1000], Loss: 0.0621\n",
      "Epoch [445/1000], Loss: 0.0616\n",
      "Epoch [446/1000], Loss: 0.0613\n",
      "Epoch [447/1000], Loss: 0.0613\n",
      "Epoch [448/1000], Loss: 0.0614\n",
      "Epoch [449/1000], Loss: 0.0612\n",
      "Epoch [450/1000], Loss: 0.0603\n",
      "Epoch [451/1000], Loss: 0.0630\n",
      "Epoch [452/1000], Loss: 0.0647\n",
      "Epoch [453/1000], Loss: 0.0626\n",
      "Epoch [454/1000], Loss: 0.0636\n",
      "Epoch [455/1000], Loss: 0.0629\n",
      "Epoch [456/1000], Loss: 0.0617\n",
      "Epoch [457/1000], Loss: 0.0605\n",
      "Epoch [458/1000], Loss: 0.0604\n",
      "Epoch [459/1000], Loss: 0.0587\n",
      "Epoch [460/1000], Loss: 0.0595\n",
      "Epoch [461/1000], Loss: 0.0596\n",
      "Epoch [462/1000], Loss: 0.0608\n",
      "Epoch [463/1000], Loss: 0.0593\n",
      "Epoch [464/1000], Loss: 0.0597\n",
      "Epoch [465/1000], Loss: 0.0599\n",
      "Epoch [466/1000], Loss: 0.0615\n",
      "Epoch [467/1000], Loss: 0.0602\n",
      "Epoch [468/1000], Loss: 0.0612\n",
      "Epoch [469/1000], Loss: 0.0594\n",
      "Epoch [470/1000], Loss: 0.0604\n",
      "Epoch [471/1000], Loss: 0.0608\n",
      "Epoch [472/1000], Loss: 0.0599\n",
      "Epoch [473/1000], Loss: 0.0614\n",
      "Epoch [474/1000], Loss: 0.0596\n",
      "Epoch [475/1000], Loss: 0.0593\n",
      "Epoch [476/1000], Loss: 0.0592\n",
      "Epoch [477/1000], Loss: 0.0592\n",
      "Epoch [478/1000], Loss: 0.0592\n",
      "Epoch [479/1000], Loss: 0.0579\n",
      "Epoch [480/1000], Loss: 0.0575\n",
      "Epoch [481/1000], Loss: 0.0572\n",
      "Epoch [482/1000], Loss: 0.0570\n",
      "Epoch [483/1000], Loss: 0.0562\n",
      "Epoch [484/1000], Loss: 0.0571\n",
      "Epoch [485/1000], Loss: 0.0561\n",
      "Epoch [486/1000], Loss: 0.0571\n",
      "Epoch [487/1000], Loss: 0.0576\n",
      "Epoch [488/1000], Loss: 0.0596\n",
      "Epoch [489/1000], Loss: 0.0577\n",
      "Epoch [490/1000], Loss: 0.0574\n",
      "Epoch [491/1000], Loss: 0.0573\n",
      "Epoch [492/1000], Loss: 0.0574\n",
      "Epoch [493/1000], Loss: 0.0567\n",
      "Epoch [494/1000], Loss: 0.0564\n",
      "Epoch [495/1000], Loss: 0.0568\n",
      "Epoch [496/1000], Loss: 0.0569\n",
      "Epoch [497/1000], Loss: 0.0590\n",
      "Epoch [498/1000], Loss: 0.0591\n",
      "Epoch [499/1000], Loss: 0.0608\n",
      "Epoch [500/1000], Loss: 0.0609\n",
      "Epoch [501/1000], Loss: 0.0600\n",
      "Epoch [502/1000], Loss: 0.0604\n",
      "Epoch [503/1000], Loss: 0.0595\n",
      "Epoch [504/1000], Loss: 0.0598\n",
      "Epoch [505/1000], Loss: 0.0564\n",
      "Epoch [506/1000], Loss: 0.0565\n",
      "Epoch [507/1000], Loss: 0.0569\n",
      "Epoch [508/1000], Loss: 0.0561\n",
      "Epoch [509/1000], Loss: 0.0567\n",
      "Epoch [510/1000], Loss: 0.0566\n",
      "Epoch [511/1000], Loss: 0.0550\n",
      "Epoch [512/1000], Loss: 0.0542\n",
      "Epoch [513/1000], Loss: 0.0539\n",
      "Epoch [514/1000], Loss: 0.0549\n",
      "Epoch [515/1000], Loss: 0.0538\n",
      "Epoch [516/1000], Loss: 0.0542\n",
      "Epoch [517/1000], Loss: 0.0545\n",
      "Epoch [518/1000], Loss: 0.0547\n",
      "Epoch [519/1000], Loss: 0.0540\n",
      "Epoch [520/1000], Loss: 0.0535\n",
      "Epoch [521/1000], Loss: 0.0543\n",
      "Epoch [522/1000], Loss: 0.0542\n",
      "Epoch [523/1000], Loss: 0.0545\n",
      "Epoch [524/1000], Loss: 0.0542\n",
      "Epoch [525/1000], Loss: 0.0539\n",
      "Epoch [526/1000], Loss: 0.0529\n",
      "Epoch [527/1000], Loss: 0.0555\n",
      "Epoch [528/1000], Loss: 0.0551\n",
      "Epoch [529/1000], Loss: 0.0536\n",
      "Epoch [530/1000], Loss: 0.0557\n",
      "Epoch [531/1000], Loss: 0.0565\n",
      "Epoch [532/1000], Loss: 0.0549\n",
      "Epoch [533/1000], Loss: 0.0545\n",
      "Epoch [534/1000], Loss: 0.0547\n",
      "Epoch [535/1000], Loss: 0.0549\n",
      "Epoch [536/1000], Loss: 0.0543\n",
      "Epoch [537/1000], Loss: 0.0538\n",
      "Epoch [538/1000], Loss: 0.0537\n",
      "Epoch [539/1000], Loss: 0.0548\n",
      "Epoch [540/1000], Loss: 0.0539\n",
      "Epoch [541/1000], Loss: 0.0520\n",
      "Epoch [542/1000], Loss: 0.0527\n",
      "Epoch [543/1000], Loss: 0.0524\n",
      "Epoch [544/1000], Loss: 0.0536\n",
      "Epoch [545/1000], Loss: 0.0541\n",
      "Epoch [546/1000], Loss: 0.0534\n",
      "Epoch [547/1000], Loss: 0.0538\n",
      "Epoch [548/1000], Loss: 0.0532\n",
      "Epoch [549/1000], Loss: 0.0531\n",
      "Epoch [550/1000], Loss: 0.0527\n",
      "Epoch [551/1000], Loss: 0.0524\n",
      "Epoch [552/1000], Loss: 0.0512\n",
      "Epoch [553/1000], Loss: 0.0508\n",
      "Epoch [554/1000], Loss: 0.0539\n",
      "Epoch [555/1000], Loss: 0.0548\n",
      "Epoch [556/1000], Loss: 0.0537\n",
      "Epoch [557/1000], Loss: 0.0527\n",
      "Epoch [558/1000], Loss: 0.0517\n",
      "Epoch [559/1000], Loss: 0.0521\n",
      "Epoch [560/1000], Loss: 0.0522\n",
      "Epoch [561/1000], Loss: 0.0535\n",
      "Epoch [562/1000], Loss: 0.0534\n",
      "Epoch [563/1000], Loss: 0.0563\n",
      "Epoch [564/1000], Loss: 0.0616\n",
      "Epoch [565/1000], Loss: 0.0567\n",
      "Epoch [566/1000], Loss: 0.0533\n",
      "Epoch [567/1000], Loss: 0.0530\n",
      "Epoch [568/1000], Loss: 0.0527\n",
      "Epoch [569/1000], Loss: 0.0523\n",
      "Epoch [570/1000], Loss: 0.0504\n",
      "Epoch [571/1000], Loss: 0.0513\n",
      "Epoch [572/1000], Loss: 0.0519\n",
      "Epoch [573/1000], Loss: 0.0517\n",
      "Epoch [574/1000], Loss: 0.0524\n",
      "Epoch [575/1000], Loss: 0.0530\n",
      "Epoch [576/1000], Loss: 0.0517\n",
      "Epoch [577/1000], Loss: 0.0524\n",
      "Epoch [578/1000], Loss: 0.0530\n",
      "Epoch [579/1000], Loss: 0.0566\n",
      "Epoch [580/1000], Loss: 0.0555\n",
      "Epoch [581/1000], Loss: 0.0516\n",
      "Epoch [582/1000], Loss: 0.0503\n",
      "Epoch [583/1000], Loss: 0.0497\n",
      "Epoch [584/1000], Loss: 0.0527\n",
      "Epoch [585/1000], Loss: 0.0525\n",
      "Epoch [586/1000], Loss: 0.0499\n",
      "Epoch [587/1000], Loss: 0.0505\n",
      "Epoch [588/1000], Loss: 0.0503\n",
      "Epoch [589/1000], Loss: 0.0525\n",
      "Epoch [590/1000], Loss: 0.0493\n",
      "Epoch [591/1000], Loss: 0.0517\n",
      "Epoch [592/1000], Loss: 0.0502\n",
      "Epoch [593/1000], Loss: 0.0498\n",
      "Epoch [594/1000], Loss: 0.0495\n",
      "Epoch [595/1000], Loss: 0.0486\n",
      "Epoch [596/1000], Loss: 0.0497\n",
      "Epoch [597/1000], Loss: 0.0493\n",
      "Epoch [598/1000], Loss: 0.0511\n",
      "Epoch [599/1000], Loss: 0.0487\n",
      "Epoch [600/1000], Loss: 0.0478\n",
      "Epoch [601/1000], Loss: 0.0489\n",
      "Epoch [602/1000], Loss: 0.0490\n",
      "Epoch [603/1000], Loss: 0.0496\n",
      "Epoch [604/1000], Loss: 0.0489\n",
      "Epoch [605/1000], Loss: 0.0492\n",
      "Epoch [606/1000], Loss: 0.0491\n",
      "Epoch [607/1000], Loss: 0.0494\n",
      "Epoch [608/1000], Loss: 0.0511\n",
      "Epoch [609/1000], Loss: 0.0514\n",
      "Epoch [610/1000], Loss: 0.0515\n",
      "Epoch [611/1000], Loss: 0.0512\n",
      "Epoch [612/1000], Loss: 0.0502\n",
      "Epoch [613/1000], Loss: 0.0492\n",
      "Epoch [614/1000], Loss: 0.0490\n",
      "Epoch [615/1000], Loss: 0.0502\n",
      "Epoch [616/1000], Loss: 0.0499\n",
      "Epoch [617/1000], Loss: 0.0494\n",
      "Epoch [618/1000], Loss: 0.0487\n",
      "Epoch [619/1000], Loss: 0.0487\n",
      "Epoch [620/1000], Loss: 0.0497\n",
      "Epoch [621/1000], Loss: 0.0497\n",
      "Epoch [622/1000], Loss: 0.0479\n",
      "Epoch [623/1000], Loss: 0.0493\n",
      "Epoch [624/1000], Loss: 0.0494\n",
      "Epoch [625/1000], Loss: 0.0484\n",
      "Epoch [626/1000], Loss: 0.0484\n",
      "Epoch [627/1000], Loss: 0.0492\n",
      "Epoch [628/1000], Loss: 0.0509\n",
      "Epoch [629/1000], Loss: 0.0510\n",
      "Epoch [630/1000], Loss: 0.0520\n",
      "Epoch [631/1000], Loss: 0.0505\n",
      "Epoch [632/1000], Loss: 0.0501\n",
      "Epoch [633/1000], Loss: 0.0501\n",
      "Epoch [634/1000], Loss: 0.0512\n",
      "Epoch [635/1000], Loss: 0.0479\n",
      "Epoch [636/1000], Loss: 0.0470\n",
      "Epoch [637/1000], Loss: 0.0470\n",
      "Epoch [638/1000], Loss: 0.0498\n",
      "Epoch [639/1000], Loss: 0.0493\n",
      "Epoch [640/1000], Loss: 0.0477\n",
      "Epoch [641/1000], Loss: 0.0464\n",
      "Epoch [642/1000], Loss: 0.0463\n",
      "Epoch [643/1000], Loss: 0.0470\n",
      "Epoch [644/1000], Loss: 0.0468\n",
      "Epoch [645/1000], Loss: 0.0479\n",
      "Epoch [646/1000], Loss: 0.0481\n",
      "Epoch [647/1000], Loss: 0.0488\n",
      "Epoch [648/1000], Loss: 0.0477\n",
      "Epoch [649/1000], Loss: 0.0475\n",
      "Epoch [650/1000], Loss: 0.0476\n",
      "Epoch [651/1000], Loss: 0.0478\n",
      "Epoch [652/1000], Loss: 0.0494\n",
      "Epoch [653/1000], Loss: 0.0481\n",
      "Epoch [654/1000], Loss: 0.0491\n",
      "Epoch [655/1000], Loss: 0.0480\n",
      "Epoch [656/1000], Loss: 0.0485\n",
      "Epoch [657/1000], Loss: 0.0460\n",
      "Epoch [658/1000], Loss: 0.0453\n",
      "Epoch [659/1000], Loss: 0.0446\n",
      "Epoch [660/1000], Loss: 0.0436\n",
      "Epoch [661/1000], Loss: 0.0446\n",
      "Epoch [662/1000], Loss: 0.0443\n",
      "Epoch [663/1000], Loss: 0.0442\n",
      "Epoch [664/1000], Loss: 0.0451\n",
      "Epoch [665/1000], Loss: 0.0462\n",
      "Epoch [666/1000], Loss: 0.0455\n",
      "Epoch [667/1000], Loss: 0.0450\n",
      "Epoch [668/1000], Loss: 0.0463\n",
      "Epoch [669/1000], Loss: 0.0456\n",
      "Epoch [670/1000], Loss: 0.0452\n",
      "Epoch [671/1000], Loss: 0.0458\n",
      "Epoch [672/1000], Loss: 0.0458\n",
      "Epoch [673/1000], Loss: 0.0454\n",
      "Epoch [674/1000], Loss: 0.0467\n",
      "Epoch [675/1000], Loss: 0.0469\n",
      "Epoch [676/1000], Loss: 0.0473\n",
      "Epoch [677/1000], Loss: 0.0463\n",
      "Epoch [678/1000], Loss: 0.0470\n",
      "Epoch [679/1000], Loss: 0.0462\n",
      "Epoch [680/1000], Loss: 0.0490\n",
      "Epoch [681/1000], Loss: 0.0485\n",
      "Epoch [682/1000], Loss: 0.0479\n",
      "Epoch [683/1000], Loss: 0.0472\n",
      "Epoch [684/1000], Loss: 0.0462\n",
      "Epoch [685/1000], Loss: 0.0460\n",
      "Epoch [686/1000], Loss: 0.0467\n",
      "Epoch [687/1000], Loss: 0.0456\n",
      "Epoch [688/1000], Loss: 0.0456\n",
      "Epoch [689/1000], Loss: 0.0453\n",
      "Epoch [690/1000], Loss: 0.0465\n",
      "Epoch [691/1000], Loss: 0.0468\n",
      "Epoch [692/1000], Loss: 0.0453\n",
      "Epoch [693/1000], Loss: 0.0457\n",
      "Epoch [694/1000], Loss: 0.0447\n",
      "Epoch [695/1000], Loss: 0.0435\n",
      "Epoch [696/1000], Loss: 0.0436\n",
      "Epoch [697/1000], Loss: 0.0429\n",
      "Epoch [698/1000], Loss: 0.0440\n",
      "Epoch [699/1000], Loss: 0.0453\n",
      "Epoch [700/1000], Loss: 0.0446\n",
      "Epoch [701/1000], Loss: 0.0468\n",
      "Epoch [702/1000], Loss: 0.0464\n",
      "Epoch [703/1000], Loss: 0.0448\n",
      "Epoch [704/1000], Loss: 0.0441\n",
      "Epoch [705/1000], Loss: 0.0444\n",
      "Epoch [706/1000], Loss: 0.0448\n",
      "Epoch [707/1000], Loss: 0.0441\n",
      "Epoch [708/1000], Loss: 0.0456\n",
      "Epoch [709/1000], Loss: 0.0441\n",
      "Epoch [710/1000], Loss: 0.0432\n",
      "Epoch [711/1000], Loss: 0.0441\n",
      "Epoch [712/1000], Loss: 0.0434\n",
      "Epoch [713/1000], Loss: 0.0425\n",
      "Epoch [714/1000], Loss: 0.0444\n",
      "Epoch [715/1000], Loss: 0.0456\n",
      "Epoch [716/1000], Loss: 0.0457\n",
      "Epoch [717/1000], Loss: 0.0443\n",
      "Epoch [718/1000], Loss: 0.0439\n",
      "Epoch [719/1000], Loss: 0.0437\n",
      "Epoch [720/1000], Loss: 0.0439\n",
      "Epoch [721/1000], Loss: 0.0426\n",
      "Epoch [722/1000], Loss: 0.0422\n",
      "Epoch [723/1000], Loss: 0.0427\n",
      "Epoch [724/1000], Loss: 0.0440\n",
      "Epoch [725/1000], Loss: 0.0440\n",
      "Epoch [726/1000], Loss: 0.0441\n",
      "Epoch [727/1000], Loss: 0.0440\n",
      "Epoch [728/1000], Loss: 0.0443\n",
      "Epoch [729/1000], Loss: 0.0439\n",
      "Epoch [730/1000], Loss: 0.0457\n",
      "Epoch [731/1000], Loss: 0.0456\n",
      "Epoch [732/1000], Loss: 0.0446\n",
      "Epoch [733/1000], Loss: 0.0461\n",
      "Epoch [734/1000], Loss: 0.0433\n",
      "Epoch [735/1000], Loss: 0.0430\n",
      "Epoch [736/1000], Loss: 0.0432\n",
      "Epoch [737/1000], Loss: 0.0442\n",
      "Epoch [738/1000], Loss: 0.0448\n",
      "Epoch [739/1000], Loss: 0.0425\n",
      "Epoch [740/1000], Loss: 0.0420\n",
      "Epoch [741/1000], Loss: 0.0423\n",
      "Epoch [742/1000], Loss: 0.0432\n",
      "Epoch [743/1000], Loss: 0.0424\n",
      "Epoch [744/1000], Loss: 0.0423\n",
      "Epoch [745/1000], Loss: 0.0416\n",
      "Epoch [746/1000], Loss: 0.0416\n",
      "Epoch [747/1000], Loss: 0.0412\n",
      "Epoch [748/1000], Loss: 0.0410\n",
      "Epoch [749/1000], Loss: 0.0413\n",
      "Epoch [750/1000], Loss: 0.0418\n",
      "Epoch [751/1000], Loss: 0.0418\n",
      "Epoch [752/1000], Loss: 0.0433\n",
      "Epoch [753/1000], Loss: 0.0438\n",
      "Epoch [754/1000], Loss: 0.0499\n",
      "Epoch [755/1000], Loss: 0.0486\n",
      "Epoch [756/1000], Loss: 0.0441\n",
      "Epoch [757/1000], Loss: 0.0436\n",
      "Epoch [758/1000], Loss: 0.0434\n",
      "Epoch [759/1000], Loss: 0.0436\n",
      "Epoch [760/1000], Loss: 0.0417\n",
      "Epoch [761/1000], Loss: 0.0421\n",
      "Epoch [762/1000], Loss: 0.0423\n",
      "Epoch [763/1000], Loss: 0.0437\n",
      "Epoch [764/1000], Loss: 0.0446\n",
      "Epoch [765/1000], Loss: 0.0436\n",
      "Epoch [766/1000], Loss: 0.0435\n",
      "Epoch [767/1000], Loss: 0.0421\n",
      "Epoch [768/1000], Loss: 0.0422\n",
      "Epoch [769/1000], Loss: 0.0438\n",
      "Epoch [770/1000], Loss: 0.0432\n",
      "Epoch [771/1000], Loss: 0.0436\n",
      "Epoch [772/1000], Loss: 0.0421\n",
      "Epoch [773/1000], Loss: 0.0415\n",
      "Epoch [774/1000], Loss: 0.0410\n",
      "Epoch [775/1000], Loss: 0.0411\n",
      "Epoch [776/1000], Loss: 0.0419\n",
      "Epoch [777/1000], Loss: 0.0413\n",
      "Epoch [778/1000], Loss: 0.0410\n",
      "Epoch [779/1000], Loss: 0.0407\n",
      "Epoch [780/1000], Loss: 0.0399\n",
      "Epoch [781/1000], Loss: 0.0412\n",
      "Epoch [782/1000], Loss: 0.0409\n",
      "Epoch [783/1000], Loss: 0.0402\n",
      "Epoch [784/1000], Loss: 0.0413\n",
      "Epoch [785/1000], Loss: 0.0420\n",
      "Epoch [786/1000], Loss: 0.0410\n",
      "Epoch [787/1000], Loss: 0.0406\n",
      "Epoch [788/1000], Loss: 0.0397\n",
      "Epoch [789/1000], Loss: 0.0406\n",
      "Epoch [790/1000], Loss: 0.0401\n",
      "Epoch [791/1000], Loss: 0.0400\n",
      "Epoch [792/1000], Loss: 0.0399\n",
      "Epoch [793/1000], Loss: 0.0410\n",
      "Epoch [794/1000], Loss: 0.0420\n",
      "Epoch [795/1000], Loss: 0.0413\n",
      "Epoch [796/1000], Loss: 0.0417\n",
      "Epoch [797/1000], Loss: 0.0407\n",
      "Epoch [798/1000], Loss: 0.0401\n",
      "Epoch [799/1000], Loss: 0.0423\n",
      "Epoch [800/1000], Loss: 0.0429\n",
      "Epoch [801/1000], Loss: 0.0409\n",
      "Epoch [802/1000], Loss: 0.0425\n",
      "Epoch [803/1000], Loss: 0.0427\n",
      "Epoch [804/1000], Loss: 0.0428\n",
      "Epoch [805/1000], Loss: 0.0441\n",
      "Epoch [806/1000], Loss: 0.0428\n",
      "Epoch [807/1000], Loss: 0.0421\n",
      "Epoch [808/1000], Loss: 0.0415\n",
      "Epoch [809/1000], Loss: 0.0411\n",
      "Epoch [810/1000], Loss: 0.0403\n",
      "Epoch [811/1000], Loss: 0.0398\n",
      "Epoch [812/1000], Loss: 0.0414\n",
      "Epoch [813/1000], Loss: 0.0431\n",
      "Epoch [814/1000], Loss: 0.0417\n",
      "Epoch [815/1000], Loss: 0.0449\n",
      "Epoch [816/1000], Loss: 0.0452\n",
      "Epoch [817/1000], Loss: 0.0418\n",
      "Epoch [818/1000], Loss: 0.0390\n",
      "Epoch [819/1000], Loss: 0.0384\n",
      "Epoch [820/1000], Loss: 0.0380\n",
      "Epoch [821/1000], Loss: 0.0373\n",
      "Epoch [822/1000], Loss: 0.0375\n",
      "Epoch [823/1000], Loss: 0.0389\n",
      "Epoch [824/1000], Loss: 0.0404\n",
      "Epoch [825/1000], Loss: 0.0405\n",
      "Epoch [826/1000], Loss: 0.0416\n",
      "Epoch [827/1000], Loss: 0.0422\n",
      "Epoch [828/1000], Loss: 0.0421\n",
      "Epoch [829/1000], Loss: 0.0403\n",
      "Epoch [830/1000], Loss: 0.0411\n",
      "Epoch [831/1000], Loss: 0.0442\n",
      "Epoch [832/1000], Loss: 0.0414\n",
      "Epoch [833/1000], Loss: 0.0393\n",
      "Epoch [834/1000], Loss: 0.0394\n",
      "Epoch [835/1000], Loss: 0.0379\n",
      "Epoch [836/1000], Loss: 0.0386\n",
      "Epoch [837/1000], Loss: 0.0393\n",
      "Epoch [838/1000], Loss: 0.0404\n",
      "Epoch [839/1000], Loss: 0.0386\n",
      "Epoch [840/1000], Loss: 0.0380\n",
      "Epoch [841/1000], Loss: 0.0379\n",
      "Epoch [842/1000], Loss: 0.0388\n",
      "Epoch [843/1000], Loss: 0.0403\n",
      "Epoch [844/1000], Loss: 0.0402\n",
      "Epoch [845/1000], Loss: 0.0409\n",
      "Epoch [846/1000], Loss: 0.0420\n",
      "Epoch [847/1000], Loss: 0.0461\n",
      "Epoch [848/1000], Loss: 0.0421\n",
      "Epoch [849/1000], Loss: 0.0405\n",
      "Epoch [850/1000], Loss: 0.0383\n",
      "Epoch [851/1000], Loss: 0.0409\n",
      "Epoch [852/1000], Loss: 0.0403\n",
      "Epoch [853/1000], Loss: 0.0397\n",
      "Epoch [854/1000], Loss: 0.0392\n",
      "Epoch [855/1000], Loss: 0.0402\n",
      "Epoch [856/1000], Loss: 0.0394\n",
      "Epoch [857/1000], Loss: 0.0397\n",
      "Epoch [858/1000], Loss: 0.0401\n",
      "Epoch [859/1000], Loss: 0.0390\n",
      "Epoch [860/1000], Loss: 0.0401\n",
      "Epoch [861/1000], Loss: 0.0395\n",
      "Epoch [862/1000], Loss: 0.0407\n",
      "Epoch [863/1000], Loss: 0.0412\n",
      "Epoch [864/1000], Loss: 0.0395\n",
      "Epoch [865/1000], Loss: 0.0398\n",
      "Epoch [866/1000], Loss: 0.0395\n",
      "Epoch [867/1000], Loss: 0.0405\n",
      "Epoch [868/1000], Loss: 0.0419\n",
      "Epoch [869/1000], Loss: 0.0418\n",
      "Epoch [870/1000], Loss: 0.0399\n",
      "Epoch [871/1000], Loss: 0.0391\n",
      "Epoch [872/1000], Loss: 0.0400\n",
      "Epoch [873/1000], Loss: 0.0405\n",
      "Epoch [874/1000], Loss: 0.0382\n",
      "Epoch [875/1000], Loss: 0.0380\n",
      "Epoch [876/1000], Loss: 0.0386\n",
      "Epoch [877/1000], Loss: 0.0395\n",
      "Epoch [878/1000], Loss: 0.0389\n",
      "Epoch [879/1000], Loss: 0.0385\n",
      "Epoch [880/1000], Loss: 0.0382\n",
      "Epoch [881/1000], Loss: 0.0392\n",
      "Epoch [882/1000], Loss: 0.0382\n",
      "Epoch [883/1000], Loss: 0.0370\n",
      "Epoch [884/1000], Loss: 0.0381\n",
      "Epoch [885/1000], Loss: 0.0395\n",
      "Epoch [886/1000], Loss: 0.0384\n",
      "Epoch [887/1000], Loss: 0.0375\n",
      "Epoch [888/1000], Loss: 0.0379\n",
      "Epoch [889/1000], Loss: 0.0373\n",
      "Epoch [890/1000], Loss: 0.0384\n",
      "Epoch [891/1000], Loss: 0.0374\n",
      "Epoch [892/1000], Loss: 0.0390\n",
      "Epoch [893/1000], Loss: 0.0388\n",
      "Epoch [894/1000], Loss: 0.0397\n",
      "Epoch [895/1000], Loss: 0.0477\n",
      "Epoch [896/1000], Loss: 0.0434\n",
      "Epoch [897/1000], Loss: 0.0388\n",
      "Epoch [898/1000], Loss: 0.0363\n",
      "Epoch [899/1000], Loss: 0.0368\n",
      "Epoch [900/1000], Loss: 0.0377\n",
      "Epoch [901/1000], Loss: 0.0387\n",
      "Epoch [902/1000], Loss: 0.0400\n",
      "Epoch [903/1000], Loss: 0.0399\n",
      "Epoch [904/1000], Loss: 0.0409\n",
      "Epoch [905/1000], Loss: 0.0399\n",
      "Epoch [906/1000], Loss: 0.0416\n",
      "Epoch [907/1000], Loss: 0.0409\n",
      "Epoch [908/1000], Loss: 0.0396\n",
      "Epoch [909/1000], Loss: 0.0386\n",
      "Epoch [910/1000], Loss: 0.0383\n",
      "Epoch [911/1000], Loss: 0.0374\n",
      "Epoch [912/1000], Loss: 0.0369\n",
      "Epoch [913/1000], Loss: 0.0372\n",
      "Epoch [914/1000], Loss: 0.0375\n",
      "Epoch [915/1000], Loss: 0.0389\n",
      "Epoch [916/1000], Loss: 0.0383\n",
      "Epoch [917/1000], Loss: 0.0375\n",
      "Epoch [918/1000], Loss: 0.0372\n",
      "Epoch [919/1000], Loss: 0.0364\n",
      "Epoch [920/1000], Loss: 0.0368\n",
      "Epoch [921/1000], Loss: 0.0364\n",
      "Epoch [922/1000], Loss: 0.0366\n",
      "Epoch [923/1000], Loss: 0.0369\n",
      "Epoch [924/1000], Loss: 0.0359\n",
      "Epoch [925/1000], Loss: 0.0359\n",
      "Epoch [926/1000], Loss: 0.0364\n",
      "Epoch [927/1000], Loss: 0.0352\n",
      "Epoch [928/1000], Loss: 0.0347\n",
      "Epoch [929/1000], Loss: 0.0367\n",
      "Epoch [930/1000], Loss: 0.0372\n",
      "Epoch [931/1000], Loss: 0.0380\n",
      "Epoch [932/1000], Loss: 0.0366\n",
      "Epoch [933/1000], Loss: 0.0354\n",
      "Epoch [934/1000], Loss: 0.0361\n",
      "Epoch [935/1000], Loss: 0.0354\n",
      "Epoch [936/1000], Loss: 0.0357\n",
      "Epoch [937/1000], Loss: 0.0370\n",
      "Epoch [938/1000], Loss: 0.0374\n",
      "Epoch [939/1000], Loss: 0.0371\n",
      "Epoch [940/1000], Loss: 0.0384\n",
      "Epoch [941/1000], Loss: 0.0389\n",
      "Epoch [942/1000], Loss: 0.0390\n",
      "Epoch [943/1000], Loss: 0.0390\n",
      "Epoch [944/1000], Loss: 0.0397\n",
      "Epoch [945/1000], Loss: 0.0392\n",
      "Epoch [946/1000], Loss: 0.0383\n",
      "Epoch [947/1000], Loss: 0.0393\n",
      "Epoch [948/1000], Loss: 0.0380\n",
      "Epoch [949/1000], Loss: 0.0379\n",
      "Epoch [950/1000], Loss: 0.0369\n",
      "Epoch [951/1000], Loss: 0.0393\n",
      "Epoch [952/1000], Loss: 0.0400\n",
      "Epoch [953/1000], Loss: 0.0394\n",
      "Epoch [954/1000], Loss: 0.0373\n",
      "Epoch [955/1000], Loss: 0.0354\n",
      "Epoch [956/1000], Loss: 0.0349\n",
      "Epoch [957/1000], Loss: 0.0377\n",
      "Epoch [958/1000], Loss: 0.0373\n",
      "Epoch [959/1000], Loss: 0.0393\n",
      "Epoch [960/1000], Loss: 0.0361\n",
      "Epoch [961/1000], Loss: 0.0352\n",
      "Epoch [962/1000], Loss: 0.0342\n",
      "Epoch [963/1000], Loss: 0.0346\n",
      "Epoch [964/1000], Loss: 0.0341\n",
      "Epoch [965/1000], Loss: 0.0346\n",
      "Epoch [966/1000], Loss: 0.0357\n",
      "Epoch [967/1000], Loss: 0.0368\n",
      "Epoch [968/1000], Loss: 0.0373\n",
      "Epoch [969/1000], Loss: 0.0380\n",
      "Epoch [970/1000], Loss: 0.0383\n",
      "Epoch [971/1000], Loss: 0.0353\n",
      "Epoch [972/1000], Loss: 0.0356\n",
      "Epoch [973/1000], Loss: 0.0354\n",
      "Epoch [974/1000], Loss: 0.0366\n",
      "Epoch [975/1000], Loss: 0.0360\n",
      "Epoch [976/1000], Loss: 0.0363\n",
      "Epoch [977/1000], Loss: 0.0349\n",
      "Epoch [978/1000], Loss: 0.0363\n",
      "Epoch [979/1000], Loss: 0.0366\n",
      "Epoch [980/1000], Loss: 0.0376\n",
      "Epoch [981/1000], Loss: 0.0361\n",
      "Epoch [982/1000], Loss: 0.0358\n",
      "Epoch [983/1000], Loss: 0.0352\n",
      "Epoch [984/1000], Loss: 0.0344\n",
      "Epoch [985/1000], Loss: 0.0346\n",
      "Epoch [986/1000], Loss: 0.0358\n",
      "Epoch [987/1000], Loss: 0.0363\n",
      "Epoch [988/1000], Loss: 0.0353\n",
      "Epoch [989/1000], Loss: 0.0343\n",
      "Epoch [990/1000], Loss: 0.0353\n",
      "Epoch [991/1000], Loss: 0.0357\n",
      "Epoch [992/1000], Loss: 0.0381\n",
      "Epoch [993/1000], Loss: 0.0358\n",
      "Epoch [994/1000], Loss: 0.0356\n",
      "Epoch [995/1000], Loss: 0.0352\n",
      "Epoch [996/1000], Loss: 0.0361\n",
      "Epoch [997/1000], Loss: 0.0367\n",
      "Epoch [998/1000], Loss: 0.0373\n",
      "Epoch [999/1000], Loss: 0.0377\n",
      "Epoch [1000/1000], Loss: 0.0365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4977957271039486,\n",
       " 0.4725069981068373,\n",
       " 0.452162591740489,\n",
       " 0.4314137063920498,\n",
       " 0.4099830389022827,\n",
       " 0.3879004120826721,\n",
       " 0.3662266358733177,\n",
       " 0.34498695097863674,\n",
       " 0.32453292049467564,\n",
       " 0.3071757238358259,\n",
       " 0.2918539959937334,\n",
       " 0.27758412156254053,\n",
       " 0.2650839611887932,\n",
       " 0.2578728049993515,\n",
       " 0.2543589826673269,\n",
       " 0.2565208626911044,\n",
       " 0.25738430209457874,\n",
       " 0.2505321642383933,\n",
       " 0.24687490053474903,\n",
       " 0.24360256362706423,\n",
       " 0.24072967655956745,\n",
       " 0.2368289725854993,\n",
       " 0.23284560814499855,\n",
       " 0.23114890605211258,\n",
       " 0.23084702715277672,\n",
       " 0.2305209757760167,\n",
       " 0.22993244975805283,\n",
       " 0.22722960636019707,\n",
       " 0.2246315125375986,\n",
       " 0.22226075641810894,\n",
       " 0.22102274931967258,\n",
       " 0.21903486270457506,\n",
       " 0.21737282909452915,\n",
       " 0.21608473360538483,\n",
       " 0.21539924945682287,\n",
       " 0.2144812848418951,\n",
       " 0.21376310102641582,\n",
       " 0.21323053538799286,\n",
       " 0.21219651773571968,\n",
       " 0.21146018616855145,\n",
       " 0.21103897131979465,\n",
       " 0.20956595614552498,\n",
       " 0.21038893423974514,\n",
       " 0.212104681879282,\n",
       " 0.21032448019832373,\n",
       " 0.20856119133532047,\n",
       " 0.2074116887524724,\n",
       " 0.20679724775254726,\n",
       " 0.20569598488509655,\n",
       " 0.20485715940594673,\n",
       " 0.20383625570684671,\n",
       " 0.2036205343902111,\n",
       " 0.2043707137927413,\n",
       " 0.20291525311768055,\n",
       " 0.20208307448774576,\n",
       " 0.2007446838542819,\n",
       " 0.19958824012428522,\n",
       " 0.19972931873053312,\n",
       " 0.19877732172608376,\n",
       " 0.19888218306005,\n",
       " 0.19865163508802652,\n",
       " 0.19852572958916426,\n",
       " 0.1981798531487584,\n",
       " 0.19800927303731441,\n",
       " 0.19628433138132095,\n",
       " 0.194022074341774,\n",
       " 0.1933960895985365,\n",
       " 0.19259859342128038,\n",
       " 0.19248979911208153,\n",
       " 0.19196174759417772,\n",
       " 0.19139251206070185,\n",
       " 0.19108339957892895,\n",
       " 0.1904101762920618,\n",
       " 0.19063574075698853,\n",
       " 0.1918897470459342,\n",
       " 0.19158087950199842,\n",
       " 0.19130553770810366,\n",
       " 0.19174030423164368,\n",
       " 0.19098028913140297,\n",
       " 0.19062249828130007,\n",
       " 0.19044424314051867,\n",
       " 0.18735168129205704,\n",
       " 0.1856507072225213,\n",
       " 0.1852590348571539,\n",
       " 0.1841567326337099,\n",
       " 0.18313575349748135,\n",
       " 0.18334216624498367,\n",
       " 0.18317286111414433,\n",
       " 0.18189488910138607,\n",
       " 0.18060054443776608,\n",
       " 0.18034080136567354,\n",
       " 0.18066634982824326,\n",
       " 0.18093833234161139,\n",
       " 0.1821040492504835,\n",
       " 0.18211616296321154,\n",
       " 0.18029466085135937,\n",
       " 0.17877547815442085,\n",
       " 0.17827718518674374,\n",
       " 0.17715376801788807,\n",
       " 0.17639400530606508,\n",
       " 0.17574349325150251,\n",
       " 0.17522386368364096,\n",
       " 0.17579831648617983,\n",
       " 0.1734571848064661,\n",
       " 0.17430123127996922,\n",
       " 0.17294471431523561,\n",
       " 0.17259075213223696,\n",
       " 0.17195750400424004,\n",
       " 0.17155829351395369,\n",
       " 0.17039929144084454,\n",
       " 0.1712023913860321,\n",
       " 0.1706664003431797,\n",
       " 0.1715720659121871,\n",
       " 0.17152998596429825,\n",
       " 0.16995860636234283,\n",
       " 0.16955456789582968,\n",
       " 0.1692060735076666,\n",
       " 0.16790830250829458,\n",
       " 0.16616007592529058,\n",
       " 0.1674331147223711,\n",
       " 0.16739756241440773,\n",
       " 0.16697506234049797,\n",
       " 0.16425258293747902,\n",
       " 0.16417316906154156,\n",
       " 0.16242389846593142,\n",
       " 0.16252809204161167,\n",
       " 0.16257702559232712,\n",
       " 0.1611248292028904,\n",
       " 0.16138107609003782,\n",
       " 0.16064587980508804,\n",
       " 0.16034377459436655,\n",
       " 0.15919580962508917,\n",
       " 0.1598044354468584,\n",
       " 0.16044614277780056,\n",
       " 0.1602037362754345,\n",
       " 0.15997297037392855,\n",
       " 0.15893984120339155,\n",
       " 0.1587949888780713,\n",
       " 0.15691773314028978,\n",
       " 0.15596319921314716,\n",
       " 0.15346375107765198,\n",
       " 0.15264284703880548,\n",
       " 0.1512848949059844,\n",
       " 0.15092273708432913,\n",
       " 0.15105914883315563,\n",
       " 0.15027974685654044,\n",
       " 0.15030627325177193,\n",
       " 0.14962558401748538,\n",
       " 0.1501099867746234,\n",
       " 0.14974282961338758,\n",
       " 0.15059107821434736,\n",
       " 0.14638385735452175,\n",
       " 0.14424530137330294,\n",
       " 0.14412133488804102,\n",
       " 0.14415353443473577,\n",
       " 0.14427249087020755,\n",
       " 0.14883597288280725,\n",
       " 0.14332783315330744,\n",
       " 0.14066605269908905,\n",
       " 0.1402128259651363,\n",
       " 0.13944826647639275,\n",
       " 0.13792795781046152,\n",
       " 0.1404186594299972,\n",
       " 0.13889109110459685,\n",
       " 0.13790562143549323,\n",
       " 0.13615442207083106,\n",
       " 0.13426344888284802,\n",
       " 0.13262205803766847,\n",
       " 0.13394114514812827,\n",
       " 0.13307316321879625,\n",
       " 0.1376539971679449,\n",
       " 0.13335898658260703,\n",
       " 0.12965921079739928,\n",
       " 0.12842682236805558,\n",
       " 0.12921455688774586,\n",
       " 0.12950915889814496,\n",
       " 0.13244887255132198,\n",
       " 0.12915796181187034,\n",
       " 0.12703145667910576,\n",
       " 0.1256787390448153,\n",
       " 0.1273781112395227,\n",
       " 0.1263279439881444,\n",
       " 0.1264815074391663,\n",
       " 0.12593064084649086,\n",
       " 0.1280355965718627,\n",
       " 0.12611815659329295,\n",
       " 0.12712915800511837,\n",
       " 0.1250983653590083,\n",
       " 0.12604293320327997,\n",
       " 0.1241188901476562,\n",
       " 0.12107931775972247,\n",
       " 0.12140960246324539,\n",
       " 0.12049634987488389,\n",
       " 0.12444488191977143,\n",
       " 0.12164684291929007,\n",
       " 0.11961971689015627,\n",
       " 0.11804948607459664,\n",
       " 0.1173534756526351,\n",
       " 0.11581106763333082,\n",
       " 0.1162487892434001,\n",
       " 0.11546577792614698,\n",
       " 0.11658240761607885,\n",
       " 0.115505944006145,\n",
       " 0.1137006408534944,\n",
       " 0.1135228923521936,\n",
       " 0.11259895283728838,\n",
       " 0.11061485623940825,\n",
       " 0.11193373007699847,\n",
       " 0.11083557363599539,\n",
       " 0.1104104588739574,\n",
       " 0.11021042196080089,\n",
       " 0.1094042812474072,\n",
       " 0.10792759852483869,\n",
       " 0.10958967125043273,\n",
       " 0.10747120855376124,\n",
       " 0.10899945348501205,\n",
       " 0.10848654108121991,\n",
       " 0.10829490190371871,\n",
       " 0.10650516115128994,\n",
       " 0.10567326564341784,\n",
       " 0.10547638311982155,\n",
       " 0.10588676482439041,\n",
       " 0.10452406806871295,\n",
       " 0.10510538192465901,\n",
       " 0.10288925794884562,\n",
       " 0.10191033780574799,\n",
       " 0.10087248031049967,\n",
       " 0.10046249255537987,\n",
       " 0.10044084908440709,\n",
       " 0.10025079315528274,\n",
       " 0.10066051501780748,\n",
       " 0.10052843205630779,\n",
       " 0.09888778487220407,\n",
       " 0.09924056753516197,\n",
       " 0.0984092173166573,\n",
       " 0.09889268362894654,\n",
       " 0.09876919630914927,\n",
       " 0.09933425858616829,\n",
       " 0.09861241467297077,\n",
       " 0.09880434582009912,\n",
       " 0.09831987833604217,\n",
       " 0.0976030477322638,\n",
       " 0.0968702263198793,\n",
       " 0.09798288950696588,\n",
       " 0.09693012898787856,\n",
       " 0.09711573366075754,\n",
       " 0.09715882642194629,\n",
       " 0.0960370465181768,\n",
       " 0.09345088852569461,\n",
       " 0.09376049740239978,\n",
       " 0.0929741314612329,\n",
       " 0.0940896300598979,\n",
       " 0.09383632568642497,\n",
       " 0.09413875406607985,\n",
       " 0.0941739697009325,\n",
       " 0.09450654592365026,\n",
       " 0.09394367877393961,\n",
       " 0.09542874060571194,\n",
       " 0.09431583620607853,\n",
       " 0.0934567372314632,\n",
       " 0.09293136885389686,\n",
       " 0.0905615366064012,\n",
       " 0.0910869836807251,\n",
       " 0.09074728423729539,\n",
       " 0.09016989590600133,\n",
       " 0.09206309309229255,\n",
       " 0.0936614889651537,\n",
       " 0.09177869465202093,\n",
       " 0.09116964088752866,\n",
       " 0.09057049406692386,\n",
       " 0.09084619721397758,\n",
       " 0.09246295737102628,\n",
       " 0.09297398431226611,\n",
       " 0.09189365012571216,\n",
       " 0.09128117235377431,\n",
       " 0.09354516910389066,\n",
       " 0.09191317204385996,\n",
       " 0.09001129446551204,\n",
       " 0.0903456718660891,\n",
       " 0.08946444187313318,\n",
       " 0.08860263181850314,\n",
       " 0.08722220500931144,\n",
       " 0.08486405713483691,\n",
       " 0.08560422295704484,\n",
       " 0.08552677510306239,\n",
       " 0.0851151617243886,\n",
       " 0.08534147264435887,\n",
       " 0.08393932320177555,\n",
       " 0.08341292897239327,\n",
       " 0.08433734066784382,\n",
       " 0.08436057111248374,\n",
       " 0.08448949735611677,\n",
       " 0.08462689444422722,\n",
       " 0.083808075170964,\n",
       " 0.08280590642243624,\n",
       " 0.08330131834372878,\n",
       " 0.08164304541423917,\n",
       " 0.08218308910727501,\n",
       " 0.081959443166852,\n",
       " 0.08316188491880894,\n",
       " 0.08449404127895832,\n",
       " 0.082933752797544,\n",
       " 0.0825604572892189,\n",
       " 0.08310002973303199,\n",
       " 0.0816322616301477,\n",
       " 0.08297206088900566,\n",
       " 0.08023919258266687,\n",
       " 0.08097093971446157,\n",
       " 0.08126652706414461,\n",
       " 0.08267836272716522,\n",
       " 0.08558118855580688,\n",
       " 0.08781940955668688,\n",
       " 0.08845140878111124,\n",
       " 0.09025645349174738,\n",
       " 0.08852056367322803,\n",
       " 0.0904399692080915,\n",
       " 0.08697429904714227,\n",
       " 0.08335366286337376,\n",
       " 0.0812179259955883,\n",
       " 0.08007728261873126,\n",
       " 0.08006934355944395,\n",
       " 0.0796890719793737,\n",
       " 0.08002702193334699,\n",
       " 0.08042281027883291,\n",
       " 0.08152873907238245,\n",
       " 0.08286278834566474,\n",
       " 0.08398032188415527,\n",
       " 0.08462485764175653,\n",
       " 0.08531714975833893,\n",
       " 0.08725426578894258,\n",
       " 0.08742117183282971,\n",
       " 0.08622486470267177,\n",
       " 0.0803720224648714,\n",
       " 0.07763062696903944,\n",
       " 0.07556971814483404,\n",
       " 0.08122526668012142,\n",
       " 0.07790417084470391,\n",
       " 0.07715526036918163,\n",
       " 0.07688031671568751,\n",
       " 0.07684543821960688,\n",
       " 0.07823102781549096,\n",
       " 0.07899763947352767,\n",
       " 0.07646813010796905,\n",
       " 0.07802384812384844,\n",
       " 0.07699021557345986,\n",
       " 0.07559399446472526,\n",
       " 0.07521463977172971,\n",
       " 0.07640802394598722,\n",
       " 0.07623985223472118,\n",
       " 0.07785853510722518,\n",
       " 0.07747526373714209,\n",
       " 0.07962702494114637,\n",
       " 0.08059557899832726,\n",
       " 0.07893141359090805,\n",
       " 0.07925890712067485,\n",
       " 0.07687396835535765,\n",
       " 0.07439541257917881,\n",
       " 0.07720998441800475,\n",
       " 0.07664039498195052,\n",
       " 0.07695295428857207,\n",
       " 0.07671761373057961,\n",
       " 0.07611080911010504,\n",
       " 0.07356045302003622,\n",
       " 0.0727466787211597,\n",
       " 0.07121985568664968,\n",
       " 0.07116843899711967,\n",
       " 0.06986915995366871,\n",
       " 0.06943218689411879,\n",
       " 0.06843102118000388,\n",
       " 0.06827129889279604,\n",
       " 0.06761491228826344,\n",
       " 0.0666610284242779,\n",
       " 0.06753697502426803,\n",
       " 0.06699556624516845,\n",
       " 0.06810399726964533,\n",
       " 0.06894886842928827,\n",
       " 0.0698961850721389,\n",
       " 0.06865031993947923,\n",
       " 0.06894323788583279,\n",
       " 0.06922786240465939,\n",
       " 0.06896883179433644,\n",
       " 0.06918902136385441,\n",
       " 0.07085485151037574,\n",
       " 0.06887579150497913,\n",
       " 0.06918818969279528,\n",
       " 0.06831032922491431,\n",
       " 0.06831444380804896,\n",
       " 0.06875131651759148,\n",
       " 0.06823318847455084,\n",
       " 0.0683584832586348,\n",
       " 0.06746092438697815,\n",
       " 0.06641592015512288,\n",
       " 0.06552513595670462,\n",
       " 0.06686829007230699,\n",
       " 0.06719180708751082,\n",
       " 0.06635628826916218,\n",
       " 0.06612606556154788,\n",
       " 0.06638651411049068,\n",
       " 0.06608939357101917,\n",
       " 0.06556462519802153,\n",
       " 0.06573834083974361,\n",
       " 0.06591506535187364,\n",
       " 0.06715469062328339,\n",
       " 0.06742555811069906,\n",
       " 0.0662773463409394,\n",
       " 0.06692155846394598,\n",
       " 0.06716391444206238,\n",
       " 0.06654505315236747,\n",
       " 0.06569603248499334,\n",
       " 0.06508461036719382,\n",
       " 0.06666605453938246,\n",
       " 0.06587350252084434,\n",
       " 0.06734112557023764,\n",
       " 0.06773271434940398,\n",
       " 0.0671899460721761,\n",
       " 0.06811779900453985,\n",
       " 0.07060098135843873,\n",
       " 0.0710234462749213,\n",
       " 0.0719113158993423,\n",
       " 0.07053225603885949,\n",
       " 0.07055074721574783,\n",
       " 0.06965392897836864,\n",
       " 0.06892165914177895,\n",
       " 0.07031772937625647,\n",
       " 0.06957307178527117,\n",
       " 0.06847840966656804,\n",
       " 0.06647140067070723,\n",
       " 0.06560832960531116,\n",
       " 0.06488641700707376,\n",
       " 0.06506733316928148,\n",
       " 0.06336859823204577,\n",
       " 0.06385095417499542,\n",
       " 0.06472983723506331,\n",
       " 0.06359950662590563,\n",
       " 0.06303284340538085,\n",
       " 0.0648397053591907,\n",
       " 0.06492391019128263,\n",
       " 0.06595684844069183,\n",
       " 0.06565475673414767,\n",
       " 0.06597141362726688,\n",
       " 0.06580647267401218,\n",
       " 0.06394411134533584,\n",
       " 0.06259258883073926,\n",
       " 0.062095359433442354,\n",
       " 0.06164792622439563,\n",
       " 0.061343272449448705,\n",
       " 0.0613125532399863,\n",
       " 0.06142992083914578,\n",
       " 0.06116616423241794,\n",
       " 0.06025900645181537,\n",
       " 0.06302597257308662,\n",
       " 0.06469211354851723,\n",
       " 0.06264124298468232,\n",
       " 0.06361890980042517,\n",
       " 0.06286818301305175,\n",
       " 0.0617274004034698,\n",
       " 0.06049443036317825,\n",
       " 0.06037184991873801,\n",
       " 0.058697917964309454,\n",
       " 0.059464287012815475,\n",
       " 0.05956421187147498,\n",
       " 0.06082344800233841,\n",
       " 0.0592503072693944,\n",
       " 0.059707267908379436,\n",
       " 0.05986095475964248,\n",
       " 0.06151179294101894,\n",
       " 0.060232835123315454,\n",
       " 0.061244646552950144,\n",
       " 0.05944082629866898,\n",
       " 0.060403333278372884,\n",
       " 0.060769270872697234,\n",
       " 0.05989096011035144,\n",
       " 0.061356098391115665,\n",
       " 0.05955594056285918,\n",
       " 0.059341732412576675,\n",
       " 0.059166935505345464,\n",
       " 0.05923995841294527,\n",
       " 0.059192962013185024,\n",
       " 0.057894619880244136,\n",
       " 0.0574795906431973,\n",
       " 0.057206341065466404,\n",
       " 0.05704249581322074,\n",
       " 0.056238635210320354,\n",
       " 0.05714452522806823,\n",
       " 0.05611790553666651,\n",
       " 0.057062487583607435,\n",
       " 0.05755393183790147,\n",
       " 0.05962552619166672,\n",
       " 0.0576821977738291,\n",
       " 0.05744540668092668,\n",
       " 0.0572806631680578,\n",
       " 0.05743629625067115,\n",
       " 0.05674904049374163,\n",
       " 0.056365002412348986,\n",
       " 0.05683145043440163,\n",
       " 0.05687371385283768,\n",
       " 0.059022277826443315,\n",
       " 0.05914888228289783,\n",
       " 0.06077349162660539,\n",
       " 0.06092481641098857,\n",
       " 0.06004230282269418,\n",
       " 0.06042070616967976,\n",
       " 0.059472359251230955,\n",
       " 0.05977931641973555,\n",
       " 0.056398154236376286,\n",
       " 0.05651613092049956,\n",
       " 0.05692559410817921,\n",
       " 0.0560846549924463,\n",
       " 0.056663637049496174,\n",
       " 0.056591999949887395,\n",
       " 0.054958137683570385,\n",
       " 0.054162977961823344,\n",
       " 0.05391714465804398,\n",
       " 0.05485770432278514,\n",
       " 0.05378625076264143,\n",
       " 0.054154309909790754,\n",
       " 0.054524261271581054,\n",
       " 0.054668314987793565,\n",
       " 0.05399582884274423,\n",
       " 0.0535209693480283,\n",
       " 0.05425513838417828,\n",
       " 0.054164131404832006,\n",
       " 0.05454717669636011,\n",
       " 0.05418744497001171,\n",
       " 0.05387590220198035,\n",
       " 0.05290287151001394,\n",
       " 0.05554437730461359,\n",
       " 0.055050144670531154,\n",
       " 0.05362575617618859,\n",
       " 0.055683232843875885,\n",
       " 0.05647458368912339,\n",
       " 0.05492443032562733,\n",
       " 0.05451870011165738,\n",
       " 0.05465746158733964,\n",
       " 0.05494904378429055,\n",
       " 0.054339163238182664,\n",
       " 0.05382960592396557,\n",
       " 0.053709415486082435,\n",
       " 0.05482730339281261,\n",
       " 0.05391803523525596,\n",
       " 0.05199718801304698,\n",
       " 0.05267701530829072,\n",
       " 0.052350444020703435,\n",
       " 0.05355351325124502,\n",
       " 0.054108748910948634,\n",
       " 0.05337919434532523,\n",
       " 0.053750673308968544,\n",
       " 0.05320273130200803,\n",
       " 0.05312957754358649,\n",
       " 0.052669443655759096,\n",
       " 0.052445472683757544,\n",
       " 0.0512436053249985,\n",
       " 0.05079957190901041,\n",
       " 0.05389595264568925,\n",
       " 0.05476783332414925,\n",
       " 0.053702401695773005,\n",
       " 0.05265200650319457,\n",
       " 0.05165117257274687,\n",
       " 0.05205631139688194,\n",
       " 0.0521679010707885,\n",
       " 0.05352835217490792,\n",
       " 0.05339096789248288,\n",
       " 0.05628705257549882,\n",
       " 0.061603518668562174,\n",
       " 0.05672392505221069,\n",
       " 0.053312042728066444,\n",
       " 0.05302275926806033,\n",
       " 0.05268853507004678,\n",
       " 0.05232825689017773,\n",
       " 0.050365114817395806,\n",
       " 0.05131154740229249,\n",
       " 0.051928581437096,\n",
       " 0.05167727661319077,\n",
       " 0.052350327372550964,\n",
       " 0.05303190881386399,\n",
       " 0.05167447868734598,\n",
       " 0.052375696366652846,\n",
       " 0.053041655104607344,\n",
       " 0.056588478619232774,\n",
       " 0.05548032675869763,\n",
       " 0.0516427056863904,\n",
       " 0.050279973074793816,\n",
       " 0.04966800822876394,\n",
       " 0.052741315914317966,\n",
       " 0.05246833711862564,\n",
       " 0.049870459362864494,\n",
       " 0.050475501688197255,\n",
       " 0.050285365199670196,\n",
       " 0.052522874204441905,\n",
       " 0.04926602030172944,\n",
       " 0.05166316078975797,\n",
       " 0.05023896787315607,\n",
       " 0.049816938349977136,\n",
       " 0.04953453643247485,\n",
       " 0.048606848111376166,\n",
       " 0.04965054546482861,\n",
       " 0.04929935303516686,\n",
       " 0.05105344019830227,\n",
       " 0.04867828404530883,\n",
       " 0.04780827974900603,\n",
       " 0.048853308660909534,\n",
       " 0.04904002230614424,\n",
       " 0.04960513487458229,\n",
       " 0.048946378054097295,\n",
       " 0.049187954515218735,\n",
       " 0.04910800978541374,\n",
       " 0.04939136770553887,\n",
       " 0.05113534815609455,\n",
       " 0.05142764328047633,\n",
       " 0.051463170908391476,\n",
       " 0.05119294533506036,\n",
       " 0.050245109712705016,\n",
       " 0.04916525073349476,\n",
       " 0.04898052476346493,\n",
       " 0.05024225544184446,\n",
       " 0.049873540410771966,\n",
       " 0.04938060906715691,\n",
       " 0.0486536945682019,\n",
       " 0.04865312576293945,\n",
       " 0.049663602374494076,\n",
       " 0.049657702911645174,\n",
       " 0.047942971577867866,\n",
       " 0.049336433643475175,\n",
       " 0.04943874920718372,\n",
       " 0.048401735723018646,\n",
       " 0.048417529789730906,\n",
       " 0.04924702341668308,\n",
       " 0.050912582548335195,\n",
       " 0.050973720382899046,\n",
       " 0.05203942954540253,\n",
       " 0.05048668454401195,\n",
       " 0.05012623150832951,\n",
       " 0.05010143551044166,\n",
       " 0.05118811340071261,\n",
       " 0.04791212338022888,\n",
       " 0.04702464188449085,\n",
       " 0.047003791434690356,\n",
       " 0.049780412344262004,\n",
       " 0.049265349516645074,\n",
       " 0.047654022462666035,\n",
       " 0.04642792139202356,\n",
       " 0.04631296428851783,\n",
       " 0.04697828087955713,\n",
       " 0.046839724062010646,\n",
       " 0.04793604160659015,\n",
       " 0.04810395045205951,\n",
       " 0.04875961970537901,\n",
       " 0.047720140079036355,\n",
       " 0.04747773241251707,\n",
       " 0.04756078659556806,\n",
       " 0.04782401630654931,\n",
       " 0.04939011321403086,\n",
       " 0.048129250993952155,\n",
       " 0.049116546753793955,\n",
       " 0.048041689209640026,\n",
       " 0.04849250637926161,\n",
       " 0.04601215082220733,\n",
       " 0.045314268907532096,\n",
       " 0.04463247652165592,\n",
       " 0.04359886748716235,\n",
       " 0.04457404790446162,\n",
       " 0.044340703170746565,\n",
       " 0.04415733413770795,\n",
       " 0.04506173892877996,\n",
       " 0.04617227730341256,\n",
       " 0.045495825819671154,\n",
       " 0.044965450186282396,\n",
       " 0.04628082853741944,\n",
       " 0.04559385380707681,\n",
       " 0.045211115619167686,\n",
       " 0.04583971947431564,\n",
       " 0.04575821035541594,\n",
       " 0.04537444398738444,\n",
       " 0.04669149499386549,\n",
       " 0.04690672014839947,\n",
       " 0.047316627111285925,\n",
       " 0.04629757744260132,\n",
       " 0.04702328098937869,\n",
       " 0.046186540741473436,\n",
       " 0.04899565293453634,\n",
       " 0.0484729737509042,\n",
       " 0.04794771037995815,\n",
       " 0.04718953603878617,\n",
       " 0.046219138661399484,\n",
       " 0.04598503722809255,\n",
       " 0.04671578691340983,\n",
       " 0.04560832562856376,\n",
       " 0.04555393313057721,\n",
       " 0.045306995743885636,\n",
       " 0.0465143327601254,\n",
       " 0.04676954890601337,\n",
       " 0.04530115774832666,\n",
       " 0.045699391746893525,\n",
       " 0.044705562526360154,\n",
       " 0.043527214555069804,\n",
       " 0.04364171624183655,\n",
       " 0.04286409611813724,\n",
       " 0.04398738965392113,\n",
       " 0.04527049697935581,\n",
       " 0.04463751963339746,\n",
       " 0.04683979763649404,\n",
       " 0.04637298337183893,\n",
       " 0.04478750634007156,\n",
       " 0.044062581146135926,\n",
       " 0.044436992378905416,\n",
       " 0.044773194240406156,\n",
       " 0.04409974324516952,\n",
       " 0.04561897832900286,\n",
       " 0.04414465953595936,\n",
       " 0.04323341557756066,\n",
       " 0.04414144204929471,\n",
       " 0.043445884017273784,\n",
       " 0.0425014803186059,\n",
       " 0.04437481821514666,\n",
       " 0.04556416976265609,\n",
       " 0.045678898226469755,\n",
       " 0.044286228250712156,\n",
       " 0.04394307732582092,\n",
       " 0.043716033454984426,\n",
       " 0.04388958006165922,\n",
       " 0.042598469415679574,\n",
       " 0.04215912381187081,\n",
       " 0.042701966129243374,\n",
       " 0.04397596558555961,\n",
       " 0.0440470646135509,\n",
       " 0.04410110181197524,\n",
       " 0.04395182221196592,\n",
       " 0.044345786329358816,\n",
       " 0.04391548130661249,\n",
       " 0.04569737147539854,\n",
       " 0.04556853836402297,\n",
       " 0.04460057243704796,\n",
       " 0.04610223229974508,\n",
       " 0.04327722359448671,\n",
       " 0.042988179717212915,\n",
       " 0.04318034602329135,\n",
       " 0.0441739649977535,\n",
       " 0.04480890370905399,\n",
       " 0.042450908571481705,\n",
       " 0.0419872896745801,\n",
       " 0.042334004770964384,\n",
       " 0.04316156799905002,\n",
       " 0.042415403528138995,\n",
       " 0.042327332543209195,\n",
       " 0.04155990807339549,\n",
       " 0.04155681747943163,\n",
       " 0.04119100561365485,\n",
       " 0.04097480303607881,\n",
       " 0.041268551256507635,\n",
       " 0.04178333003073931,\n",
       " 0.041790520772337914,\n",
       " 0.04334406973794103,\n",
       " 0.0438130097463727,\n",
       " 0.04985499591566622,\n",
       " 0.04860906023532152,\n",
       " 0.044084722409024835,\n",
       " 0.04361458518542349,\n",
       " 0.04342865920625627,\n",
       " 0.04362316429615021,\n",
       " 0.04172373143956065,\n",
       " 0.042082535568624735,\n",
       " 0.04229314182884991,\n",
       " 0.04366665054112673,\n",
       " 0.044612002559006214,\n",
       " 0.04361243802122772,\n",
       " 0.04352776752784848,\n",
       " 0.04209729633294046,\n",
       " 0.04215749097056687,\n",
       " 0.04383096331730485,\n",
       " 0.04319069697521627,\n",
       " 0.04363015876151621,\n",
       " 0.04212060151621699,\n",
       " 0.041522740153595805,\n",
       " 0.04097390244714916,\n",
       " 0.04106093244627118,\n",
       " 0.04193571489304304,\n",
       " 0.04130654432810843,\n",
       " 0.04099041689187288,\n",
       " 0.040700233075767756,\n",
       " 0.03985264874063432,\n",
       " 0.04124478669837117,\n",
       " 0.040869414107874036,\n",
       " 0.04022473515942693,\n",
       " 0.041327525628730655,\n",
       " 0.042005712166428566,\n",
       " 0.040974387899041176,\n",
       " 0.040569416247308254,\n",
       " 0.0397085587028414,\n",
       " 0.04060393455438316,\n",
       " 0.0400780332274735,\n",
       " 0.03998243412934244,\n",
       " 0.03994809999130666,\n",
       " 0.04096887959167361,\n",
       " 0.04199232114478946,\n",
       " 0.04133122996427119,\n",
       " 0.04165676701813936,\n",
       " 0.04066459182649851,\n",
       " 0.04014617088250816,\n",
       " 0.042287709191441536,\n",
       " 0.04287249222397804,\n",
       " 0.040858222637325525,\n",
       " 0.04249225091189146,\n",
       " 0.04267901764251292,\n",
       " 0.042837082175537944,\n",
       " 0.04414172284305096,\n",
       " 0.04283493524417281,\n",
       " 0.042125774547457695,\n",
       " 0.04151355009526014,\n",
       " 0.041116439271718264,\n",
       " 0.04025891562923789,\n",
       " 0.039844808634370565,\n",
       " 0.04141797870397568,\n",
       " 0.043138843262568116,\n",
       " 0.04169039917178452,\n",
       " 0.04491260787472129,\n",
       " 0.04523318517021835,\n",
       " 0.041762316366657615,\n",
       " 0.03896464151330292,\n",
       " 0.03839821252040565,\n",
       " 0.03797517507337034,\n",
       " 0.03731819172389805,\n",
       " 0.03753441013395786,\n",
       " 0.03893213882111013,\n",
       " 0.040392433758825064,\n",
       " 0.04051322257146239,\n",
       " 0.04160692705772817,\n",
       " 0.042179502081125975,\n",
       " 0.042074690805748105,\n",
       " 0.04032680741511285,\n",
       " 0.041089583188295364,\n",
       " 0.04422761942259967,\n",
       " 0.04135730187408626,\n",
       " 0.03934749937616289,\n",
       " 0.03944567870348692,\n",
       " 0.03788190730847418,\n",
       " 0.038596840342506766,\n",
       " 0.039327207021415234,\n",
       " 0.040449210442602634,\n",
       " 0.03856508480384946,\n",
       " 0.037998844869434834,\n",
       " 0.0379381834063679,\n",
       " 0.038771322229877114,\n",
       " 0.04029464349150658,\n",
       " 0.04021228617057204,\n",
       " 0.04089359729550779,\n",
       " 0.041971499333158135,\n",
       " 0.04607559903524816,\n",
       " 0.04206862114369869,\n",
       " 0.040466867154464126,\n",
       " 0.03831669292412698,\n",
       " 0.040939808590337634,\n",
       " 0.04034545528702438,\n",
       " 0.039680418791249394,\n",
       " 0.039229354122653604,\n",
       " 0.040189969819039106,\n",
       " 0.039413067512214184,\n",
       " 0.03965314198285341,\n",
       " 0.04013012605719268,\n",
       " 0.03902923408895731,\n",
       " 0.04014910035766661,\n",
       " 0.03952399152331054,\n",
       " 0.04065706953406334,\n",
       " 0.04120898828841746,\n",
       " 0.03947358182631433,\n",
       " 0.03984081046655774,\n",
       " 0.03948073019273579,\n",
       " 0.04051962750963867,\n",
       " 0.04188423347659409,\n",
       " 0.04178238147869706,\n",
       " 0.03987368429079652,\n",
       " 0.039124633418396115,\n",
       " 0.040039598010480404,\n",
       " 0.040547242388129234,\n",
       " 0.03821061784401536,\n",
       " 0.03795506455935538,\n",
       " 0.03863561828620732,\n",
       " 0.03953898046165705,\n",
       " 0.03885098034515977,\n",
       " 0.03850754746235907,\n",
       " 0.038211419247090816,\n",
       " 0.03922622092068195,\n",
       " 0.03816344868391752,\n",
       " 0.036956413416191936,\n",
       " 0.03814081125892699,\n",
       " 0.039508220506832004,\n",
       " 0.03838139958679676,\n",
       " 0.03748125396668911,\n",
       " 0.03789422661066055,\n",
       " 0.03731140191666782,\n",
       " 0.03837895602919161,\n",
       " 0.03735782555304468,\n",
       " 0.039012615801766515,\n",
       " 0.03876956598833203,\n",
       " 0.03969272505491972,\n",
       " 0.04772889753803611,\n",
       " 0.04338729404844344,\n",
       " 0.0388182473834604,\n",
       " 0.036344725638628006,\n",
       " 0.036777627654373646,\n",
       " 0.03773218533024192,\n",
       " 0.03868665173649788,\n",
       " 0.04002775403205305,\n",
       " 0.03992247744463384,\n",
       " 0.040931218303740025,\n",
       " 0.03989959810860455,\n",
       " 0.041621583281084895,\n",
       " 0.04088802565820515,\n",
       " 0.03957547899335623,\n",
       " 0.03862118907272816,\n",
       " 0.03826992539688945,\n",
       " 0.03737167245708406,\n",
       " 0.036874637473374605,\n",
       " 0.037167558213695884,\n",
       " 0.037486781599000096,\n",
       " 0.038916971534490585,\n",
       " 0.03829800430685282,\n",
       " 0.037492405623197556,\n",
       " 0.03715236973948777,\n",
       " 0.03637525998055935,\n",
       " 0.03684305620845407,\n",
       " 0.036443749675527215,\n",
       " 0.036615363904275,\n",
       " 0.03691439609974623,\n",
       " 0.035928969038650393,\n",
       " 0.03590708156116307,\n",
       " 0.03641181020066142,\n",
       " 0.035224614082835615,\n",
       " 0.034664319129660726,\n",
       " 0.03674440993927419,\n",
       " 0.03716549766249955,\n",
       " 0.03803204430732876,\n",
       " 0.036607492133043706,\n",
       " 0.035401983419433236,\n",
       " 0.03605462214909494,\n",
       " 0.035436066100373864,\n",
       " 0.03573336312547326,\n",
       " 0.03698005527257919,\n",
       " 0.03739235899411142,\n",
       " 0.03713380196131766,\n",
       " 0.03837871411815286,\n",
       " 0.03894118219614029,\n",
       " 0.03902587969787419,\n",
       " 0.03900731122121215,\n",
       " 0.03972249082289636,\n",
       " 0.03917161445133388,\n",
       " 0.038328260416164994,\n",
       " 0.039260791847482324,\n",
       " 0.038040566723793745,\n",
       " 0.03787311143241823,\n",
       " 0.03694633557461202,\n",
       " 0.03929375624284148,\n",
       " 0.039997560903429985,\n",
       " 0.03940776246599853,\n",
       " 0.03732362494338304,\n",
       " 0.0354011335875839,\n",
       " 0.03493229439482093,\n",
       " 0.03768649010453373,\n",
       " 0.03725137352012098,\n",
       " 0.03934450913220644,\n",
       " 0.03608023817650974,\n",
       " 0.03522503050044179,\n",
       " 0.03420426254160702,\n",
       " 0.03460715094115585,\n",
       " 0.03407649986911565,\n",
       " 0.03464735089801252,\n",
       " 0.0356973628513515,\n",
       " 0.03683919971808791,\n",
       " 0.0373038190882653,\n",
       " 0.03804104751907289,\n",
       " 0.03831783891655505,\n",
       " 0.03531309973914176,\n",
       " 0.035636895801872015,\n",
       " 0.03540002950467169,\n",
       " 0.036608278285712004,\n",
       " 0.036010521464049816,\n",
       " 0.03629637754056603,\n",
       " 0.034886832581833005,\n",
       " 0.03634353680536151,\n",
       " 0.036649830639362335,\n",
       " 0.03760043880902231,\n",
       " 0.03612119844183326,\n",
       " 0.03577938920352608,\n",
       " 0.035161984618753195,\n",
       " 0.0344221000559628,\n",
       " 0.03457514848560095,\n",
       " 0.035773013602010906,\n",
       " 0.03630436072126031,\n",
       " 0.03526866890024394,\n",
       " 0.03432519093621522,\n",
       " 0.03533172840252519,\n",
       " 0.03570689237676561,\n",
       " 0.03814846999011934,\n",
       " 0.035755789023824036,\n",
       " 0.03563061507884413,\n",
       " 0.03520907857455313,\n",
       " 0.03609785798471421,\n",
       " 0.03674687352031469,\n",
       " 0.03730132710188627,\n",
       " 0.03766120667569339,\n",
       " 0.03648717957548797]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch = {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "\n",
    "VAD_best = VAD_Beta(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device)\n",
    "trainer_best = VAD_Trainer(var_decoder=VAD_best, dataloader=train_dl, latent_dim=128, device=device, lr=0.001)\n",
    "trainer_best.train(num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d65214-623e-464a-80cb-ef3469536724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_tsne(train_ds, trainer_best.latents, f\"tsne_Beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Sample specific vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2bfd32-1c89-4f4f-9494-9d7f6e7adc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "temp_latents_best = torch.randn(10, 128).to(device)\n",
    "latents_best = torch.nn.Parameter(torch.stack([temp_latents_best[label,:] for label in test_dl.dataset.y])).to(device)\n",
    "opt = optim.Adam([latents_best], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6568406b-c6c5-40af-9339-9825107b7bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD has finished test evaluation with a test loss of 0.23139685671776533.\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(model=VAD_best, test_dl=test_dl, opt=opt, latents=latents_best, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5d3b54-cf39-4e45-af3f-8a573b9b3007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_tsne(test_ds, latents_best, f\"tsne_test_beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(latents_best)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [latents_best[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "sampled_test_images = VAD_best(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = VAD_best(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images_VAD_beta.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images_VAD_beta.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e2177-94b2-4e8d-aef6-de51ba8fef9c",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bf2b190-96a2-4808-be01-fdc3d7c7cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random.seed(56)\n",
    "sampled_indices = random.sample(range(len(latents_best)), 2)\n",
    "sampled_latents = [latents_best[i] for i in sampled_indices]\n",
    "weights = np.linspace(0, 1, 7)\n",
    "interpolated_latents = [w * sampled_latents[0] + (1 - w) * sampled_latents[1] for w in weights]\n",
    "interpolated_latents_tensor = torch.stack(interpolated_latents)\n",
    "interpolated_images = VAD_best(interpolated_latents_tensor).view(-1, 1, 28, 28)\n",
    "utils.save_images(interpolated_images, \"interpolated_images_beta.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
