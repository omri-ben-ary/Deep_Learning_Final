{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb3860-f4ed-4dc2-bb65-fd5bab45d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from VariationalAutoDecoder_Gumbel import VariationalAutoDecoder as VAD_Gumbel\n",
    "from VAD_Trainer import VAD_Trainer\n",
    "import utils\n",
    "from evaluate import evaluate_model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d227682-fe2c-4635-8f22-43695ac358ee",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb85c21c-5862-401b-88a4-e20796edf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_dl, test_ds, test_dl = utils.create_dataloaders(data_path=\"dataset\" ,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b43a-4434-4820-92c0-caf522961bb1",
   "metadata": {},
   "source": [
    "## Train Auto Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06f910-ca78-4244-9055-91cfcf343f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "architectures = [\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# latent_dims = [dim for dim in [64, 32, 128, 16, 10] for _ in range(5)]\n",
    "VADs = [VAD(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device) for arch in architectures]# for _ in range(5)]\n",
    "# learning_rates = [lr for lr in [0.001, 0.0005, 0.0001, 0.002, 0.005] for _ in range(5)]\n",
    "trainers = [VAD_Trainer(var_decoder=VADs[i], dataloader=train_dl, latent_dim=128, device=device, lr=1e-2) for i in range(len(VADs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baccde-a05f-4922-b4b9-681d7aecfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "temp_latents = torch.randn(10, 128).to(device)\n",
    "latents_list = [torch.nn.Parameter(torch.stack([temp_latents[label,:] for label in train_dl.dataset.y])).to(device) for i in range(len(VADs))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results_VAD_temp.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=500)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=VADs[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list[index], epochs=500, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27147e64-6c63-4528-bc12-99750ba8ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainers)):\n",
    "    utils.plot_tsne(train_ds, trainers[i].latents, f\"tsne_kl_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64685dc-0a81-42d3-b67b-cf78d1e34e9e",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801bf72-57eb-4d39-9a97-0330af95e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "\n",
    "learning_rates = [0.001, 0.0001, 0.005]\n",
    "VADs_ft = [VAD(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device) for _ in range(len(learning_rates))]# for _ in range(5)]\n",
    "trainers_ft = [VAD_Trainer(var_decoder=VADs_ft[i], dataloader=train_dl, latent_dim=128, device=device, lr=learning_rates[i]) for i in range(len(VADs_ft))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d52bc6-5d45-4c1b-8c4a-ec606d33ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results list to hold all the data\n",
    "num_test_samples = len(test_dl.dataset)\n",
    "\n",
    "# Create latent parameters and optimizers for each trainer\n",
    "temp_latents_ft = torch.randn(10, 128).to(device)\n",
    "latents_list_ft = [torch.nn.Parameter(torch.stack([temp_latents_ft[label,:] for label in train_dl.dataset.y])).to(device) for i in range(len(VADs_ft))]\n",
    "optimizers = [optim.Adam([latents], lr=1e-3) for latents in latents_list_ft]\n",
    "\n",
    "# Save results to a CSV file\n",
    "csv_file_path = 'results_VAD_ft.csv'\n",
    "\n",
    "# Write header to the CSV file first\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Index'] + [f'Epoch {i+1} Loss' for i in range(500)] + ['Final Test Loss']\n",
    "    writer.writerow(header)\n",
    "\n",
    "# Main training and evaluation loop\n",
    "for index, trainer in enumerate(trainers_ft):\n",
    "    start_time = time.time()  # Record the start time\n",
    "    train_loss = trainer.train(num_epochs=500)  # Train the model\n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"Trainer {index} has finished training in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "    test_loss = evaluate_model(model=VADs_ft[index], test_dl=test_dl, opt=optimizers[index], latents=latents_list_ft[index], epochs=500, device=device) \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    print(f\"AD {index} has finished test evaluation in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Prepare the row to be saved\n",
    "    row = [index] + train_loss + [test_loss]\n",
    "\n",
    "    # Append results to the CSV file after each iteration\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afa67a-6bc0-4802-8dfd-d3cf9350207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainers_ft)):\n",
    "    utils.plot_tsne(train_ds, trainers_ft[i].latents, f\"tsne_kl_ft_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033aaf-061c-4bd6-b32e-40466dd1027f",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc862a48-c47c-49ce-b233-71192301ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.4888\n",
      "Epoch [2/1000], Loss: 0.4584\n",
      "Epoch [3/1000], Loss: 0.4350\n",
      "Epoch [4/1000], Loss: 0.4123\n",
      "Epoch [5/1000], Loss: 0.3900\n",
      "Epoch [6/1000], Loss: 0.3673\n",
      "Epoch [7/1000], Loss: 0.3454\n",
      "Epoch [8/1000], Loss: 0.3254\n",
      "Epoch [9/1000], Loss: 0.3069\n",
      "Epoch [10/1000], Loss: 0.2935\n",
      "Epoch [11/1000], Loss: 0.2827\n",
      "Epoch [12/1000], Loss: 0.2735\n",
      "Epoch [13/1000], Loss: 0.2671\n",
      "Epoch [14/1000], Loss: 0.2610\n",
      "Epoch [15/1000], Loss: 0.2574\n",
      "Epoch [16/1000], Loss: 0.2544\n",
      "Epoch [17/1000], Loss: 0.2546\n",
      "Epoch [18/1000], Loss: 0.2495\n",
      "Epoch [19/1000], Loss: 0.2471\n",
      "Epoch [20/1000], Loss: 0.2441\n",
      "Epoch [21/1000], Loss: 0.2421\n",
      "Epoch [22/1000], Loss: 0.2394\n",
      "Epoch [23/1000], Loss: 0.2379\n",
      "Epoch [24/1000], Loss: 0.2366\n",
      "Epoch [25/1000], Loss: 0.2376\n",
      "Epoch [26/1000], Loss: 0.2384\n",
      "Epoch [27/1000], Loss: 0.2360\n",
      "Epoch [28/1000], Loss: 0.2325\n",
      "Epoch [29/1000], Loss: 0.2272\n",
      "Epoch [30/1000], Loss: 0.2242\n",
      "Epoch [31/1000], Loss: 0.2221\n",
      "Epoch [32/1000], Loss: 0.2206\n",
      "Epoch [33/1000], Loss: 0.2191\n",
      "Epoch [34/1000], Loss: 0.2176\n",
      "Epoch [35/1000], Loss: 0.2168\n",
      "Epoch [36/1000], Loss: 0.2173\n",
      "Epoch [37/1000], Loss: 0.2165\n",
      "Epoch [38/1000], Loss: 0.2170\n",
      "Epoch [39/1000], Loss: 0.2174\n",
      "Epoch [40/1000], Loss: 0.2170\n",
      "Epoch [41/1000], Loss: 0.2153\n",
      "Epoch [42/1000], Loss: 0.2143\n",
      "Epoch [43/1000], Loss: 0.2135\n",
      "Epoch [44/1000], Loss: 0.2139\n",
      "Epoch [45/1000], Loss: 0.2133\n",
      "Epoch [46/1000], Loss: 0.2116\n",
      "Epoch [47/1000], Loss: 0.2089\n",
      "Epoch [48/1000], Loss: 0.2068\n",
      "Epoch [49/1000], Loss: 0.2059\n",
      "Epoch [50/1000], Loss: 0.2053\n",
      "Epoch [51/1000], Loss: 0.2056\n",
      "Epoch [52/1000], Loss: 0.2065\n",
      "Epoch [53/1000], Loss: 0.2060\n",
      "Epoch [54/1000], Loss: 0.2055\n",
      "Epoch [55/1000], Loss: 0.2028\n",
      "Epoch [56/1000], Loss: 0.2004\n",
      "Epoch [57/1000], Loss: 0.1994\n",
      "Epoch [58/1000], Loss: 0.1981\n",
      "Epoch [59/1000], Loss: 0.1975\n",
      "Epoch [60/1000], Loss: 0.1965\n",
      "Epoch [61/1000], Loss: 0.1965\n",
      "Epoch [62/1000], Loss: 0.1974\n",
      "Epoch [63/1000], Loss: 0.1968\n",
      "Epoch [64/1000], Loss: 0.1965\n",
      "Epoch [65/1000], Loss: 0.1971\n",
      "Epoch [66/1000], Loss: 0.1971\n",
      "Epoch [67/1000], Loss: 0.1957\n",
      "Epoch [68/1000], Loss: 0.1949\n",
      "Epoch [69/1000], Loss: 0.1919\n",
      "Epoch [70/1000], Loss: 0.1911\n",
      "Epoch [71/1000], Loss: 0.1902\n",
      "Epoch [72/1000], Loss: 0.1891\n",
      "Epoch [73/1000], Loss: 0.1895\n",
      "Epoch [74/1000], Loss: 0.1899\n",
      "Epoch [75/1000], Loss: 0.1919\n",
      "Epoch [76/1000], Loss: 0.1908\n",
      "Epoch [77/1000], Loss: 0.1900\n",
      "Epoch [78/1000], Loss: 0.1895\n",
      "Epoch [79/1000], Loss: 0.1901\n",
      "Epoch [80/1000], Loss: 0.1899\n",
      "Epoch [81/1000], Loss: 0.1897\n",
      "Epoch [82/1000], Loss: 0.1889\n",
      "Epoch [83/1000], Loss: 0.1887\n",
      "Epoch [84/1000], Loss: 0.1881\n",
      "Epoch [85/1000], Loss: 0.1857\n",
      "Epoch [86/1000], Loss: 0.1824\n",
      "Epoch [87/1000], Loss: 0.1814\n",
      "Epoch [88/1000], Loss: 0.1794\n",
      "Epoch [89/1000], Loss: 0.1794\n",
      "Epoch [90/1000], Loss: 0.1789\n",
      "Epoch [91/1000], Loss: 0.1775\n",
      "Epoch [92/1000], Loss: 0.1778\n",
      "Epoch [93/1000], Loss: 0.1789\n",
      "Epoch [94/1000], Loss: 0.1788\n",
      "Epoch [95/1000], Loss: 0.1792\n",
      "Epoch [96/1000], Loss: 0.1798\n",
      "Epoch [97/1000], Loss: 0.1777\n",
      "Epoch [98/1000], Loss: 0.1777\n",
      "Epoch [99/1000], Loss: 0.1772\n",
      "Epoch [100/1000], Loss: 0.1774\n",
      "Epoch [101/1000], Loss: 0.1789\n",
      "Epoch [102/1000], Loss: 0.1776\n",
      "Epoch [103/1000], Loss: 0.1760\n",
      "Epoch [104/1000], Loss: 0.1749\n",
      "Epoch [105/1000], Loss: 0.1740\n",
      "Epoch [106/1000], Loss: 0.1732\n",
      "Epoch [107/1000], Loss: 0.1725\n",
      "Epoch [108/1000], Loss: 0.1724\n",
      "Epoch [109/1000], Loss: 0.1722\n",
      "Epoch [110/1000], Loss: 0.1716\n",
      "Epoch [111/1000], Loss: 0.1713\n",
      "Epoch [112/1000], Loss: 0.1708\n",
      "Epoch [113/1000], Loss: 0.1691\n",
      "Epoch [114/1000], Loss: 0.1691\n",
      "Epoch [115/1000], Loss: 0.1680\n",
      "Epoch [116/1000], Loss: 0.1680\n",
      "Epoch [117/1000], Loss: 0.1676\n",
      "Epoch [118/1000], Loss: 0.1673\n",
      "Epoch [119/1000], Loss: 0.1669\n",
      "Epoch [120/1000], Loss: 0.1665\n",
      "Epoch [121/1000], Loss: 0.1649\n",
      "Epoch [122/1000], Loss: 0.1667\n",
      "Epoch [123/1000], Loss: 0.1670\n",
      "Epoch [124/1000], Loss: 0.1674\n",
      "Epoch [125/1000], Loss: 0.1662\n",
      "Epoch [126/1000], Loss: 0.1644\n",
      "Epoch [127/1000], Loss: 0.1644\n",
      "Epoch [128/1000], Loss: 0.1626\n",
      "Epoch [129/1000], Loss: 0.1609\n",
      "Epoch [130/1000], Loss: 0.1611\n",
      "Epoch [131/1000], Loss: 0.1607\n",
      "Epoch [132/1000], Loss: 0.1602\n",
      "Epoch [133/1000], Loss: 0.1599\n",
      "Epoch [134/1000], Loss: 0.1589\n",
      "Epoch [135/1000], Loss: 0.1586\n",
      "Epoch [136/1000], Loss: 0.1580\n",
      "Epoch [137/1000], Loss: 0.1587\n",
      "Epoch [138/1000], Loss: 0.1592\n",
      "Epoch [139/1000], Loss: 0.1590\n",
      "Epoch [140/1000], Loss: 0.1570\n",
      "Epoch [141/1000], Loss: 0.1558\n",
      "Epoch [142/1000], Loss: 0.1550\n",
      "Epoch [143/1000], Loss: 0.1548\n",
      "Epoch [144/1000], Loss: 0.1548\n",
      "Epoch [145/1000], Loss: 0.1537\n",
      "Epoch [146/1000], Loss: 0.1529\n",
      "Epoch [147/1000], Loss: 0.1535\n",
      "Epoch [148/1000], Loss: 0.1526\n",
      "Epoch [149/1000], Loss: 0.1509\n",
      "Epoch [150/1000], Loss: 0.1500\n",
      "Epoch [151/1000], Loss: 0.1502\n",
      "Epoch [152/1000], Loss: 0.1499\n",
      "Epoch [153/1000], Loss: 0.1506\n",
      "Epoch [154/1000], Loss: 0.1500\n",
      "Epoch [155/1000], Loss: 0.1498\n",
      "Epoch [156/1000], Loss: 0.1500\n",
      "Epoch [157/1000], Loss: 0.1515\n",
      "Epoch [158/1000], Loss: 0.1496\n",
      "Epoch [159/1000], Loss: 0.1497\n",
      "Epoch [160/1000], Loss: 0.1493\n",
      "Epoch [161/1000], Loss: 0.1488\n",
      "Epoch [162/1000], Loss: 0.1505\n",
      "Epoch [163/1000], Loss: 0.1505\n",
      "Epoch [164/1000], Loss: 0.1534\n",
      "Epoch [165/1000], Loss: 0.1509\n",
      "Epoch [166/1000], Loss: 0.1475\n",
      "Epoch [167/1000], Loss: 0.1433\n",
      "Epoch [168/1000], Loss: 0.1423\n",
      "Epoch [169/1000], Loss: 0.1440\n",
      "Epoch [170/1000], Loss: 0.1447\n",
      "Epoch [171/1000], Loss: 0.1453\n",
      "Epoch [172/1000], Loss: 0.1482\n",
      "Epoch [173/1000], Loss: 0.1476\n",
      "Epoch [174/1000], Loss: 0.1441\n",
      "Epoch [175/1000], Loss: 0.1437\n",
      "Epoch [176/1000], Loss: 0.1436\n",
      "Epoch [177/1000], Loss: 0.1444\n",
      "Epoch [178/1000], Loss: 0.1429\n",
      "Epoch [179/1000], Loss: 0.1441\n",
      "Epoch [180/1000], Loss: 0.1421\n",
      "Epoch [181/1000], Loss: 0.1410\n",
      "Epoch [182/1000], Loss: 0.1385\n",
      "Epoch [183/1000], Loss: 0.1375\n",
      "Epoch [184/1000], Loss: 0.1358\n",
      "Epoch [185/1000], Loss: 0.1375\n",
      "Epoch [186/1000], Loss: 0.1373\n",
      "Epoch [187/1000], Loss: 0.1348\n",
      "Epoch [188/1000], Loss: 0.1350\n",
      "Epoch [189/1000], Loss: 0.1352\n",
      "Epoch [190/1000], Loss: 0.1378\n",
      "Epoch [191/1000], Loss: 0.1356\n",
      "Epoch [192/1000], Loss: 0.1359\n",
      "Epoch [193/1000], Loss: 0.1363\n",
      "Epoch [194/1000], Loss: 0.1349\n",
      "Epoch [195/1000], Loss: 0.1347\n",
      "Epoch [196/1000], Loss: 0.1343\n",
      "Epoch [197/1000], Loss: 0.1341\n",
      "Epoch [198/1000], Loss: 0.1351\n",
      "Epoch [199/1000], Loss: 0.1355\n",
      "Epoch [200/1000], Loss: 0.1356\n",
      "Epoch [201/1000], Loss: 0.1341\n",
      "Epoch [202/1000], Loss: 0.1347\n",
      "Epoch [203/1000], Loss: 0.1316\n",
      "Epoch [204/1000], Loss: 0.1300\n",
      "Epoch [205/1000], Loss: 0.1306\n",
      "Epoch [206/1000], Loss: 0.1305\n",
      "Epoch [207/1000], Loss: 0.1296\n",
      "Epoch [208/1000], Loss: 0.1298\n",
      "Epoch [209/1000], Loss: 0.1304\n",
      "Epoch [210/1000], Loss: 0.1325\n",
      "Epoch [211/1000], Loss: 0.1303\n",
      "Epoch [212/1000], Loss: 0.1288\n",
      "Epoch [213/1000], Loss: 0.1271\n",
      "Epoch [214/1000], Loss: 0.1271\n",
      "Epoch [215/1000], Loss: 0.1271\n",
      "Epoch [216/1000], Loss: 0.1256\n",
      "Epoch [217/1000], Loss: 0.1254\n",
      "Epoch [218/1000], Loss: 0.1250\n",
      "Epoch [219/1000], Loss: 0.1257\n",
      "Epoch [220/1000], Loss: 0.1246\n",
      "Epoch [221/1000], Loss: 0.1252\n",
      "Epoch [222/1000], Loss: 0.1251\n",
      "Epoch [223/1000], Loss: 0.1269\n",
      "Epoch [224/1000], Loss: 0.1288\n",
      "Epoch [225/1000], Loss: 0.1260\n",
      "Epoch [226/1000], Loss: 0.1238\n",
      "Epoch [227/1000], Loss: 0.1242\n",
      "Epoch [228/1000], Loss: 0.1213\n",
      "Epoch [229/1000], Loss: 0.1211\n",
      "Epoch [230/1000], Loss: 0.1215\n",
      "Epoch [231/1000], Loss: 0.1214\n",
      "Epoch [232/1000], Loss: 0.1215\n",
      "Epoch [233/1000], Loss: 0.1212\n",
      "Epoch [234/1000], Loss: 0.1199\n",
      "Epoch [235/1000], Loss: 0.1205\n",
      "Epoch [236/1000], Loss: 0.1220\n",
      "Epoch [237/1000], Loss: 0.1196\n",
      "Epoch [238/1000], Loss: 0.1196\n",
      "Epoch [239/1000], Loss: 0.1198\n",
      "Epoch [240/1000], Loss: 0.1195\n",
      "Epoch [241/1000], Loss: 0.1206\n",
      "Epoch [242/1000], Loss: 0.1180\n",
      "Epoch [243/1000], Loss: 0.1158\n",
      "Epoch [244/1000], Loss: 0.1165\n",
      "Epoch [245/1000], Loss: 0.1158\n",
      "Epoch [246/1000], Loss: 0.1152\n",
      "Epoch [247/1000], Loss: 0.1168\n",
      "Epoch [248/1000], Loss: 0.1168\n",
      "Epoch [249/1000], Loss: 0.1161\n",
      "Epoch [250/1000], Loss: 0.1151\n",
      "Epoch [251/1000], Loss: 0.1155\n",
      "Epoch [252/1000], Loss: 0.1144\n",
      "Epoch [253/1000], Loss: 0.1144\n",
      "Epoch [254/1000], Loss: 0.1136\n",
      "Epoch [255/1000], Loss: 0.1126\n",
      "Epoch [256/1000], Loss: 0.1123\n",
      "Epoch [257/1000], Loss: 0.1146\n",
      "Epoch [258/1000], Loss: 0.1188\n",
      "Epoch [259/1000], Loss: 0.1194\n",
      "Epoch [260/1000], Loss: 0.1192\n",
      "Epoch [261/1000], Loss: 0.1170\n",
      "Epoch [262/1000], Loss: 0.1107\n",
      "Epoch [263/1000], Loss: 0.1117\n",
      "Epoch [264/1000], Loss: 0.1099\n",
      "Epoch [265/1000], Loss: 0.1114\n",
      "Epoch [266/1000], Loss: 0.1120\n",
      "Epoch [267/1000], Loss: 0.1113\n",
      "Epoch [268/1000], Loss: 0.1101\n",
      "Epoch [269/1000], Loss: 0.1102\n",
      "Epoch [270/1000], Loss: 0.1094\n",
      "Epoch [271/1000], Loss: 0.1101\n",
      "Epoch [272/1000], Loss: 0.1116\n",
      "Epoch [273/1000], Loss: 0.1099\n",
      "Epoch [274/1000], Loss: 0.1093\n",
      "Epoch [275/1000], Loss: 0.1080\n",
      "Epoch [276/1000], Loss: 0.1077\n",
      "Epoch [277/1000], Loss: 0.1084\n",
      "Epoch [278/1000], Loss: 0.1100\n",
      "Epoch [279/1000], Loss: 0.1090\n",
      "Epoch [280/1000], Loss: 0.1071\n",
      "Epoch [281/1000], Loss: 0.1065\n",
      "Epoch [282/1000], Loss: 0.1068\n",
      "Epoch [283/1000], Loss: 0.1051\n",
      "Epoch [284/1000], Loss: 0.1043\n",
      "Epoch [285/1000], Loss: 0.1038\n",
      "Epoch [286/1000], Loss: 0.1034\n",
      "Epoch [287/1000], Loss: 0.1050\n",
      "Epoch [288/1000], Loss: 0.1046\n",
      "Epoch [289/1000], Loss: 0.1045\n",
      "Epoch [290/1000], Loss: 0.1038\n",
      "Epoch [291/1000], Loss: 0.1058\n",
      "Epoch [292/1000], Loss: 0.1064\n",
      "Epoch [293/1000], Loss: 0.1049\n",
      "Epoch [294/1000], Loss: 0.1053\n",
      "Epoch [295/1000], Loss: 0.1053\n",
      "Epoch [296/1000], Loss: 0.1065\n",
      "Epoch [297/1000], Loss: 0.1060\n",
      "Epoch [298/1000], Loss: 0.1054\n",
      "Epoch [299/1000], Loss: 0.1031\n",
      "Epoch [300/1000], Loss: 0.1015\n",
      "Epoch [301/1000], Loss: 0.1003\n",
      "Epoch [302/1000], Loss: 0.1018\n",
      "Epoch [303/1000], Loss: 0.1000\n",
      "Epoch [304/1000], Loss: 0.0995\n",
      "Epoch [305/1000], Loss: 0.1013\n",
      "Epoch [306/1000], Loss: 0.1019\n",
      "Epoch [307/1000], Loss: 0.1015\n",
      "Epoch [308/1000], Loss: 0.1030\n",
      "Epoch [309/1000], Loss: 0.1031\n",
      "Epoch [310/1000], Loss: 0.1013\n",
      "Epoch [311/1000], Loss: 0.0998\n",
      "Epoch [312/1000], Loss: 0.0999\n",
      "Epoch [313/1000], Loss: 0.1019\n",
      "Epoch [314/1000], Loss: 0.1023\n",
      "Epoch [315/1000], Loss: 0.1011\n",
      "Epoch [316/1000], Loss: 0.0985\n",
      "Epoch [317/1000], Loss: 0.0977\n",
      "Epoch [318/1000], Loss: 0.0988\n",
      "Epoch [319/1000], Loss: 0.0989\n",
      "Epoch [320/1000], Loss: 0.0996\n",
      "Epoch [321/1000], Loss: 0.0996\n",
      "Epoch [322/1000], Loss: 0.0991\n",
      "Epoch [323/1000], Loss: 0.0981\n",
      "Epoch [324/1000], Loss: 0.0975\n",
      "Epoch [325/1000], Loss: 0.0961\n",
      "Epoch [326/1000], Loss: 0.0972\n",
      "Epoch [327/1000], Loss: 0.0976\n",
      "Epoch [328/1000], Loss: 0.0972\n",
      "Epoch [329/1000], Loss: 0.0990\n",
      "Epoch [330/1000], Loss: 0.0984\n",
      "Epoch [331/1000], Loss: 0.0972\n",
      "Epoch [332/1000], Loss: 0.0986\n",
      "Epoch [333/1000], Loss: 0.0988\n",
      "Epoch [334/1000], Loss: 0.0988\n",
      "Epoch [335/1000], Loss: 0.0978\n",
      "Epoch [336/1000], Loss: 0.0965\n",
      "Epoch [337/1000], Loss: 0.0949\n",
      "Epoch [338/1000], Loss: 0.0939\n",
      "Epoch [339/1000], Loss: 0.0936\n",
      "Epoch [340/1000], Loss: 0.0943\n",
      "Epoch [341/1000], Loss: 0.0931\n",
      "Epoch [342/1000], Loss: 0.0929\n",
      "Epoch [343/1000], Loss: 0.0941\n",
      "Epoch [344/1000], Loss: 0.0955\n",
      "Epoch [345/1000], Loss: 0.0966\n",
      "Epoch [346/1000], Loss: 0.0934\n",
      "Epoch [347/1000], Loss: 0.0911\n",
      "Epoch [348/1000], Loss: 0.0903\n",
      "Epoch [349/1000], Loss: 0.0909\n",
      "Epoch [350/1000], Loss: 0.0932\n",
      "Epoch [351/1000], Loss: 0.0919\n",
      "Epoch [352/1000], Loss: 0.0905\n",
      "Epoch [353/1000], Loss: 0.0908\n",
      "Epoch [354/1000], Loss: 0.0923\n",
      "Epoch [355/1000], Loss: 0.0928\n",
      "Epoch [356/1000], Loss: 0.0933\n",
      "Epoch [357/1000], Loss: 0.0938\n",
      "Epoch [358/1000], Loss: 0.0916\n",
      "Epoch [359/1000], Loss: 0.0922\n",
      "Epoch [360/1000], Loss: 0.0916\n",
      "Epoch [361/1000], Loss: 0.0918\n",
      "Epoch [362/1000], Loss: 0.0933\n",
      "Epoch [363/1000], Loss: 0.0937\n",
      "Epoch [364/1000], Loss: 0.0944\n",
      "Epoch [365/1000], Loss: 0.0971\n",
      "Epoch [366/1000], Loss: 0.0987\n",
      "Epoch [367/1000], Loss: 0.0999\n",
      "Epoch [368/1000], Loss: 0.1014\n",
      "Epoch [369/1000], Loss: 0.1017\n",
      "Epoch [370/1000], Loss: 0.1008\n",
      "Epoch [371/1000], Loss: 0.1014\n",
      "Epoch [372/1000], Loss: 0.1000\n",
      "Epoch [373/1000], Loss: 0.0992\n",
      "Epoch [374/1000], Loss: 0.0983\n",
      "Epoch [375/1000], Loss: 0.0980\n",
      "Epoch [376/1000], Loss: 0.0945\n",
      "Epoch [377/1000], Loss: 0.0936\n",
      "Epoch [378/1000], Loss: 0.0909\n",
      "Epoch [379/1000], Loss: 0.0894\n",
      "Epoch [380/1000], Loss: 0.0891\n",
      "Epoch [381/1000], Loss: 0.0899\n",
      "Epoch [382/1000], Loss: 0.0905\n",
      "Epoch [383/1000], Loss: 0.0891\n",
      "Epoch [384/1000], Loss: 0.0897\n",
      "Epoch [385/1000], Loss: 0.0885\n",
      "Epoch [386/1000], Loss: 0.0882\n",
      "Epoch [387/1000], Loss: 0.0897\n",
      "Epoch [388/1000], Loss: 0.0898\n",
      "Epoch [389/1000], Loss: 0.0883\n",
      "Epoch [390/1000], Loss: 0.0884\n",
      "Epoch [391/1000], Loss: 0.0901\n",
      "Epoch [392/1000], Loss: 0.0899\n",
      "Epoch [393/1000], Loss: 0.0910\n",
      "Epoch [394/1000], Loss: 0.0905\n",
      "Epoch [395/1000], Loss: 0.0898\n",
      "Epoch [396/1000], Loss: 0.0878\n",
      "Epoch [397/1000], Loss: 0.0866\n",
      "Epoch [398/1000], Loss: 0.0863\n",
      "Epoch [399/1000], Loss: 0.0892\n",
      "Epoch [400/1000], Loss: 0.0894\n",
      "Epoch [401/1000], Loss: 0.0886\n",
      "Epoch [402/1000], Loss: 0.0879\n",
      "Epoch [403/1000], Loss: 0.0872\n",
      "Epoch [404/1000], Loss: 0.0877\n",
      "Epoch [405/1000], Loss: 0.0899\n",
      "Epoch [406/1000], Loss: 0.0903\n",
      "Epoch [407/1000], Loss: 0.0919\n",
      "Epoch [408/1000], Loss: 0.0896\n",
      "Epoch [409/1000], Loss: 0.0908\n",
      "Epoch [410/1000], Loss: 0.0947\n",
      "Epoch [411/1000], Loss: 0.1000\n",
      "Epoch [412/1000], Loss: 0.0991\n",
      "Epoch [413/1000], Loss: 0.0990\n",
      "Epoch [414/1000], Loss: 0.0960\n",
      "Epoch [415/1000], Loss: 0.0931\n",
      "Epoch [416/1000], Loss: 0.0925\n",
      "Epoch [417/1000], Loss: 0.0911\n",
      "Epoch [418/1000], Loss: 0.0909\n",
      "Epoch [419/1000], Loss: 0.0896\n",
      "Epoch [420/1000], Loss: 0.0897\n",
      "Epoch [421/1000], Loss: 0.0910\n",
      "Epoch [422/1000], Loss: 0.0920\n",
      "Epoch [423/1000], Loss: 0.0932\n",
      "Epoch [424/1000], Loss: 0.0923\n",
      "Epoch [425/1000], Loss: 0.0895\n",
      "Epoch [426/1000], Loss: 0.0879\n",
      "Epoch [427/1000], Loss: 0.0866\n",
      "Epoch [428/1000], Loss: 0.0865\n",
      "Epoch [429/1000], Loss: 0.0874\n",
      "Epoch [430/1000], Loss: 0.0855\n",
      "Epoch [431/1000], Loss: 0.0849\n",
      "Epoch [432/1000], Loss: 0.0847\n",
      "Epoch [433/1000], Loss: 0.0848\n",
      "Epoch [434/1000], Loss: 0.0854\n",
      "Epoch [435/1000], Loss: 0.0863\n",
      "Epoch [436/1000], Loss: 0.0857\n",
      "Epoch [437/1000], Loss: 0.0859\n",
      "Epoch [438/1000], Loss: 0.0843\n",
      "Epoch [439/1000], Loss: 0.0848\n",
      "Epoch [440/1000], Loss: 0.0851\n",
      "Epoch [441/1000], Loss: 0.0870\n",
      "Epoch [442/1000], Loss: 0.0863\n",
      "Epoch [443/1000], Loss: 0.0855\n",
      "Epoch [444/1000], Loss: 0.0838\n",
      "Epoch [445/1000], Loss: 0.0838\n",
      "Epoch [446/1000], Loss: 0.0835\n",
      "Epoch [447/1000], Loss: 0.0849\n",
      "Epoch [448/1000], Loss: 0.0843\n",
      "Epoch [449/1000], Loss: 0.0841\n",
      "Epoch [450/1000], Loss: 0.0841\n",
      "Epoch [451/1000], Loss: 0.0841\n",
      "Epoch [452/1000], Loss: 0.0839\n",
      "Epoch [453/1000], Loss: 0.0840\n",
      "Epoch [454/1000], Loss: 0.0835\n",
      "Epoch [455/1000], Loss: 0.0827\n",
      "Epoch [456/1000], Loss: 0.0824\n",
      "Epoch [457/1000], Loss: 0.0822\n",
      "Epoch [458/1000], Loss: 0.0826\n",
      "Epoch [459/1000], Loss: 0.0830\n",
      "Epoch [460/1000], Loss: 0.0849\n",
      "Epoch [461/1000], Loss: 0.0840\n",
      "Epoch [462/1000], Loss: 0.0851\n",
      "Epoch [463/1000], Loss: 0.0860\n",
      "Epoch [464/1000], Loss: 0.0877\n",
      "Epoch [465/1000], Loss: 0.0854\n",
      "Epoch [466/1000], Loss: 0.0846\n",
      "Epoch [467/1000], Loss: 0.0861\n",
      "Epoch [468/1000], Loss: 0.0847\n",
      "Epoch [469/1000], Loss: 0.0870\n",
      "Epoch [470/1000], Loss: 0.0856\n",
      "Epoch [471/1000], Loss: 0.0841\n",
      "Epoch [472/1000], Loss: 0.0830\n",
      "Epoch [473/1000], Loss: 0.0809\n",
      "Epoch [474/1000], Loss: 0.0797\n",
      "Epoch [475/1000], Loss: 0.0789\n",
      "Epoch [476/1000], Loss: 0.0783\n",
      "Epoch [477/1000], Loss: 0.0789\n",
      "Epoch [478/1000], Loss: 0.0808\n",
      "Epoch [479/1000], Loss: 0.0805\n",
      "Epoch [480/1000], Loss: 0.0798\n",
      "Epoch [481/1000], Loss: 0.0799\n",
      "Epoch [482/1000], Loss: 0.0812\n",
      "Epoch [483/1000], Loss: 0.0801\n",
      "Epoch [484/1000], Loss: 0.0796\n",
      "Epoch [485/1000], Loss: 0.0805\n",
      "Epoch [486/1000], Loss: 0.0812\n",
      "Epoch [487/1000], Loss: 0.0815\n",
      "Epoch [488/1000], Loss: 0.0822\n",
      "Epoch [489/1000], Loss: 0.0807\n",
      "Epoch [490/1000], Loss: 0.0807\n",
      "Epoch [491/1000], Loss: 0.0799\n",
      "Epoch [492/1000], Loss: 0.0794\n",
      "Epoch [493/1000], Loss: 0.0814\n",
      "Epoch [494/1000], Loss: 0.0820\n",
      "Epoch [495/1000], Loss: 0.0826\n",
      "Epoch [496/1000], Loss: 0.0802\n",
      "Epoch [497/1000], Loss: 0.0794\n",
      "Epoch [498/1000], Loss: 0.0804\n",
      "Epoch [499/1000], Loss: 0.0804\n",
      "Epoch [500/1000], Loss: 0.0798\n",
      "Epoch [501/1000], Loss: 0.0788\n",
      "Epoch [502/1000], Loss: 0.0797\n",
      "Epoch [503/1000], Loss: 0.0809\n",
      "Epoch [504/1000], Loss: 0.0822\n",
      "Epoch [505/1000], Loss: 0.0824\n",
      "Epoch [506/1000], Loss: 0.0840\n",
      "Epoch [507/1000], Loss: 0.0840\n",
      "Epoch [508/1000], Loss: 0.0836\n",
      "Epoch [509/1000], Loss: 0.0858\n",
      "Epoch [510/1000], Loss: 0.0868\n",
      "Epoch [511/1000], Loss: 0.0876\n",
      "Epoch [512/1000], Loss: 0.0896\n",
      "Epoch [513/1000], Loss: 0.0921\n",
      "Epoch [514/1000], Loss: 0.0875\n",
      "Epoch [515/1000], Loss: 0.0849\n",
      "Epoch [516/1000], Loss: 0.0819\n",
      "Epoch [517/1000], Loss: 0.0796\n",
      "Epoch [518/1000], Loss: 0.0784\n",
      "Epoch [519/1000], Loss: 0.0781\n",
      "Epoch [520/1000], Loss: 0.0775\n",
      "Epoch [521/1000], Loss: 0.0769\n",
      "Epoch [522/1000], Loss: 0.0771\n",
      "Epoch [523/1000], Loss: 0.0764\n",
      "Epoch [524/1000], Loss: 0.0758\n",
      "Epoch [525/1000], Loss: 0.0762\n",
      "Epoch [526/1000], Loss: 0.0768\n",
      "Epoch [527/1000], Loss: 0.0770\n",
      "Epoch [528/1000], Loss: 0.0774\n",
      "Epoch [529/1000], Loss: 0.0768\n",
      "Epoch [530/1000], Loss: 0.0773\n",
      "Epoch [531/1000], Loss: 0.0776\n",
      "Epoch [532/1000], Loss: 0.0782\n",
      "Epoch [533/1000], Loss: 0.0790\n",
      "Epoch [534/1000], Loss: 0.0803\n",
      "Epoch [535/1000], Loss: 0.0815\n",
      "Epoch [536/1000], Loss: 0.0847\n",
      "Epoch [537/1000], Loss: 0.0859\n",
      "Epoch [538/1000], Loss: 0.0860\n",
      "Epoch [539/1000], Loss: 0.0830\n",
      "Epoch [540/1000], Loss: 0.0804\n",
      "Epoch [541/1000], Loss: 0.0782\n",
      "Epoch [542/1000], Loss: 0.0762\n",
      "Epoch [543/1000], Loss: 0.0762\n",
      "Epoch [544/1000], Loss: 0.0775\n",
      "Epoch [545/1000], Loss: 0.0810\n",
      "Epoch [546/1000], Loss: 0.0818\n",
      "Epoch [547/1000], Loss: 0.0816\n",
      "Epoch [548/1000], Loss: 0.0820\n",
      "Epoch [549/1000], Loss: 0.0806\n",
      "Epoch [550/1000], Loss: 0.0801\n",
      "Epoch [551/1000], Loss: 0.0802\n",
      "Epoch [552/1000], Loss: 0.0795\n",
      "Epoch [553/1000], Loss: 0.0794\n",
      "Epoch [554/1000], Loss: 0.0789\n",
      "Epoch [555/1000], Loss: 0.0804\n",
      "Epoch [556/1000], Loss: 0.0798\n",
      "Epoch [557/1000], Loss: 0.0810\n",
      "Epoch [558/1000], Loss: 0.0803\n",
      "Epoch [559/1000], Loss: 0.0812\n",
      "Epoch [560/1000], Loss: 0.0805\n",
      "Epoch [561/1000], Loss: 0.0761\n",
      "Epoch [562/1000], Loss: 0.0736\n",
      "Epoch [563/1000], Loss: 0.0719\n",
      "Epoch [564/1000], Loss: 0.0709\n",
      "Epoch [565/1000], Loss: 0.0716\n",
      "Epoch [566/1000], Loss: 0.0748\n",
      "Epoch [567/1000], Loss: 0.0762\n",
      "Epoch [568/1000], Loss: 0.0751\n",
      "Epoch [569/1000], Loss: 0.0734\n",
      "Epoch [570/1000], Loss: 0.0739\n",
      "Epoch [571/1000], Loss: 0.0733\n",
      "Epoch [572/1000], Loss: 0.0731\n",
      "Epoch [573/1000], Loss: 0.0747\n",
      "Epoch [574/1000], Loss: 0.0738\n",
      "Epoch [575/1000], Loss: 0.0749\n",
      "Epoch [576/1000], Loss: 0.0752\n",
      "Epoch [577/1000], Loss: 0.0751\n",
      "Epoch [578/1000], Loss: 0.0742\n",
      "Epoch [579/1000], Loss: 0.0741\n",
      "Epoch [580/1000], Loss: 0.0736\n",
      "Epoch [581/1000], Loss: 0.0737\n",
      "Epoch [582/1000], Loss: 0.0754\n",
      "Epoch [583/1000], Loss: 0.0758\n",
      "Epoch [584/1000], Loss: 0.0757\n",
      "Epoch [585/1000], Loss: 0.0758\n",
      "Epoch [586/1000], Loss: 0.0751\n",
      "Epoch [587/1000], Loss: 0.0745\n",
      "Epoch [588/1000], Loss: 0.0720\n",
      "Epoch [589/1000], Loss: 0.0711\n",
      "Epoch [590/1000], Loss: 0.0722\n",
      "Epoch [591/1000], Loss: 0.0720\n",
      "Epoch [592/1000], Loss: 0.0723\n",
      "Epoch [593/1000], Loss: 0.0731\n",
      "Epoch [594/1000], Loss: 0.0715\n",
      "Epoch [595/1000], Loss: 0.0731\n",
      "Epoch [596/1000], Loss: 0.0738\n",
      "Epoch [597/1000], Loss: 0.0731\n",
      "Epoch [598/1000], Loss: 0.0735\n",
      "Epoch [599/1000], Loss: 0.0712\n",
      "Epoch [600/1000], Loss: 0.0713\n",
      "Epoch [601/1000], Loss: 0.0707\n",
      "Epoch [602/1000], Loss: 0.0708\n",
      "Epoch [603/1000], Loss: 0.0728\n",
      "Epoch [604/1000], Loss: 0.0706\n",
      "Epoch [605/1000], Loss: 0.0697\n",
      "Epoch [606/1000], Loss: 0.0701\n",
      "Epoch [607/1000], Loss: 0.0702\n",
      "Epoch [608/1000], Loss: 0.0696\n",
      "Epoch [609/1000], Loss: 0.0686\n",
      "Epoch [610/1000], Loss: 0.0687\n",
      "Epoch [611/1000], Loss: 0.0691\n",
      "Epoch [612/1000], Loss: 0.0687\n",
      "Epoch [613/1000], Loss: 0.0686\n",
      "Epoch [614/1000], Loss: 0.0686\n",
      "Epoch [615/1000], Loss: 0.0699\n",
      "Epoch [616/1000], Loss: 0.0696\n",
      "Epoch [617/1000], Loss: 0.0694\n",
      "Epoch [618/1000], Loss: 0.0703\n",
      "Epoch [619/1000], Loss: 0.0708\n",
      "Epoch [620/1000], Loss: 0.0698\n",
      "Epoch [621/1000], Loss: 0.0715\n",
      "Epoch [622/1000], Loss: 0.0713\n",
      "Epoch [623/1000], Loss: 0.0721\n",
      "Epoch [624/1000], Loss: 0.0736\n",
      "Epoch [625/1000], Loss: 0.0746\n",
      "Epoch [626/1000], Loss: 0.0741\n",
      "Epoch [627/1000], Loss: 0.0739\n",
      "Epoch [628/1000], Loss: 0.0747\n",
      "Epoch [629/1000], Loss: 0.0753\n",
      "Epoch [630/1000], Loss: 0.0756\n",
      "Epoch [631/1000], Loss: 0.0751\n",
      "Epoch [632/1000], Loss: 0.0725\n",
      "Epoch [633/1000], Loss: 0.0711\n",
      "Epoch [634/1000], Loss: 0.0695\n",
      "Epoch [635/1000], Loss: 0.0683\n",
      "Epoch [636/1000], Loss: 0.0668\n",
      "Epoch [637/1000], Loss: 0.0668\n",
      "Epoch [638/1000], Loss: 0.0659\n",
      "Epoch [639/1000], Loss: 0.0671\n",
      "Epoch [640/1000], Loss: 0.0664\n",
      "Epoch [641/1000], Loss: 0.0678\n",
      "Epoch [642/1000], Loss: 0.0696\n",
      "Epoch [643/1000], Loss: 0.0697\n",
      "Epoch [644/1000], Loss: 0.0713\n",
      "Epoch [645/1000], Loss: 0.0694\n",
      "Epoch [646/1000], Loss: 0.0692\n",
      "Epoch [647/1000], Loss: 0.0693\n",
      "Epoch [648/1000], Loss: 0.0684\n",
      "Epoch [649/1000], Loss: 0.0685\n",
      "Epoch [650/1000], Loss: 0.0680\n",
      "Epoch [651/1000], Loss: 0.0680\n",
      "Epoch [652/1000], Loss: 0.0694\n",
      "Epoch [653/1000], Loss: 0.0693\n",
      "Epoch [654/1000], Loss: 0.0692\n",
      "Epoch [655/1000], Loss: 0.0691\n",
      "Epoch [656/1000], Loss: 0.0690\n",
      "Epoch [657/1000], Loss: 0.0705\n",
      "Epoch [658/1000], Loss: 0.0718\n",
      "Epoch [659/1000], Loss: 0.0702\n",
      "Epoch [660/1000], Loss: 0.0711\n",
      "Epoch [661/1000], Loss: 0.0719\n",
      "Epoch [662/1000], Loss: 0.0702\n",
      "Epoch [663/1000], Loss: 0.0678\n",
      "Epoch [664/1000], Loss: 0.0663\n",
      "Epoch [665/1000], Loss: 0.0668\n",
      "Epoch [666/1000], Loss: 0.0686\n",
      "Epoch [667/1000], Loss: 0.0686\n",
      "Epoch [668/1000], Loss: 0.0678\n",
      "Epoch [669/1000], Loss: 0.0691\n",
      "Epoch [670/1000], Loss: 0.0681\n",
      "Epoch [671/1000], Loss: 0.0671\n",
      "Epoch [672/1000], Loss: 0.0678\n",
      "Epoch [673/1000], Loss: 0.0691\n",
      "Epoch [674/1000], Loss: 0.0671\n",
      "Epoch [675/1000], Loss: 0.0665\n",
      "Epoch [676/1000], Loss: 0.0673\n",
      "Epoch [677/1000], Loss: 0.0671\n",
      "Epoch [678/1000], Loss: 0.0673\n",
      "Epoch [679/1000], Loss: 0.0675\n",
      "Epoch [680/1000], Loss: 0.0660\n",
      "Epoch [681/1000], Loss: 0.0643\n",
      "Epoch [682/1000], Loss: 0.0637\n",
      "Epoch [683/1000], Loss: 0.0646\n",
      "Epoch [684/1000], Loss: 0.0630\n",
      "Epoch [685/1000], Loss: 0.0629\n",
      "Epoch [686/1000], Loss: 0.0638\n",
      "Epoch [687/1000], Loss: 0.0643\n",
      "Epoch [688/1000], Loss: 0.0634\n",
      "Epoch [689/1000], Loss: 0.0634\n",
      "Epoch [690/1000], Loss: 0.0630\n",
      "Epoch [691/1000], Loss: 0.0633\n",
      "Epoch [692/1000], Loss: 0.0643\n",
      "Epoch [693/1000], Loss: 0.0659\n",
      "Epoch [694/1000], Loss: 0.0653\n",
      "Epoch [695/1000], Loss: 0.0652\n",
      "Epoch [696/1000], Loss: 0.0652\n",
      "Epoch [697/1000], Loss: 0.0650\n",
      "Epoch [698/1000], Loss: 0.0650\n",
      "Epoch [699/1000], Loss: 0.0654\n",
      "Epoch [700/1000], Loss: 0.0654\n",
      "Epoch [701/1000], Loss: 0.0663\n",
      "Epoch [702/1000], Loss: 0.0681\n",
      "Epoch [703/1000], Loss: 0.0702\n",
      "Epoch [704/1000], Loss: 0.0697\n",
      "Epoch [705/1000], Loss: 0.0692\n",
      "Epoch [706/1000], Loss: 0.0688\n",
      "Epoch [707/1000], Loss: 0.0686\n",
      "Epoch [708/1000], Loss: 0.0676\n",
      "Epoch [709/1000], Loss: 0.0668\n",
      "Epoch [710/1000], Loss: 0.0664\n",
      "Epoch [711/1000], Loss: 0.0654\n",
      "Epoch [712/1000], Loss: 0.0660\n",
      "Epoch [713/1000], Loss: 0.0656\n",
      "Epoch [714/1000], Loss: 0.0650\n",
      "Epoch [715/1000], Loss: 0.0660\n",
      "Epoch [716/1000], Loss: 0.0650\n",
      "Epoch [717/1000], Loss: 0.0669\n",
      "Epoch [718/1000], Loss: 0.0700\n",
      "Epoch [719/1000], Loss: 0.0701\n",
      "Epoch [720/1000], Loss: 0.0692\n",
      "Epoch [721/1000], Loss: 0.0677\n",
      "Epoch [722/1000], Loss: 0.0687\n",
      "Epoch [723/1000], Loss: 0.0683\n",
      "Epoch [724/1000], Loss: 0.0684\n",
      "Epoch [725/1000], Loss: 0.0687\n",
      "Epoch [726/1000], Loss: 0.0669\n",
      "Epoch [727/1000], Loss: 0.0667\n",
      "Epoch [728/1000], Loss: 0.0660\n",
      "Epoch [729/1000], Loss: 0.0649\n",
      "Epoch [730/1000], Loss: 0.0659\n",
      "Epoch [731/1000], Loss: 0.0644\n",
      "Epoch [732/1000], Loss: 0.0647\n",
      "Epoch [733/1000], Loss: 0.0661\n",
      "Epoch [734/1000], Loss: 0.0675\n",
      "Epoch [735/1000], Loss: 0.0671\n",
      "Epoch [736/1000], Loss: 0.0668\n",
      "Epoch [737/1000], Loss: 0.0661\n",
      "Epoch [738/1000], Loss: 0.0657\n",
      "Epoch [739/1000], Loss: 0.0664\n",
      "Epoch [740/1000], Loss: 0.0663\n",
      "Epoch [741/1000], Loss: 0.0645\n",
      "Epoch [742/1000], Loss: 0.0633\n",
      "Epoch [743/1000], Loss: 0.0630\n",
      "Epoch [744/1000], Loss: 0.0636\n",
      "Epoch [745/1000], Loss: 0.0649\n",
      "Epoch [746/1000], Loss: 0.0644\n",
      "Epoch [747/1000], Loss: 0.0637\n",
      "Epoch [748/1000], Loss: 0.0636\n",
      "Epoch [749/1000], Loss: 0.0653\n",
      "Epoch [750/1000], Loss: 0.0633\n",
      "Epoch [751/1000], Loss: 0.0630\n",
      "Epoch [752/1000], Loss: 0.0624\n",
      "Epoch [753/1000], Loss: 0.0627\n",
      "Epoch [754/1000], Loss: 0.0638\n",
      "Epoch [755/1000], Loss: 0.0630\n",
      "Epoch [756/1000], Loss: 0.0620\n",
      "Epoch [757/1000], Loss: 0.0621\n",
      "Epoch [758/1000], Loss: 0.0610\n",
      "Epoch [759/1000], Loss: 0.0631\n",
      "Epoch [760/1000], Loss: 0.0624\n",
      "Epoch [761/1000], Loss: 0.0622\n",
      "Epoch [762/1000], Loss: 0.0620\n",
      "Epoch [763/1000], Loss: 0.0616\n",
      "Epoch [764/1000], Loss: 0.0621\n",
      "Epoch [765/1000], Loss: 0.0628\n",
      "Epoch [766/1000], Loss: 0.0628\n",
      "Epoch [767/1000], Loss: 0.0631\n",
      "Epoch [768/1000], Loss: 0.0628\n",
      "Epoch [769/1000], Loss: 0.0621\n",
      "Epoch [770/1000], Loss: 0.0622\n",
      "Epoch [771/1000], Loss: 0.0626\n",
      "Epoch [772/1000], Loss: 0.0636\n",
      "Epoch [773/1000], Loss: 0.0634\n",
      "Epoch [774/1000], Loss: 0.0638\n",
      "Epoch [775/1000], Loss: 0.0651\n",
      "Epoch [776/1000], Loss: 0.0629\n",
      "Epoch [777/1000], Loss: 0.0619\n",
      "Epoch [778/1000], Loss: 0.0634\n",
      "Epoch [779/1000], Loss: 0.0643\n",
      "Epoch [780/1000], Loss: 0.0636\n",
      "Epoch [781/1000], Loss: 0.0628\n",
      "Epoch [782/1000], Loss: 0.0608\n",
      "Epoch [783/1000], Loss: 0.0606\n",
      "Epoch [784/1000], Loss: 0.0618\n",
      "Epoch [785/1000], Loss: 0.0632\n",
      "Epoch [786/1000], Loss: 0.0611\n",
      "Epoch [787/1000], Loss: 0.0605\n",
      "Epoch [788/1000], Loss: 0.0602\n",
      "Epoch [789/1000], Loss: 0.0604\n",
      "Epoch [790/1000], Loss: 0.0614\n",
      "Epoch [791/1000], Loss: 0.0623\n",
      "Epoch [792/1000], Loss: 0.0623\n",
      "Epoch [793/1000], Loss: 0.0617\n",
      "Epoch [794/1000], Loss: 0.0606\n",
      "Epoch [795/1000], Loss: 0.0612\n",
      "Epoch [796/1000], Loss: 0.0611\n",
      "Epoch [797/1000], Loss: 0.0604\n",
      "Epoch [798/1000], Loss: 0.0616\n",
      "Epoch [799/1000], Loss: 0.0611\n",
      "Epoch [800/1000], Loss: 0.0611\n",
      "Epoch [801/1000], Loss: 0.0614\n",
      "Epoch [802/1000], Loss: 0.0609\n",
      "Epoch [803/1000], Loss: 0.0632\n",
      "Epoch [804/1000], Loss: 0.0618\n",
      "Epoch [805/1000], Loss: 0.0607\n",
      "Epoch [806/1000], Loss: 0.0604\n",
      "Epoch [807/1000], Loss: 0.0601\n",
      "Epoch [808/1000], Loss: 0.0601\n",
      "Epoch [809/1000], Loss: 0.0618\n",
      "Epoch [810/1000], Loss: 0.0615\n",
      "Epoch [811/1000], Loss: 0.0612\n",
      "Epoch [812/1000], Loss: 0.0619\n",
      "Epoch [813/1000], Loss: 0.0609\n",
      "Epoch [814/1000], Loss: 0.0601\n",
      "Epoch [815/1000], Loss: 0.0599\n",
      "Epoch [816/1000], Loss: 0.0599\n",
      "Epoch [817/1000], Loss: 0.0599\n",
      "Epoch [818/1000], Loss: 0.0614\n",
      "Epoch [819/1000], Loss: 0.0609\n",
      "Epoch [820/1000], Loss: 0.0626\n",
      "Epoch [821/1000], Loss: 0.0636\n",
      "Epoch [822/1000], Loss: 0.0627\n",
      "Epoch [823/1000], Loss: 0.0632\n",
      "Epoch [824/1000], Loss: 0.0624\n",
      "Epoch [825/1000], Loss: 0.0622\n",
      "Epoch [826/1000], Loss: 0.0627\n",
      "Epoch [827/1000], Loss: 0.0620\n",
      "Epoch [828/1000], Loss: 0.0635\n",
      "Epoch [829/1000], Loss: 0.0621\n",
      "Epoch [830/1000], Loss: 0.0608\n",
      "Epoch [831/1000], Loss: 0.0611\n",
      "Epoch [832/1000], Loss: 0.0605\n",
      "Epoch [833/1000], Loss: 0.0606\n",
      "Epoch [834/1000], Loss: 0.0600\n",
      "Epoch [835/1000], Loss: 0.0605\n",
      "Epoch [836/1000], Loss: 0.0612\n",
      "Epoch [837/1000], Loss: 0.0601\n",
      "Epoch [838/1000], Loss: 0.0606\n",
      "Epoch [839/1000], Loss: 0.0599\n",
      "Epoch [840/1000], Loss: 0.0593\n",
      "Epoch [841/1000], Loss: 0.0593\n",
      "Epoch [842/1000], Loss: 0.0591\n",
      "Epoch [843/1000], Loss: 0.0584\n",
      "Epoch [844/1000], Loss: 0.0570\n",
      "Epoch [845/1000], Loss: 0.0579\n",
      "Epoch [846/1000], Loss: 0.0575\n",
      "Epoch [847/1000], Loss: 0.0572\n",
      "Epoch [848/1000], Loss: 0.0568\n",
      "Epoch [849/1000], Loss: 0.0561\n",
      "Epoch [850/1000], Loss: 0.0569\n",
      "Epoch [851/1000], Loss: 0.0570\n",
      "Epoch [852/1000], Loss: 0.0567\n",
      "Epoch [853/1000], Loss: 0.0570\n",
      "Epoch [854/1000], Loss: 0.0572\n",
      "Epoch [855/1000], Loss: 0.0577\n",
      "Epoch [856/1000], Loss: 0.0573\n",
      "Epoch [857/1000], Loss: 0.0579\n",
      "Epoch [858/1000], Loss: 0.0582\n",
      "Epoch [859/1000], Loss: 0.0583\n",
      "Epoch [860/1000], Loss: 0.0591\n",
      "Epoch [861/1000], Loss: 0.0588\n",
      "Epoch [862/1000], Loss: 0.0579\n",
      "Epoch [863/1000], Loss: 0.0580\n",
      "Epoch [864/1000], Loss: 0.0588\n",
      "Epoch [865/1000], Loss: 0.0589\n",
      "Epoch [866/1000], Loss: 0.0590\n",
      "Epoch [867/1000], Loss: 0.0584\n",
      "Epoch [868/1000], Loss: 0.0585\n",
      "Epoch [869/1000], Loss: 0.0579\n",
      "Epoch [870/1000], Loss: 0.0586\n",
      "Epoch [871/1000], Loss: 0.0582\n",
      "Epoch [872/1000], Loss: 0.0596\n",
      "Epoch [873/1000], Loss: 0.0597\n",
      "Epoch [874/1000], Loss: 0.0586\n",
      "Epoch [875/1000], Loss: 0.0570\n",
      "Epoch [876/1000], Loss: 0.0575\n",
      "Epoch [877/1000], Loss: 0.0577\n",
      "Epoch [878/1000], Loss: 0.0580\n",
      "Epoch [879/1000], Loss: 0.0578\n",
      "Epoch [880/1000], Loss: 0.0589\n",
      "Epoch [881/1000], Loss: 0.0582\n",
      "Epoch [882/1000], Loss: 0.0593\n",
      "Epoch [883/1000], Loss: 0.0589\n",
      "Epoch [884/1000], Loss: 0.0604\n",
      "Epoch [885/1000], Loss: 0.0584\n",
      "Epoch [886/1000], Loss: 0.0580\n",
      "Epoch [887/1000], Loss: 0.0585\n",
      "Epoch [888/1000], Loss: 0.0568\n",
      "Epoch [889/1000], Loss: 0.0569\n",
      "Epoch [890/1000], Loss: 0.0574\n",
      "Epoch [891/1000], Loss: 0.0573\n",
      "Epoch [892/1000], Loss: 0.0575\n",
      "Epoch [893/1000], Loss: 0.0592\n",
      "Epoch [894/1000], Loss: 0.0602\n",
      "Epoch [895/1000], Loss: 0.0595\n",
      "Epoch [896/1000], Loss: 0.0584\n",
      "Epoch [897/1000], Loss: 0.0575\n",
      "Epoch [898/1000], Loss: 0.0570\n",
      "Epoch [899/1000], Loss: 0.0561\n",
      "Epoch [900/1000], Loss: 0.0563\n",
      "Epoch [901/1000], Loss: 0.0560\n",
      "Epoch [902/1000], Loss: 0.0558\n",
      "Epoch [903/1000], Loss: 0.0561\n",
      "Epoch [904/1000], Loss: 0.0553\n",
      "Epoch [905/1000], Loss: 0.0561\n",
      "Epoch [906/1000], Loss: 0.0567\n",
      "Epoch [907/1000], Loss: 0.0570\n",
      "Epoch [908/1000], Loss: 0.0576\n",
      "Epoch [909/1000], Loss: 0.0596\n",
      "Epoch [910/1000], Loss: 0.0580\n",
      "Epoch [911/1000], Loss: 0.0569\n",
      "Epoch [912/1000], Loss: 0.0552\n",
      "Epoch [913/1000], Loss: 0.0543\n",
      "Epoch [914/1000], Loss: 0.0546\n",
      "Epoch [915/1000], Loss: 0.0540\n",
      "Epoch [916/1000], Loss: 0.0535\n",
      "Epoch [917/1000], Loss: 0.0543\n",
      "Epoch [918/1000], Loss: 0.0544\n",
      "Epoch [919/1000], Loss: 0.0545\n",
      "Epoch [920/1000], Loss: 0.0557\n",
      "Epoch [921/1000], Loss: 0.0565\n",
      "Epoch [922/1000], Loss: 0.0560\n",
      "Epoch [923/1000], Loss: 0.0547\n",
      "Epoch [924/1000], Loss: 0.0542\n",
      "Epoch [925/1000], Loss: 0.0563\n",
      "Epoch [926/1000], Loss: 0.0555\n",
      "Epoch [927/1000], Loss: 0.0556\n",
      "Epoch [928/1000], Loss: 0.0540\n",
      "Epoch [929/1000], Loss: 0.0546\n",
      "Epoch [930/1000], Loss: 0.0556\n",
      "Epoch [931/1000], Loss: 0.0546\n",
      "Epoch [932/1000], Loss: 0.0546\n",
      "Epoch [933/1000], Loss: 0.0550\n",
      "Epoch [934/1000], Loss: 0.0545\n",
      "Epoch [935/1000], Loss: 0.0553\n",
      "Epoch [936/1000], Loss: 0.0561\n",
      "Epoch [937/1000], Loss: 0.0549\n",
      "Epoch [938/1000], Loss: 0.0567\n",
      "Epoch [939/1000], Loss: 0.0569\n",
      "Epoch [940/1000], Loss: 0.0563\n",
      "Epoch [941/1000], Loss: 0.0565\n",
      "Epoch [942/1000], Loss: 0.0560\n",
      "Epoch [943/1000], Loss: 0.0556\n",
      "Epoch [944/1000], Loss: 0.0564\n",
      "Epoch [945/1000], Loss: 0.0553\n",
      "Epoch [946/1000], Loss: 0.0548\n",
      "Epoch [947/1000], Loss: 0.0543\n",
      "Epoch [948/1000], Loss: 0.0546\n",
      "Epoch [949/1000], Loss: 0.0556\n",
      "Epoch [950/1000], Loss: 0.0559\n",
      "Epoch [951/1000], Loss: 0.0564\n",
      "Epoch [952/1000], Loss: 0.0578\n",
      "Epoch [953/1000], Loss: 0.0594\n",
      "Epoch [954/1000], Loss: 0.0586\n",
      "Epoch [955/1000], Loss: 0.0582\n",
      "Epoch [956/1000], Loss: 0.0581\n",
      "Epoch [957/1000], Loss: 0.0566\n",
      "Epoch [958/1000], Loss: 0.0571\n",
      "Epoch [959/1000], Loss: 0.0561\n",
      "Epoch [960/1000], Loss: 0.0550\n",
      "Epoch [961/1000], Loss: 0.0542\n",
      "Epoch [962/1000], Loss: 0.0539\n",
      "Epoch [963/1000], Loss: 0.0542\n",
      "Epoch [964/1000], Loss: 0.0559\n",
      "Epoch [965/1000], Loss: 0.0557\n",
      "Epoch [966/1000], Loss: 0.0562\n",
      "Epoch [967/1000], Loss: 0.0560\n",
      "Epoch [968/1000], Loss: 0.0556\n",
      "Epoch [969/1000], Loss: 0.0556\n",
      "Epoch [970/1000], Loss: 0.0565\n",
      "Epoch [971/1000], Loss: 0.0564\n",
      "Epoch [972/1000], Loss: 0.0569\n",
      "Epoch [973/1000], Loss: 0.0583\n",
      "Epoch [974/1000], Loss: 0.0570\n",
      "Epoch [975/1000], Loss: 0.0556\n",
      "Epoch [976/1000], Loss: 0.0548\n",
      "Epoch [977/1000], Loss: 0.0538\n",
      "Epoch [978/1000], Loss: 0.0536\n",
      "Epoch [979/1000], Loss: 0.0536\n",
      "Epoch [980/1000], Loss: 0.0548\n",
      "Epoch [981/1000], Loss: 0.0547\n",
      "Epoch [982/1000], Loss: 0.0557\n",
      "Epoch [983/1000], Loss: 0.0569\n",
      "Epoch [984/1000], Loss: 0.0555\n",
      "Epoch [985/1000], Loss: 0.0558\n",
      "Epoch [986/1000], Loss: 0.0562\n",
      "Epoch [987/1000], Loss: 0.0558\n",
      "Epoch [988/1000], Loss: 0.0550\n",
      "Epoch [989/1000], Loss: 0.0547\n",
      "Epoch [990/1000], Loss: 0.0546\n",
      "Epoch [991/1000], Loss: 0.0543\n",
      "Epoch [992/1000], Loss: 0.0542\n",
      "Epoch [993/1000], Loss: 0.0530\n",
      "Epoch [994/1000], Loss: 0.0531\n",
      "Epoch [995/1000], Loss: 0.0529\n",
      "Epoch [996/1000], Loss: 0.0529\n",
      "Epoch [997/1000], Loss: 0.0534\n",
      "Epoch [998/1000], Loss: 0.0537\n",
      "Epoch [999/1000], Loss: 0.0548\n",
      "Epoch [1000/1000], Loss: 0.0544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48879022896289825,\n",
       " 0.4583757072687149,\n",
       " 0.4350278601050377,\n",
       " 0.41225976683199406,\n",
       " 0.3900135736912489,\n",
       " 0.3672998361289501,\n",
       " 0.3454465363174677,\n",
       " 0.32537527941167355,\n",
       " 0.30685845017433167,\n",
       " 0.2935327049344778,\n",
       " 0.28268183395266533,\n",
       " 0.2734734546393156,\n",
       " 0.2671023244038224,\n",
       " 0.2610362060368061,\n",
       " 0.25739262718707323,\n",
       " 0.25440324656665325,\n",
       " 0.2545580752193928,\n",
       " 0.24947510194033384,\n",
       " 0.24712234176695347,\n",
       " 0.2441177824512124,\n",
       " 0.24208341352641582,\n",
       " 0.23943259567022324,\n",
       " 0.23791407886892557,\n",
       " 0.23658426851034164,\n",
       " 0.237641136161983,\n",
       " 0.23843551147729158,\n",
       " 0.23596907127648592,\n",
       " 0.23245725221931934,\n",
       " 0.2272246303036809,\n",
       " 0.22415499109774828,\n",
       " 0.22206301148980856,\n",
       " 0.22058347705751657,\n",
       " 0.21909974422305822,\n",
       " 0.21759890858083963,\n",
       " 0.21681061573326588,\n",
       " 0.21733608096837997,\n",
       " 0.21654117479920387,\n",
       " 0.21704340539872646,\n",
       " 0.21742731239646673,\n",
       " 0.21698286291211843,\n",
       " 0.21528511587530375,\n",
       " 0.21434705704450607,\n",
       " 0.21353930793702602,\n",
       " 0.21385412570089102,\n",
       " 0.21327823493629694,\n",
       " 0.2115912763401866,\n",
       " 0.2089016530662775,\n",
       " 0.20676645450294018,\n",
       " 0.2058739922940731,\n",
       " 0.20525837782770395,\n",
       " 0.2055917652323842,\n",
       " 0.20648817997425795,\n",
       " 0.20602519437670708,\n",
       " 0.2054562410339713,\n",
       " 0.20281743723899126,\n",
       " 0.200353660620749,\n",
       " 0.19942997116595507,\n",
       " 0.19807687774300575,\n",
       " 0.1974728899076581,\n",
       " 0.1965265367180109,\n",
       " 0.19646522402763367,\n",
       " 0.19738103728741407,\n",
       " 0.19681589491665363,\n",
       " 0.19654430635273457,\n",
       " 0.1971477633342147,\n",
       " 0.19711875170469284,\n",
       " 0.19567705877125263,\n",
       " 0.19485891703516245,\n",
       " 0.19188465271145105,\n",
       " 0.19106607977300882,\n",
       " 0.19016040675342083,\n",
       " 0.18912966549396515,\n",
       " 0.1894577657803893,\n",
       " 0.1899362299591303,\n",
       " 0.19190386962145567,\n",
       " 0.19083096459507942,\n",
       " 0.18997582141309977,\n",
       " 0.1894781617447734,\n",
       " 0.1901278169825673,\n",
       " 0.18993054423481226,\n",
       " 0.18973728362470865,\n",
       " 0.1888576066121459,\n",
       " 0.18867434095591307,\n",
       " 0.18811444658786058,\n",
       " 0.1857479913160205,\n",
       " 0.18243239726871252,\n",
       " 0.1814424293115735,\n",
       " 0.1793634621426463,\n",
       " 0.17937641497701406,\n",
       " 0.17893258668482304,\n",
       " 0.1774896802380681,\n",
       " 0.1777970353141427,\n",
       " 0.17887144349515438,\n",
       " 0.17878764495253563,\n",
       " 0.17918272875249386,\n",
       " 0.17982553131878376,\n",
       " 0.17770785931497812,\n",
       " 0.17772069945931435,\n",
       " 0.17723356559872627,\n",
       " 0.1773703284561634,\n",
       " 0.1788766859099269,\n",
       " 0.17764158733189106,\n",
       " 0.1760175870731473,\n",
       " 0.17486108466982841,\n",
       " 0.17402218654751778,\n",
       " 0.17323942575603724,\n",
       " 0.1725058825686574,\n",
       " 0.17242780327796936,\n",
       " 0.17219148948788643,\n",
       " 0.1716146720573306,\n",
       " 0.17127646133303642,\n",
       " 0.17078322358429432,\n",
       " 0.1690532397478819,\n",
       " 0.16914802230894566,\n",
       " 0.1679808609187603,\n",
       " 0.16797965299338102,\n",
       " 0.16762927267700434,\n",
       " 0.16732048336416483,\n",
       " 0.16688936948776245,\n",
       " 0.16653881408274174,\n",
       " 0.16493001766502857,\n",
       " 0.16669033747166395,\n",
       " 0.16703526861965656,\n",
       " 0.16743475012481213,\n",
       " 0.16615346167236567,\n",
       " 0.16436875332146883,\n",
       " 0.16438063699752092,\n",
       " 0.1625756062567234,\n",
       " 0.1609421707689762,\n",
       " 0.16105061955749989,\n",
       " 0.16067993827164173,\n",
       " 0.16017859522253275,\n",
       " 0.15989686455577612,\n",
       " 0.15887737646698952,\n",
       " 0.15860967617481947,\n",
       " 0.15795201901346445,\n",
       " 0.15874560922384262,\n",
       " 0.15919804479926825,\n",
       " 0.1589623112231493,\n",
       " 0.15702927578240633,\n",
       " 0.15583427529782057,\n",
       " 0.1549645783379674,\n",
       " 0.15476081985980272,\n",
       " 0.1547618554905057,\n",
       " 0.1536728898063302,\n",
       " 0.1528547564521432,\n",
       " 0.15346765611320734,\n",
       " 0.15259138215333223,\n",
       " 0.15086693223565817,\n",
       " 0.15004516579210758,\n",
       " 0.150240499060601,\n",
       " 0.14986111503094435,\n",
       " 0.15061041712760925,\n",
       " 0.14996570628136396,\n",
       " 0.14982884470373392,\n",
       " 0.150001117028296,\n",
       " 0.151530503295362,\n",
       " 0.1495944345369935,\n",
       " 0.1497083194553852,\n",
       " 0.1493277046829462,\n",
       " 0.1487726029008627,\n",
       " 0.1504940278828144,\n",
       " 0.15046493476256728,\n",
       " 0.15343455784022808,\n",
       " 0.15086718276143074,\n",
       " 0.14745386969298124,\n",
       " 0.14330192329362035,\n",
       " 0.14230023510754108,\n",
       " 0.14402814861387014,\n",
       " 0.14469640096649528,\n",
       " 0.14534306805580854,\n",
       " 0.14823831897228956,\n",
       " 0.14763433206826448,\n",
       " 0.14410123182460666,\n",
       " 0.14367277640849352,\n",
       " 0.14358834363520145,\n",
       " 0.14444319577887654,\n",
       " 0.14289026288315654,\n",
       " 0.14414696069434285,\n",
       " 0.14210945088416338,\n",
       " 0.1410119696520269,\n",
       " 0.13849170180037618,\n",
       " 0.13748911581933498,\n",
       " 0.13582401955500245,\n",
       " 0.13751598773524165,\n",
       " 0.13727431884035468,\n",
       " 0.13476008595898747,\n",
       " 0.13498389814049006,\n",
       " 0.13523181295022368,\n",
       " 0.13780805049464107,\n",
       " 0.13560137106105685,\n",
       " 0.13591259671375155,\n",
       " 0.1363354050554335,\n",
       " 0.13494142144918442,\n",
       " 0.13473355118185282,\n",
       " 0.13427256932482123,\n",
       " 0.13414462469518185,\n",
       " 0.1350629674270749,\n",
       " 0.1354561261832714,\n",
       " 0.13561468804255128,\n",
       " 0.13406404620036483,\n",
       " 0.13473482197150588,\n",
       " 0.13162098685279489,\n",
       " 0.12999057117849588,\n",
       " 0.13064044760540128,\n",
       " 0.130539461504668,\n",
       " 0.12957429978996515,\n",
       " 0.12977255461737514,\n",
       " 0.130411212798208,\n",
       " 0.13249167054891586,\n",
       " 0.13028018176555634,\n",
       " 0.12880709813907743,\n",
       " 0.1271258764900267,\n",
       " 0.12714419420808554,\n",
       " 0.12714026030153036,\n",
       " 0.1256079412996769,\n",
       " 0.125390755943954,\n",
       " 0.1250413404777646,\n",
       " 0.12572781136259437,\n",
       " 0.1246245251968503,\n",
       " 0.1251565609127283,\n",
       " 0.12508817901834846,\n",
       " 0.12692359229549766,\n",
       " 0.12876170221716166,\n",
       " 0.12597629660740495,\n",
       " 0.12384400423616171,\n",
       " 0.1242265859618783,\n",
       " 0.12125361477956176,\n",
       " 0.12113957898691297,\n",
       " 0.12147545348852873,\n",
       " 0.12140995496883988,\n",
       " 0.12152369087561965,\n",
       " 0.12116679875180125,\n",
       " 0.11986903939396143,\n",
       " 0.12053127586841583,\n",
       " 0.12197703216224909,\n",
       " 0.1195674198679626,\n",
       " 0.11960711050778627,\n",
       " 0.11979859508574009,\n",
       " 0.11946641653776169,\n",
       " 0.12058874173089862,\n",
       " 0.11797897610813379,\n",
       " 0.11576149426400661,\n",
       " 0.11646074429154396,\n",
       " 0.11576171685010195,\n",
       " 0.11515626683831215,\n",
       " 0.11682654730975628,\n",
       " 0.11684994818642735,\n",
       " 0.11614773888140917,\n",
       " 0.11510987719520926,\n",
       " 0.11552314227446914,\n",
       " 0.11438001086935401,\n",
       " 0.11444755410775542,\n",
       " 0.11360639706254005,\n",
       " 0.11260927049443126,\n",
       " 0.11225597048178315,\n",
       " 0.11463631922379136,\n",
       " 0.11875525303184986,\n",
       " 0.11940316436812282,\n",
       " 0.11915918765589595,\n",
       " 0.11703221360221505,\n",
       " 0.11074591940268874,\n",
       " 0.11168711958453059,\n",
       " 0.10986538650467992,\n",
       " 0.11140977870672941,\n",
       " 0.1119618103839457,\n",
       " 0.11128578661009669,\n",
       " 0.11014792835339904,\n",
       " 0.11017603101208806,\n",
       " 0.10943767474964261,\n",
       " 0.11011365940794349,\n",
       " 0.11163064325228333,\n",
       " 0.10994054609909654,\n",
       " 0.10929503431543708,\n",
       " 0.10796378832310438,\n",
       " 0.10765899065881968,\n",
       " 0.10839506844058633,\n",
       " 0.10997679084539413,\n",
       " 0.10899123130366206,\n",
       " 0.10710401413962245,\n",
       " 0.10645396588370204,\n",
       " 0.1068435232155025,\n",
       " 0.10512579279020429,\n",
       " 0.10434083361178637,\n",
       " 0.10375551506876945,\n",
       " 0.10338356392458081,\n",
       " 0.10496321553364396,\n",
       " 0.10459825396537781,\n",
       " 0.10452993307262659,\n",
       " 0.10379412490874529,\n",
       " 0.10576896602287889,\n",
       " 0.10635618027299643,\n",
       " 0.10487704444676638,\n",
       " 0.10531953349709511,\n",
       " 0.105324593372643,\n",
       " 0.10649474477395415,\n",
       " 0.10604421701282263,\n",
       " 0.1054149386473,\n",
       " 0.10308515792712569,\n",
       " 0.10148490173742175,\n",
       " 0.10031521506607533,\n",
       " 0.10179700562730432,\n",
       " 0.10004379320889711,\n",
       " 0.09946109866723418,\n",
       " 0.10131208831444383,\n",
       " 0.10191632248461246,\n",
       " 0.10154193174093962,\n",
       " 0.10300185065716505,\n",
       " 0.10311818215996027,\n",
       " 0.10132456105202436,\n",
       " 0.09984407899901271,\n",
       " 0.09992944169789553,\n",
       " 0.10190992802381516,\n",
       " 0.10233211191371083,\n",
       " 0.1011401847936213,\n",
       " 0.09851881954818964,\n",
       " 0.0976545694284141,\n",
       " 0.09879485657438636,\n",
       " 0.09893234958872199,\n",
       " 0.09963484760373831,\n",
       " 0.09957723133265972,\n",
       " 0.09909026511013508,\n",
       " 0.09807131206616759,\n",
       " 0.0974587113596499,\n",
       " 0.09610204258933663,\n",
       " 0.0971722798421979,\n",
       " 0.09760142164304852,\n",
       " 0.09722576336935163,\n",
       " 0.09902173606678843,\n",
       " 0.0983647657558322,\n",
       " 0.09717368520796299,\n",
       " 0.09860957181081176,\n",
       " 0.09884189488366246,\n",
       " 0.09881931589916348,\n",
       " 0.0977635090239346,\n",
       " 0.09653388755396008,\n",
       " 0.09485046565532684,\n",
       " 0.0939448238350451,\n",
       " 0.0935898870229721,\n",
       " 0.09429103741422296,\n",
       " 0.09307659603655338,\n",
       " 0.09293905179947615,\n",
       " 0.09412413975223899,\n",
       " 0.09546246891841292,\n",
       " 0.09658981347456574,\n",
       " 0.09342917520552874,\n",
       " 0.09110510721802711,\n",
       " 0.09029251104220748,\n",
       " 0.09090062417089939,\n",
       " 0.09320924011990428,\n",
       " 0.09185108495876193,\n",
       " 0.09053149726241827,\n",
       " 0.0907740481197834,\n",
       " 0.09231010312214494,\n",
       " 0.09275111276656389,\n",
       " 0.09333766950294375,\n",
       " 0.09376703668385744,\n",
       " 0.09157889941707253,\n",
       " 0.0921675250865519,\n",
       " 0.09155629482120275,\n",
       " 0.0918062194250524,\n",
       " 0.09331431891769171,\n",
       " 0.0937385349534452,\n",
       " 0.09440439520403743,\n",
       " 0.09707095567137003,\n",
       " 0.09874236676841974,\n",
       " 0.09988545486703515,\n",
       " 0.10138002131134272,\n",
       " 0.10166558157652617,\n",
       " 0.10083555802702904,\n",
       " 0.10143613908439875,\n",
       " 0.10002882545813918,\n",
       " 0.09920552046969533,\n",
       " 0.0983479805290699,\n",
       " 0.09799932362511754,\n",
       " 0.09454701701179147,\n",
       " 0.09358185483142734,\n",
       " 0.09091182053089142,\n",
       " 0.08936708187684417,\n",
       " 0.08908618986606598,\n",
       " 0.08994326973333955,\n",
       " 0.09047098597511649,\n",
       " 0.08905215514823794,\n",
       " 0.08968400023877621,\n",
       " 0.08851019851863384,\n",
       " 0.08820687234401703,\n",
       " 0.08968241745606065,\n",
       " 0.08984431019052863,\n",
       " 0.088321712333709,\n",
       " 0.08837201306596398,\n",
       " 0.09013254335150123,\n",
       " 0.08993966784328222,\n",
       " 0.09099920326843858,\n",
       " 0.09052601549774408,\n",
       " 0.08976583974435925,\n",
       " 0.08783271536231041,\n",
       " 0.08658006088808179,\n",
       " 0.08625626657158136,\n",
       " 0.08924216916784644,\n",
       " 0.08944567106664181,\n",
       " 0.08864656370133162,\n",
       " 0.08793292986229062,\n",
       " 0.0871969354338944,\n",
       " 0.08768393658101559,\n",
       " 0.08986643748357892,\n",
       " 0.09025121247395873,\n",
       " 0.09192227199673653,\n",
       " 0.08960328157991171,\n",
       " 0.09077473124489188,\n",
       " 0.09470430528745055,\n",
       " 0.10003770887851715,\n",
       " 0.0990797532722354,\n",
       " 0.09903188422322273,\n",
       " 0.095962586812675,\n",
       " 0.09306029975414276,\n",
       " 0.09246897092089057,\n",
       " 0.09108122764155269,\n",
       " 0.09087937325239182,\n",
       " 0.0895854402333498,\n",
       " 0.08966315910220146,\n",
       " 0.09098199661821127,\n",
       " 0.09199521830305457,\n",
       " 0.09318086365237832,\n",
       " 0.09226309228688478,\n",
       " 0.08947458071634173,\n",
       " 0.087903774343431,\n",
       " 0.08662587869912386,\n",
       " 0.08653671480715275,\n",
       " 0.08739525452256203,\n",
       " 0.08547061309218407,\n",
       " 0.08491922449320555,\n",
       " 0.0846686428412795,\n",
       " 0.08483776077628136,\n",
       " 0.08538078283891082,\n",
       " 0.08631594432517886,\n",
       " 0.08567540347576141,\n",
       " 0.08591007255017757,\n",
       " 0.08429683092981577,\n",
       " 0.08481637854129076,\n",
       " 0.08510198630392551,\n",
       " 0.0870330729521811,\n",
       " 0.08629778632894158,\n",
       " 0.08548679342493415,\n",
       " 0.08379505667835474,\n",
       " 0.08381156949326396,\n",
       " 0.08351914072409272,\n",
       " 0.08490177057683468,\n",
       " 0.08431352395564318,\n",
       " 0.08410347579047084,\n",
       " 0.08407780202105641,\n",
       " 0.08413122314959764,\n",
       " 0.08394758263602853,\n",
       " 0.08396102767437696,\n",
       " 0.08353020483627915,\n",
       " 0.08266439475119114,\n",
       " 0.08238824643194675,\n",
       " 0.08216653158888221,\n",
       " 0.0825587254948914,\n",
       " 0.08296855818480253,\n",
       " 0.08485321747139096,\n",
       " 0.08401414146646857,\n",
       " 0.08509578369557858,\n",
       " 0.08601960213854909,\n",
       " 0.08765117824077606,\n",
       " 0.0854050051420927,\n",
       " 0.08458920381963253,\n",
       " 0.08612951403483748,\n",
       " 0.08474552584812045,\n",
       " 0.08700038446113467,\n",
       " 0.08560038870200515,\n",
       " 0.08414341509342194,\n",
       " 0.08299184357747436,\n",
       " 0.08090492896735668,\n",
       " 0.07969145011156797,\n",
       " 0.07891251798719168,\n",
       " 0.07826366368681192,\n",
       " 0.07887891912832856,\n",
       " 0.08078577555716038,\n",
       " 0.08048706408590078,\n",
       " 0.07979475939646363,\n",
       " 0.07991414610296488,\n",
       " 0.08118705451488495,\n",
       " 0.08008995791897178,\n",
       " 0.07960043149068952,\n",
       " 0.08048694860190153,\n",
       " 0.08121633715927601,\n",
       " 0.08148513454943895,\n",
       " 0.08215896738693118,\n",
       " 0.08074613148346543,\n",
       " 0.08066220255568624,\n",
       " 0.07987822964787483,\n",
       " 0.0794347901828587,\n",
       " 0.08136735437437892,\n",
       " 0.08197616785764694,\n",
       " 0.08263447461649776,\n",
       " 0.08019806118682027,\n",
       " 0.07939785486087203,\n",
       " 0.08041157899424434,\n",
       " 0.08038726635277271,\n",
       " 0.07975380960851908,\n",
       " 0.07883000513538718,\n",
       " 0.079724189825356,\n",
       " 0.08092732587829232,\n",
       " 0.08219861472025514,\n",
       " 0.08240860654041171,\n",
       " 0.08395987842231989,\n",
       " 0.08399612829089165,\n",
       " 0.08356148609891534,\n",
       " 0.0858471835963428,\n",
       " 0.08679901482537389,\n",
       " 0.08759363740682602,\n",
       " 0.0896489410661161,\n",
       " 0.09214655077084899,\n",
       " 0.08749035000801086,\n",
       " 0.08493249537423253,\n",
       " 0.08187580155208707,\n",
       " 0.0795632372610271,\n",
       " 0.07838626438751817,\n",
       " 0.07813628716394305,\n",
       " 0.07747559109702706,\n",
       " 0.07694545481353998,\n",
       " 0.07706441078335047,\n",
       " 0.07644167495891452,\n",
       " 0.07583028031513095,\n",
       " 0.07618281291797757,\n",
       " 0.07675464218482375,\n",
       " 0.07695856969803572,\n",
       " 0.07740148203447461,\n",
       " 0.07678755279630423,\n",
       " 0.07732143113389611,\n",
       " 0.07760933227837086,\n",
       " 0.07817339524626732,\n",
       " 0.07902597868815064,\n",
       " 0.08025066275149584,\n",
       " 0.08153714844956994,\n",
       " 0.08468645438551903,\n",
       " 0.08594191493466496,\n",
       " 0.08598534297198057,\n",
       " 0.08304034033790231,\n",
       " 0.08040199056267738,\n",
       " 0.07817478710785508,\n",
       " 0.07623574649915099,\n",
       " 0.07616907032206655,\n",
       " 0.077516108751297,\n",
       " 0.08100203843787313,\n",
       " 0.08182176295667887,\n",
       " 0.08157362043857574,\n",
       " 0.08198940940201283,\n",
       " 0.08059278037399054,\n",
       " 0.0800960254855454,\n",
       " 0.08016914268955588,\n",
       " 0.07950198510661721,\n",
       " 0.07938176672905684,\n",
       " 0.07894349982962012,\n",
       " 0.08038385305553675,\n",
       " 0.07983225071802735,\n",
       " 0.08099447190761566,\n",
       " 0.08027577167376876,\n",
       " 0.08124741306528449,\n",
       " 0.08053848426789045,\n",
       " 0.07612733822315931,\n",
       " 0.07356905145570636,\n",
       " 0.07189823687076569,\n",
       " 0.07094793487340212,\n",
       " 0.07162654725834727,\n",
       " 0.07477122265845537,\n",
       " 0.07623047195374966,\n",
       " 0.07506112242117524,\n",
       " 0.07335658138617873,\n",
       " 0.07390881376340985,\n",
       " 0.07332810014486313,\n",
       " 0.07306150859221816,\n",
       " 0.07471510348841548,\n",
       " 0.0738222748041153,\n",
       " 0.07486255466938019,\n",
       " 0.07524667819961905,\n",
       " 0.07506901631131768,\n",
       " 0.07422137167304754,\n",
       " 0.07409976050257683,\n",
       " 0.07357655046507716,\n",
       " 0.0736575978808105,\n",
       " 0.07540796510875225,\n",
       " 0.07584889372810721,\n",
       " 0.07571642752736807,\n",
       " 0.07584837824106216,\n",
       " 0.07507853023707867,\n",
       " 0.0745362164452672,\n",
       " 0.07199952844530344,\n",
       " 0.07107538124546409,\n",
       " 0.07215377688407898,\n",
       " 0.07198048569262028,\n",
       " 0.07233904721215367,\n",
       " 0.0731162317097187,\n",
       " 0.07152826152741909,\n",
       " 0.07308081304654479,\n",
       " 0.07383320992812514,\n",
       " 0.07314937189221382,\n",
       " 0.07354901125654578,\n",
       " 0.07120032049715519,\n",
       " 0.07132629305124283,\n",
       " 0.07065807655453682,\n",
       " 0.07076503615826368,\n",
       " 0.07279595546424389,\n",
       " 0.07058353582397103,\n",
       " 0.06974339531734586,\n",
       " 0.07007816573604941,\n",
       " 0.07021812768653035,\n",
       " 0.06957818055525422,\n",
       " 0.06859506946057081,\n",
       " 0.06870657345280051,\n",
       " 0.06911400193348527,\n",
       " 0.0686604087240994,\n",
       " 0.06858224608004093,\n",
       " 0.06856729229912162,\n",
       " 0.06992036057636142,\n",
       " 0.06959793996065855,\n",
       " 0.06942742737010121,\n",
       " 0.0703434064052999,\n",
       " 0.07083175424486399,\n",
       " 0.0698221733327955,\n",
       " 0.07149975560605526,\n",
       " 0.07126920763403177,\n",
       " 0.07210682798177004,\n",
       " 0.07363267056643963,\n",
       " 0.07464860798791051,\n",
       " 0.07407111953943968,\n",
       " 0.07391035836189985,\n",
       " 0.07473406428471208,\n",
       " 0.07529029110446572,\n",
       " 0.07558531779795885,\n",
       " 0.07506903633475304,\n",
       " 0.07249876158311963,\n",
       " 0.07108682347461581,\n",
       " 0.06945340894162655,\n",
       " 0.06832307763397694,\n",
       " 0.06675311224535108,\n",
       " 0.066776740597561,\n",
       " 0.06592294061556458,\n",
       " 0.06706420285627246,\n",
       " 0.06636771862395108,\n",
       " 0.06775014474987984,\n",
       " 0.0696356650441885,\n",
       " 0.06970441387966275,\n",
       " 0.07130315853282809,\n",
       " 0.06944378558546305,\n",
       " 0.06916479580104351,\n",
       " 0.0692597976885736,\n",
       " 0.06840022280812263,\n",
       " 0.06849545985460281,\n",
       " 0.06797709269449115,\n",
       " 0.06799944513477385,\n",
       " 0.0693856393918395,\n",
       " 0.06932209804654121,\n",
       " 0.0691839293576777,\n",
       " 0.06912769796326756,\n",
       " 0.06898130429908633,\n",
       " 0.07051171362400055,\n",
       " 0.07184200314804912,\n",
       " 0.07023179065436125,\n",
       " 0.0710584968328476,\n",
       " 0.0718928906135261,\n",
       " 0.07018436957150698,\n",
       " 0.06781168631277978,\n",
       " 0.06630874332040548,\n",
       " 0.06679086806252599,\n",
       " 0.06861002021469176,\n",
       " 0.06855478696525097,\n",
       " 0.06784495175816119,\n",
       " 0.06905745924450457,\n",
       " 0.06807302683591843,\n",
       " 0.06712035089731216,\n",
       " 0.0678478165064007,\n",
       " 0.0690904560033232,\n",
       " 0.067068328615278,\n",
       " 0.06652309652417898,\n",
       " 0.06727182469330728,\n",
       " 0.06709188688546419,\n",
       " 0.06728012207895517,\n",
       " 0.06753094657324255,\n",
       " 0.06602069898508489,\n",
       " 0.06429248419590294,\n",
       " 0.06368188303895295,\n",
       " 0.06462551630102098,\n",
       " 0.06304996111430228,\n",
       " 0.06287939287722111,\n",
       " 0.06375892297364771,\n",
       " 0.06430032826028764,\n",
       " 0.06344577064737678,\n",
       " 0.0634009747300297,\n",
       " 0.06300806952640414,\n",
       " 0.06333453906700015,\n",
       " 0.06434724107384682,\n",
       " 0.06590947834774852,\n",
       " 0.06532282079569995,\n",
       " 0.06521527166478336,\n",
       " 0.06519397324882448,\n",
       " 0.0650190724991262,\n",
       " 0.06498033972457051,\n",
       " 0.06535725202411413,\n",
       " 0.06539042806252837,\n",
       " 0.0663452185690403,\n",
       " 0.06811841996386647,\n",
       " 0.07018343871459365,\n",
       " 0.06965920375660062,\n",
       " 0.06918909773230553,\n",
       " 0.06883302517235279,\n",
       " 0.06859828298911452,\n",
       " 0.067578942514956,\n",
       " 0.06682782992720604,\n",
       " 0.06641838047653437,\n",
       " 0.0653585558757186,\n",
       " 0.06604530825279653,\n",
       " 0.06560036144219339,\n",
       " 0.06500296760350466,\n",
       " 0.06604991341009736,\n",
       " 0.06495417398400605,\n",
       " 0.06691137608140707,\n",
       " 0.07002806547097862,\n",
       " 0.0700913299806416,\n",
       " 0.06917684013023973,\n",
       " 0.06765226600691676,\n",
       " 0.06866247672587633,\n",
       " 0.06825643638148904,\n",
       " 0.06839859997853637,\n",
       " 0.0686943088658154,\n",
       " 0.06686942465603352,\n",
       " 0.0666652552317828,\n",
       " 0.06601518159732223,\n",
       " 0.06494094245135784,\n",
       " 0.06585844955407083,\n",
       " 0.06441437499597669,\n",
       " 0.06465313420630991,\n",
       " 0.06613031146116555,\n",
       " 0.06750693474896252,\n",
       " 0.06706139422021806,\n",
       " 0.06683155288919806,\n",
       " 0.0661005296278745,\n",
       " 0.06571796257048845,\n",
       " 0.06639626272954047,\n",
       " 0.06626330898143351,\n",
       " 0.06446680217050016,\n",
       " 0.06325078196823597,\n",
       " 0.06299481308087707,\n",
       " 0.06361790490336716,\n",
       " 0.06493412563577294,\n",
       " 0.06437790207564831,\n",
       " 0.06371643650345504,\n",
       " 0.06358615914359689,\n",
       " 0.06533070025034249,\n",
       " 0.06333840801380575,\n",
       " 0.06300933635793626,\n",
       " 0.062350413762032986,\n",
       " 0.06274795765057206,\n",
       " 0.063838868169114,\n",
       " 0.06300132093019783,\n",
       " 0.06200386397540569,\n",
       " 0.06213666358962655,\n",
       " 0.06100414786487818,\n",
       " 0.06305003236047924,\n",
       " 0.062381650088354945,\n",
       " 0.06218618177808821,\n",
       " 0.06196237076073885,\n",
       " 0.06159024219959974,\n",
       " 0.06212565745227039,\n",
       " 0.06283611175604165,\n",
       " 0.06284346291795373,\n",
       " 0.06310289981774986,\n",
       " 0.06275848741643131,\n",
       " 0.062099475180730224,\n",
       " 0.062178903724998236,\n",
       " 0.06262173620052636,\n",
       " 0.06364167830906808,\n",
       " 0.06342877098359168,\n",
       " 0.06383741763420403,\n",
       " 0.0651326971128583,\n",
       " 0.06291111395694315,\n",
       " 0.061923231929540634,\n",
       " 0.06337271304801106,\n",
       " 0.06425562058575451,\n",
       " 0.0635830475948751,\n",
       " 0.06278501986525953,\n",
       " 0.06083786254748702,\n",
       " 0.060555153992027044,\n",
       " 0.061805367935448885,\n",
       " 0.06318114465102553,\n",
       " 0.06114688306115568,\n",
       " 0.06053643114864826,\n",
       " 0.0601546170655638,\n",
       " 0.06036922358907759,\n",
       " 0.06136665120720863,\n",
       " 0.062331875087693334,\n",
       " 0.06233171233907342,\n",
       " 0.061652116710320115,\n",
       " 0.06062634661793709,\n",
       " 0.061247502686455846,\n",
       " 0.06114336452446878,\n",
       " 0.0604329570196569,\n",
       " 0.061622872250154614,\n",
       " 0.0611148769967258,\n",
       " 0.06106357369571924,\n",
       " 0.06140573648735881,\n",
       " 0.0608614319935441,\n",
       " 0.06324936682358384,\n",
       " 0.061799155781045556,\n",
       " 0.060734905768185854,\n",
       " 0.06044228537939489,\n",
       " 0.0601423098705709,\n",
       " 0.06006333348341286,\n",
       " 0.06183311063796282,\n",
       " 0.06154474150389433,\n",
       " 0.06123893125914037,\n",
       " 0.061860687332227826,\n",
       " 0.06086996849626303,\n",
       " 0.06012379098683596,\n",
       " 0.05993294739164412,\n",
       " 0.05989575106650591,\n",
       " 0.05987371364608407,\n",
       " 0.061415350530296564,\n",
       " 0.06085814815014601,\n",
       " 0.0626457768958062,\n",
       " 0.06362651707604527,\n",
       " 0.06269089342094958,\n",
       " 0.06315128761343658,\n",
       " 0.06236127740703523,\n",
       " 0.06220748322084546,\n",
       " 0.06272729695774615,\n",
       " 0.06200585304759443,\n",
       " 0.06351448874920607,\n",
       " 0.06212087161839008,\n",
       " 0.060819882433861494,\n",
       " 0.06105517805553973,\n",
       " 0.0605206023901701,\n",
       " 0.06056030699983239,\n",
       " 0.06004771846346557,\n",
       " 0.060542532010003924,\n",
       " 0.06118033337406814,\n",
       " 0.060054349014535546,\n",
       " 0.06055624480359256,\n",
       " 0.05991539126262069,\n",
       " 0.05932892649434507,\n",
       " 0.059318057261407375,\n",
       " 0.05910393386147916,\n",
       " 0.05836072005331516,\n",
       " 0.05696088704280555,\n",
       " 0.05789817776530981,\n",
       " 0.05746918893419206,\n",
       " 0.057215682696551085,\n",
       " 0.056757927406579256,\n",
       " 0.05605260236188769,\n",
       " 0.05693719466216862,\n",
       " 0.057002044981345534,\n",
       " 0.05672223470173776,\n",
       " 0.056973852682858706,\n",
       " 0.057166907005012035,\n",
       " 0.057657808996737,\n",
       " 0.05734868813306093,\n",
       " 0.05785759445279837,\n",
       " 0.058150908444076777,\n",
       " 0.058291102992370725,\n",
       " 0.05914499866776168,\n",
       " 0.05880091921426356,\n",
       " 0.057915672892704606,\n",
       " 0.05798049201257527,\n",
       " 0.05882352730259299,\n",
       " 0.05890208180062473,\n",
       " 0.05902862618677318,\n",
       " 0.058432390447705984,\n",
       " 0.058472591917961836,\n",
       " 0.057933123083785176,\n",
       " 0.058558568358421326,\n",
       " 0.058172192657366395,\n",
       " 0.05962410778738558,\n",
       " 0.059682847233489156,\n",
       " 0.05856377282179892,\n",
       " 0.056977496948093176,\n",
       " 0.057505093049257994,\n",
       " 0.05771957151591778,\n",
       " 0.058006476843729615,\n",
       " 0.05775629333220422,\n",
       " 0.058860239339992404,\n",
       " 0.05815736181102693,\n",
       " 0.05928190448321402,\n",
       " 0.058886005776003,\n",
       " 0.06038521626032889,\n",
       " 0.0584344498347491,\n",
       " 0.0580046228133142,\n",
       " 0.058539690682664514,\n",
       " 0.056774620432406664,\n",
       " 0.05686777085065842,\n",
       " 0.05741623160429299,\n",
       " 0.057300277054309845,\n",
       " 0.057493461994454265,\n",
       " 0.05924467206932604,\n",
       " 0.06023114011622965,\n",
       " 0.05947738513350487,\n",
       " 0.05840039881877601,\n",
       " 0.057456146692857146,\n",
       " 0.05702955019660294,\n",
       " 0.056140189757570624,\n",
       " 0.05626205378212035,\n",
       " 0.05603012605570257,\n",
       " 0.05579712591134012,\n",
       " 0.05605516256764531,\n",
       " 0.05529146874323487,\n",
       " 0.05605945107527077,\n",
       " 0.056670565623790026,\n",
       " 0.05703068454749882,\n",
       " 0.05763037106953561,\n",
       " 0.059600418200716376,\n",
       " 0.05800335272215307,\n",
       " 0.05687552294693887,\n",
       " 0.05523862480185926,\n",
       " 0.05426022480241954,\n",
       " 0.05456302734091878,\n",
       " 0.05398222804069519,\n",
       " 0.05350702628493309,\n",
       " 0.05428383080288768,\n",
       " 0.05441162805072963,\n",
       " 0.054504243889823556,\n",
       " 0.05572367622517049,\n",
       " 0.05646385811269283,\n",
       " 0.05600155144929886,\n",
       " 0.05468601267784834,\n",
       " 0.054200886050239205,\n",
       " 0.056274340488016605,\n",
       " 0.05553893907926977,\n",
       " 0.05564298131503165,\n",
       " 0.05402771080844104,\n",
       " 0.05455135810188949,\n",
       " 0.055557601153850555,\n",
       " 0.054567934246733785,\n",
       " 0.054647342301905155,\n",
       " 0.05503992224112153,\n",
       " 0.05447876243852079,\n",
       " 0.05529622687026858,\n",
       " 0.05605714279226959,\n",
       " 0.05491971108131111,\n",
       " 0.056736047146841884,\n",
       " 0.05694341426715255,\n",
       " 0.05625382228754461,\n",
       " 0.05652416101656854,\n",
       " 0.05595936602912843,\n",
       " 0.055569373769685626,\n",
       " 0.056350917322561145,\n",
       " 0.0553375487215817,\n",
       " 0.05481172166764736,\n",
       " 0.05430326773785055,\n",
       " 0.054626994766294956,\n",
       " 0.055630975402891636,\n",
       " 0.05593367898836732,\n",
       " 0.056419563945382833,\n",
       " 0.05780031136237085,\n",
       " 0.059352082666009665,\n",
       " 0.05863112420774996,\n",
       " 0.05821544793434441,\n",
       " 0.05811081198044121,\n",
       " 0.05664537148550153,\n",
       " 0.05713234352879226,\n",
       " 0.05612704926170409,\n",
       " 0.055027633206918836,\n",
       " 0.05418789177201688,\n",
       " 0.053936093812808394,\n",
       " 0.05422044708393514,\n",
       " 0.05592173966579139,\n",
       " 0.055722871562466025,\n",
       " 0.05619072890840471,\n",
       " 0.05599529296159744,\n",
       " 0.055553081911057234,\n",
       " 0.055593426804989576,\n",
       " 0.05645104288123548,\n",
       " 0.056410908000543714,\n",
       " 0.056906438898295164,\n",
       " 0.058292866917327046,\n",
       " 0.05695377150550485,\n",
       " 0.05564062902703881,\n",
       " 0.054768705973401666,\n",
       " 0.053825306706130505,\n",
       " 0.05363790621049702,\n",
       " 0.05360671621747315,\n",
       " 0.0547856988850981,\n",
       " 0.054711275501176715,\n",
       " 0.05569668090902269,\n",
       " 0.05689954152330756,\n",
       " 0.055500497575849295,\n",
       " 0.055826575262472034,\n",
       " 0.05617730598896742,\n",
       " 0.055837043561041355,\n",
       " 0.05496060149744153,\n",
       " 0.054704078705981374,\n",
       " 0.054591325810179114,\n",
       " 0.054282399360090494,\n",
       " 0.054168928880244493,\n",
       " 0.05297059821896255,\n",
       " 0.05313663720153272,\n",
       " 0.05294150416739285,\n",
       " 0.052943163784220815,\n",
       " 0.05338227399624884,\n",
       " 0.0537276070099324,\n",
       " 0.05476700887084007,\n",
       " 0.05435547907836735]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch = {\n",
    "        \"mu_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        ),\n",
    "        \"log_var_net\": nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "    }\n",
    "\n",
    "VAD_best = VAD_Gumbel(mu_layers=arch['mu_net'], var_layers=arch['log_var_net'], device=device)\n",
    "trainer_best = VAD_Trainer(var_decoder=VAD_best, dataloader=train_dl, latent_dim=128, device=device, lr=0.001)\n",
    "trainer_best.train(num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d65214-623e-464a-80cb-ef3469536724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_tsne(train_ds, trainer_best.latents, f\"tsne_Gumbel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd661-ea43-4e60-93f0-cade1c016a2f",
   "metadata": {},
   "source": [
    "## Sample specific vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2bfd32-1c89-4f4f-9494-9d7f6e7adc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = len(test_dl.dataset)\n",
    "temp_latents_best = torch.randn(10, 128).to(device)\n",
    "latents_best = torch.nn.Parameter(torch.stack([temp_latents_best[label,:] for label in test_dl.dataset.y])).to(device)\n",
    "opt = optim.Adam([latents_best], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6568406b-c6c5-40af-9339-9825107b7bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD has finished test evaluation with a test loss of 0.15479616168886423.\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(model=VAD_best, test_dl=test_dl, opt=opt, latents=latents_best, epochs=1000, device=device)\n",
    "print(f\"AD has finished test evaluation with a test loss of {test_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5d3b54-cf39-4e45-af3f-8a573b9b3007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_tsne(test_ds, latents_best, f\"tsne_test_Gumbel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad556234-27db-4273-b409-cc4d0380268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 5 indices from the test dataset\n",
    "random.seed(6)\n",
    "sampled_indices = random.sample(range(len(latents_best)), 5)\n",
    "\n",
    "# Extract the corresponding vectors (input data) and their labels\n",
    "sampled_latents = [latents_best[i] for i in sampled_indices]  # Only selecting input data, not labels\n",
    "\n",
    "# Convert to a single tensor (optional)\n",
    "sampled_latents_tensor = torch.stack(sampled_latents)\n",
    "random_latents_tensor = torch.randn_like(sampled_latents_tensor)\n",
    "\n",
    "sampled_test_images = VAD_best(sampled_latents_tensor).view(-1, 1, 28, 28)\n",
    "random_test_images = VAD_best(random_latents_tensor).view(-1, 1, 28, 28)\n",
    "\n",
    "utils.save_images(sampled_test_images, \"sampled_test_images_VAD_Gumbel.png\")\n",
    "utils.save_images(random_test_images, \"random_test_images_VAD_Gumbel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3c1d2-20be-4513-a96b-333500bd74a2",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3637ac7-10ef-4da8-a47c-1c1a84f8ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random.seed(44)\n",
    "sampled_indices = random.sample(range(len(latents_best)), 2)\n",
    "sampled_latents = [latents_best[i] for i in sampled_indices]\n",
    "weights = np.linspace(0, 1, 7)\n",
    "interpolated_latents = [w * sampled_latents[0] + (1 - w) * sampled_latents[1] for w in weights]\n",
    "interpolated_latents_tensor = torch.stack(interpolated_latents)\n",
    "interpolated_images = VAD_best(interpolated_latents_tensor).view(-1, 1, 28, 28)\n",
    "utils.save_images(interpolated_images, \"interpolated_images_gumbell.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
